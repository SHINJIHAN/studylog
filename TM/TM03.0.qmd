---
title: "3장: 네이버 블로그 크롤링"
format: html
---

> Reporting Date: March. 18, 2025

동적 크롤링을 위한 준비 및 네이버 블로그 크롤링 실습에 대해 다루고자 한다.


01 자바 설치 방법


 1 .  파이썬과 자바의 관계
일반적으로 파이썬은 자바 없이 독립적으로 실행할 수 있다.

하지만, 특정 라이브러리(예: JPype, PySpark, Jython 등)는
자바(Java)를 필요로 한다.

따라서 사용하려는 기능이 자바 기반이라면, 먼저 자바가 설치되어 있어야 한다.



 2 .  자바 설치 여부 확인
Anaconda 프롬프트 실행한 다음 명령어 입력 후 실행.

# 자바가 설치되어 있다면 버전 정보가 출력됨.
# "java is not recognized..." 오류가 발생하면 자바가 설치되지 않은 것임.

java -version


 3 .  내 컴퓨터에 자바 설치하기
Oracle 공식 홈페이지에서 JDK 다운로드 설치 후, 환경 변수를 설정해야 한다.


Download the Latest Java LTS Free

Subscribe to Java SE and get the most comprehensive Java support available, with 24/7 global access to the experts.

www.oracle.com


환경 변수 설정 (Windows 기준)

제어판 → 시스템 및 보안 → 시스템 → 고급 시스템 설정
고급 탭 → 환경 변수 버튼 클릭
시스템 변수에서 "새로 만들기" 클릭
변수 이름:  JAVA_HOME
변수 값:      C:\Program Files\Java\jdk-XX.X.X (설치된 JDK 경로 입력)
Path 변수 편집  →  ;%JAVA_HOME%\bin 추가

Anaconda 프롬프트 또는 명령 프롬프트에서
다시 입력하여 정상적으로 출력되는지 확인한다.



02 Selenium을 사용한 동적 크롤링


 1 .  Selenium 설치
웹 브라우저에서 동적 크롤링 시 가장 많이 사용하는 패키지.

과거에는 웹드라이버 버전에 맞는 경로를 지정해줘야 했지만,
현재는 패키지의 새버전에 의해 자동적으로 맞춰진다.

pip install selenium

 2 .  웹드라이버 다운로드
사용하는 브라우저에 맞는 WebDriver를 다운로드해야 한다.


Chrome 다운로드 및 설치 - 컴퓨터 - Google Chrome 고객센터

도움이 되었나요? 어떻게 하면 개선할 수 있을까요? 예아니요

support.google.com
다운로드한 WebDriver를 실행 파일 경로에 두거나,
Python 코드에서 직접 경로를 지정해야 한다.



03 네이버 블로그 크롤링


1. 라이브러리 불러오기.

from selenium import webdriver       # 웹 브라우저 자동화
from bs4 import BeautifulSoup as BS  # HTML 및 XML 파싱

import pandas as pd  # 데이터 조작 및 분석
import requests      # HTTP 요청을 보내기 위한 모듈
import datetime      # 날짜 및 시간 연산
import pickle        # 파이썬 객체 직렬화
import time          # 코드 실행 간격 조절
import re            # 정규 표현식을 사용하여 문자열 처리

# Selenium에서 다양한 방법으로 HTML 요소를 찾기
from selenium.webdriver.common.by import By


2. Selenium을 이용한 네이버 블로그 검색 자동화.

# 크롬 드라이버 실행
driver = webdriver.Chrome()

# 네이버 블로그 검색 페이지로 이동
# 검색할 키워드 지정 및 데이터 수집기간 설정한 뒤
# 복사한 URL을 붙여 넣으면 되며, 아래 코드는 가독성을 위해 일부러 줄바꿈을 시도함
driver.get('''
https://search.naver.com/search.naver?
ssc=tab.blog.all&query=%EB%85%B8%EC%9D%B8%20%EB%B6%80%EC%96%91
&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20240301to20240325'''.replace("\n", ""))

URL 가져오는 방법



실행 화면


3. 웹 페이지 자동 스크롤 함수.

def doScrollDown(whileSeconds):
    start = datetime.datetime.now() # 스크롤 다운 시작 시간 설정
    end = start + datetime.timedelta(seconds=whileSeconds) # 스크롤 다운 종료 시간

	while True:
        # 페이지 맨 아래로 스크롤 다운
        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
        time.sleep(1) # 1초 대기

        # 종료 시간에 도달하면 반복 종료
        if datetime.datetime.now() > end:
            break
            
doScrollDown(2) # 스크롤 다운 시간 설정


4. 웹 페이지에서 제목과 URL 추출하기.

# 제목과 URL을 저장할 리스트 초기화
title_list = []
url_list = []

# 현재 페이지에서 클래스명이 'title_link'인 요소들을 찾음
titles = driver.find_elements(By.CLASS_NAME, 'title_link')

for i, title_element in enumerate(titles):
    try:
        # 요소에서 제목을 추출하여 title_list에 추가
        title_list.append(title_element.text)
        # 요소에서 URL을 추출하여 url_list에 추가
        url_list.append(title_element.get_attribute('href'))
    except:
        print("오류 발생")  # 예외 발생 시 출력
        continue  # 오류가 발생해도 다음 요소 처리 계속 진행

    # 10번째 항목마다 진행 상황 출력
    if (i + 1) % 10 == 0:
        print(f"{i + 1}개 수집 완료")



5. 블로그 본문 및 메타데이터 크롤링 자동화.

# 1] 크롤링 데이터 저장 리스트 초기화
new_doc, like_cnt, comment_cnt, comment_list, img_cnt, div_cnt = [], [], [], [], [], []

# 2] 블로그 본문 크롤링
for i in range(len(url_list)):
    url_path = url_list[i]       # URL 불러오기
    driver.switch_to.window(driver.window_handles[0])             # 첫 번째 탭으로 이동
    driver.execute_script("window.open('{}')".format(url_path))   # 새 탭 열기(URL 실행)
    driver.switch_to.window(driver.window_handles[1])             # 두 번째 탭으로 이동

    time.sleep(1)  # 1초 대기
    try:
        iframes = driver.find_elements(By.TAG_NAME, "iframe")
        d = ''     # 댓글 변수 초기화

        # 댓글 영역의 HTML 코드 가져오기
        if len(iframes) > 0:		    # iframes의 존재 확인
            driver.switch_to.frame(0)       # 첫 번째 iframe으로 전환 및 내용 가져옴
            html = driver.page_source       # HTML 코드 가져와 변수 저장
            soup = BS(html, "html.parser")  # 저장된 코드 파싱 및 soup 생성

            # 3] 블로그 본문 추출
            try:
                a = soup.find("div", class_="se-main-container").get_text()
			except: # 블로그 본문을 찾지 못할 경우
                a = soup.find("div", id="postListBody")     # 일반 블로그에 경우
                a = re.sub("[^ㄱ-ㅎㅏ-ㅣ가-힣]", "", str(a)) # 정규표현식 -> 한글만 남김

            # 4] 좋아요 수 추출
            try:
                b = soup.find("em", class_="u_cnt_count").get_text()
            except:
                b = "null"

            # 5] 댓글 수 추출
            try:
                c = soup.find("em", id="commentCount").get_text()
            except:
                c = "null"

            # 6] 댓글 추출
            try: # 댓글을 모두 보기 위해 버튼 클릭
                comment = driver.find_elements(By.CLASS_NAME, "btn_arr")
                comment[-1].click()  # 마지막 댓글 버튼 클릭
                time.sleep(1)
                commentLen = len(driver.find_elements(By.CLASS_NAME, "u_cbox_page"))
                d = "\n".join([comment.text for comment in driver.find_elements(By.CLASS_NAME, "u_cbox_text_wrap")])
            except:
                d = "null"

            # 7] 이미지 및 영상 수 추출
            e = len(soup.find_all("img", class_="se-image-resource egjs-visible"))
            f = len(soup.find_all("div", class_="pzp-ui-dimmed pzp-dimmed pzp-pc__dimmed"))

            # 8] 데이터 리스트에 추가
            new_doc.append(a)
            like_cnt.append(b)
            comment_cnt.append(c)
            comment_list.append(d)
            img_cnt.append(e)
            div_cnt.append(f)

            driver.switch_to.default_content()  # 기본 콘텐츠로 전환
        else:
            # 데이터가 없을 경우 빈 값 추가
            new_doc.append(' ')
            like_cnt.append(' ')
            comment_cnt.append(' ')
            comment_list.append(' ')
            img_cnt.append(' ')
            div_cnt.append(' ')

    except Exception as e:
        # 예외 발생 시 에러 메시지와 함께 빈 값 추가
        print(f"Error at {url_path}: {e}")
        new_doc.append(' ')
        like_cnt.append(' ')
        comment_cnt.append(' ')
        comment_list.append(' ')
        img_cnt.append(' ')
        div_cnt.append(' ')

    driver.close()  # 현재 탭 닫기
    time.sleep(0.3)  # 0.3초 대기

    # 매 10번마다 진행 상황 출력
    if (i+1) % 10 == 0:
        print(f"진행 상황: {i+1}/{len(url_list)}")



6. 데이터프레임으로 변환.

# 크롤링한 데이터를 데이터프레임으로 변환
raw_data = pd.DataFrame({
    "title": title_list,
    "doc": new_doc,
    "like": like_cnt,
    "comment_cnt": comment_cnt,
    "commnet_list": comment_list,
    "img": img_cnt,
    "div": div_cnt,
    "ch": "naver",
    "ch2": "blog"
})

# 데이터프레임을 pickle 파일로 저장
file_path = "C:/Users/jkl12/텍스트마이닝/"  # 슬래시 사용
with open(file_path + "노인부양blog.pkl", "wb") as f:
    pickle.dump(raw_data, f)

# 크롬 드라이버 종료
driver.quit()

# 저장된 pickle 파일을 불러오기
with open(file_path + "노인부양blog.pkl", "rb") as f:
    temp_file = pickle.load(f)

# 데이터프레임을 CSV 파일로 저장
temp_file.to_csv(file_path + "노인부양blog.csv", index=False, encoding="utf-8-sig")
# 파일 경로 지정
file_path = r"C:\Users\jkl12\텍스트마이닝\노인부양blog.csv"

# CSV 파일 불러오기
df = pd.read_csv(file_path, encoding="utf-8-sig")
df

---

[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판