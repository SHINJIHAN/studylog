---
title: "2장: 네이버 뉴스 기사 제목 크롤링"
format: html
---

> Reporting Date: March. 11, 2025

웹 크롤링 개념 및 정적 크롤링 실습에 대해 다루고자 한다.

# 01 데이터 종류


## 1 .  정형 데이터
(Structured Data)

일정한 형식을 갖춘 데이터로, 데이터베이스의 테이블처럼 행과 열로 정리된다.

예: 엑셀, SQL 데이터베이스, 고객 정보(이름, 나이, 주소 등).

룰세팅(Rule Setting) 데이터를 저장할 때
고정된 형식(테이블, 행/열 구조, 스키마 등)을 미리 정의하는 것.



 2 .  비정형 데이터
(Unstructured Data)

형식이 일정하지 않아
체계적으로 저장하기 어려운 데이터.

예: 텍스트(SNS 게시글, 이메일), 이미지, 동영상, 음성 데이터.

주로 자연어 처리(NLP)나 텍스트 마이닝 등의 기법을 활용해 분석.



 3 .  반정형 데이터
(Semi-Structured Data)

일정한 구조를 가지지만 완전히 정형화되지 않은 데이터.
태그나 특정한 형식(XML, JSON 등)을 포함하여 구조화가 가능하다.

예: HTML, JSON, XML 파일, 로그 데이터.



02 크롤링


 1 .  정적 크롤링
웹 페이지의 HTML 소스 코드를
직접 가져와서 필요한 데이터를 추출하는 방식.

기본적으로, requests 라이브러리로 웹 페이지 HTML을
가져와 BeautifulSoup으로 데이터 추출한다.

페이지 로딩 속도가 빠르고, 서버 부하가 적으나
JavaScript로 생성되는 데이터는 가져올 수 없다.

HTML만으로 필요한 정보를 얻을 수 있다면
→ 정적 크롤링이 유리하다.



 2 .  동적 크롤링
웹 브라우저를 실제로 실행하여
JavaScript로 로드되는 데이터까지 가져오는 방식.

기본적으로, Selenium이나
Playwright 같은 브라우저 자동화 도구 사용한다.

JavaScript 렌더링된 데이터를 포함하여 크롤링 가능하나,
속도가 느리고, 서버 부하가 높다.

JavaScript로 데이터가 동적으로 로딩된다면
→ 동적 크롤링이 필요하다.



03 라이브러리
Jupyter Notebook에서 실행하는 명령어는
기본적으로 일반적인 Python 실행 환경에서도 동일하게 사용할 수 있다.

(예: 터미널, 명령 프롬프트, 다른 IDE)



 1 .  requests
파이썬에서 HTTP 요청을 보내고 응답을 받을 수 있는 라이브러리로,
주로 웹에서 데이터를 가져오거나 서버에 데이터를 전송하는 데 사용된다.

웹사이트와 데이터를 주고받는 과정에서 사용되는
HTTP 프로토콜을 쉽게 다룰 수 있도록 도와준다.



① GET 요청 - requests.get()
웹 페이지의 정보를 가져올 때 사용된다.
이는 브라우저에서 주소를 입력하고 페이지를 여는 것과 같은 동작이다.



② POST 요청 - requests.post()
서버에 데이터를 전송할 때 사용된다.
회원가입, 로그인, 데이터 저장 등의 작업에서 활용된다.



③ JSON 응답 처리 - response.json()
서버에서 JSON 형식의 데이터를 받으면,
.json() 메서드를 사용하여 딕셔너리로 변환할 수 있다.





 2 .  BeautifulSoup4
HTML/XML 문서를 파싱하여 원하는 데이터를 추출하는 라이브러리로,
문서를 구성하는 요소를 개별적인 구조(태그, 속성, 텍스트 등)로 나눈다.

먼저, HTML 문서를 파싱하여 태그 간의
계층을 이해할 수 있는 트리 구조로 변환한다.

이를 통해 특정 태그나 클래스에 접근할 수 있으며,
CSS 선택자를 활용하여 원하는 요소를 쉽게 선택할 수 있다.

또한, get_text() 메서드를 사용하면 태그 내부의 텍스트만
추출할 수 있어 데이터 정제 작업이 용이하다.





 3 .  selenium
웹 브라우저를 자동으로 제어하는 라이브러리로,
클릭, 입력, 스크롤 등의 동작을 수행할 수 있다.

JavaScript로 동적으로 변경되는 웹 페이지의 데이터도
가져올 수 있어 정적인 크롤링 방식보다 더 유연하다.

이를 사용하려면 Chrome, Firefox 등
웹 브라우저에 맞는 드라이버가 필요하며,
이를 통해 실제 브라우저를 실행하고 조작할 수 있다.

과거에는 웹 브라우저와 드라이버의 버전이 맞아야 했지만,
현재는 자동 업데이트 기능 덕분에 큰 문제가 없다.





 4 .  pandas
데이터 분석 및 처리를 위한 필수 라이브러리로,
CSV, Excel, JSON 등의 다양한 형식의 데이터를
데이터프레임으로 불러와 조작할 수 있다.

또한, 결측값을 처리하거나 특정 조건에 따라
데이터를 필터링하고 정렬하는 등 정리 작업이 가능하다.

뿐만 아니라, 데이터를 그룹화하여 분석할 수 있는 groupby() 기능,
기초 통계를 확인할 수 있는 describe() 메서드,

특정 연산을 적용할 수 있는 apply() 메서드 등을 제공하여,
보다 효과적인 데이터 분석을 지원한다.



04 정적 크롤링
다음은 네이버 뉴스 기사에 대해 정적 크롤링을 수행하는 코드이다.



1. 설치된 라이브러리를 불러오는 과정.

# HTTP 요청을 보내기 위한 라이브러리
import requests

# HTML 파싱을 위한 라이브러리
from bs4 import BeautifulSoup as bs

# 데이터 분석 및 처리 라이브러리
import pandas as pd


2. 사용자가 제공한 값을 바탕으로
추출에 대한 기능을 설정하는 과정.

# 첫 줄은 검색어 입력을 위한 변수
query = input('입력 키워드: ')

# 날짜 및 설정 값 입력 (모두 변수이며, 함수 호출을 포함함)
start_date = input('시작 날짜 (YYYY.MM.DD): ')  
end_date = input('마지막 날짜 (YYYY.MM.DD): ')  
num_pages = int(input('추출 페이지 수 기입: '))  # 입력값을 정수로 변환
csv_filename = input('파일명 (예: news.csv): ')

# 한 페이지에 표시되는 기사 수 설정 변수.
num_articles_per_page = 10 # 정수로 설정.

data = []  # 데이터 저장 리스트
입력 키워드: 부동산

# 일반적으로 최소 3개월 이상의 기간을 확보하는 것이 좋다.
시작 날짜 (YYYY.MM.DD): 2025.01.01
마지막 날짜 (YYYY.MM.DD): 2025.03.17

추출 페이지 수 기입: 100
파일명 (예: news.csv): 2025.1Q.csv
2025.1Q.csv 파일 저장 완료! 뉴스 개수: 1000개


3. 웹 크롤링 시 웹사이트에
정상적인 요청을 보내기 위한 헤더 설정. 

headers = {
	# User-Agent: Chrome 110 버전을 사용 중인 Windows 10 사용자처럼 
    # 보이도록 해 서버의 차단을 피하고, 요청을 정상 처리하도록 도움.
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}


4. 반복문을 통해 페이지별로 데이터를 요청하는 과정.

# 인덱스를 1로 시작하도록 함
for page in range(1, num_pages + 1):

	# 네이버 뉴스 페이지네이션 규칙 적용
    start_index = 1 + (page - 1) * 10
    
    # URL 생성
    search_url = (
        f"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={query}"
        f"&start={start_index}&pd=3&ds={start_date}&de={end_date}"
    )
    
    # HTML 문서 가져오기
    response = requests.get(search_url, headers=headers)
    
    if response.status_code != 200:
        print(f"페이지 {page} 요청 실패 (상태 코드: {response.status_code})")
        continue


5. 뉴스기사 제목 가져오기.

    soup = bs(response.text, 'html.parser')  # 파싱
    titles = soup.select('a.news_tit')  # 네이버 뉴스 타이틀 선택자
    
    for title in titles:
        data.append(title.get_text())  # 추출한 제목 데이터 추가

    time.sleep(1)  # 요청 간격 조정 (너무 빠르면 차단될 가능성 있음)


6. 뉴스 제목 데이터를 데이터프레임으로 변환한 뒤,
CSV 파일로 저장하는 역할을 한다.

# 리스트는 단순한 배열이므로, 데이터를 체계적으로 정리하기 어렵다.
# 데이터프레임은 표 형태로 데이터를 저장하므로, 한눈에 보기 쉽고 컬럼별로 정리할 수 있다.

# 데이터프레임 생성
df = pd.DataFrame(data, columns=['뉴스 제목'])

# CSV 저장
csv_filename = input('파일명 (예: news.csv): ')
df.to_csv(csv_filename, index=False, encoding='utf-8-sig')

print(f"{csv_filename} 파일 저장 완료! 뉴스 개수: {len(df)}개")


05 데이터 전처리


1. 라이브러리 불러오기.

from wordcloud import WordCloud  # 워드 클라우드 생성
import matplotlib.pyplot as plt  # 데이터 시각화 (그래프, 이미지 출력)
from collections import Counter  # 단어 빈도수 계산을 위한 라이브러리

import re  # 정규표현식을 활용한 텍스트 전처리 (특정 문자 제거, 패턴 찾기 등)
import pandas as pd  # 데이터프레임을 활용한 데이터 처리 및 분석
from konlpy.tag import Okt  # 한국어 형태소 분석기(Open Korean Text), (단어 추출 및 품사 태깅)


2. CSV 파일에서 뉴스 제목 추출 및 결합.

# CSV 파일 로드
csv_filename = input("CSV 파일 경로: ")
df = pd.read_csv(csv_filename)

# NaN 값 제거
df = df.dropna(subset=['뉴스 제목'])

# C열(뉴스 제목)을 하나의 문자열로 결합
c_list = df['뉴스 제목'].tolist()

# 리스트를 하나의 벡터 자료로 합치기
vector = ' '.join(c_list)


3. 형태소 분석을 통한 명사 추출 및 빈도수 계산.

# 형태소 분석 (명사 추출)
okt = Okt()
nouns = okt.nouns(vector)

# 한 글자 단어 제거
filtered_nouns = [word for word in nouns if len(word) > 1]

# 불용어 리스트
stop_words = ['부동산, 한국']  # 필요시 추가

# 불용어 제거
filtered_words = [word for word in filtered_nouns if word not in stop_words]

# 단어 빈도수 계산
word_counts = Counter(filtered_words)
print(word_counts.most_common(10))  # 가장 많이 나온 단어 10개 출력


06 워드 클라우드
워드 클라우드 생성하기.

try:
    wordcloud = WordCloud(
    	# Windows의 'Malgun Gothic' 폰트
        font_path="C:/Users/jkl12/Downloads/NanumGothicBold.otf",
        width=800, height=400, background_color='white', colormap = "Accent" # 칼라맵
    ).generate_from_frequencies(word_counts)

except:
    wordcloud = WordCloud(
        font_path=None,  # 기본 폰트로 설정 (Mac/Linux 대비)
        width=800, height=400, background_color='white'
    ).generate_from_frequencies(word_counts)

# 그래프 출력
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
CSV 파일 경로: 2025.1Q.csv
[('투자', 139), ('서울', 122), ('금융', 117), ('집값', 94), ('시장', 79), 
 ('거래', 76), ('주택', 71), ('강남', 71), ('아파트', 66), ('펀드', 64)]
 
 # 특이한 형태의 시각화는 정보 왜곡을 초래할 수 있으므로, 
 # 가능한 한 전통적인 형태로 출력하는 것이 더 효과적이다.

---

[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판