---
title: "3장: 네이버 카페 크롤링"
format: html
---

> Reporting Date: April. 01, 2025

네이버 카페 크롤랑에 대해 다루고자 한다.



합칠려면 모든 변수가 동일하게 들어가야 한다.

from selenium import webdriver                   # 브라우저 자동화
from bs4 import BeautifulSoup as BS              # html 내용 파싱
from selenium.webdriver.common.by import By      # 다양한 방법으로 엘리먼트를 찾기
from selenium.webdriver.common.keys import Keys  # Keys 클래스 가져오기(키보드 입력 제어)

import pandas as pd  # 데이터 조작 및 분석
import datetime      # 날짜와 시간 연산
import requests      # Http 요청을 보내기
import pickle        # 파이썬 객체 직렬화
import time          # 코드 실행 속도 조절
import re            # 정규 표현식 사용




1. 

driver = webdriver.Chrome()
driver.get('https://search.naver.com/search.naver?ssc=tab.cafe.all&sm=tab_jum&query=%EB%85%B8%EC%9D%B8+%EB%B6%80%EC%96%91&nso=so%3Ar%2Cp%3Afrom20240301to20240325')


3. 

title_list = []
url_list = []

# 검색 결과에서 모든 제목 링크 요소 가져오기 (스크롤 다운 포함)
for _ in range(2):  # 5번 스크롤 내리기 (필요에 따라 조절 가능)
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  # 스크롤 맨 아래로 이동
    time.sleep(1)  # 데이터 로딩을 기다리기 위해 1초 대기

titles = driver.find_elements(By.XPATH, "//*[@id='main_pack']/section/div[1]/ul/li/div/div[2]/div[2]/a")

for i, title_element in enumerate(titles, start=1):  # 1부터 카운트 시작
    try:
        title_list.append(title_element.text)  # 제목 추가
        url_list.append(title_element.get_attribute("href"))  # URL 추가

    except Exception as e:
        print(f"오류 발생: {e}")  # 오류 메시지 출력

    if i % 10 == 0:  # 진행 상황 출력 (10개 단위)
        print(f"진행 중: {i}개 완료")

print("데이터 수집 완료!")  # 최종 완료 메시지 출력


4. 

# 본문, 좋아요 수, 댓글 수, 댓글, 이미지 수, 영상 수를 저장할 리스트 초기화
new_doc = []  
like_cnt = []  
comment_cnt = []  
comment_list = []  
img_cnt = []  
div_cnt = []  

# 카페 글 크롤링
for i in range(len(url_list)):
    url_path = url_list[i]  # URL 불러오기
    driver.switch_to.window(driver.window_handles[0])  # 첫 번째 탭으로 이동
    driver.execute_script(f"window.open('{url_path}')")  # 새 탭에서 URL 실행
    driver.switch_to.window(driver.window_handles[1])  # 두 번째 탭으로 이동

    time.sleep(2)  # 2초 대기

    try:
        iframes = driver.find_elements(By.TAG_NAME, 'iframe')  # 카페 iframe 찾기
        
        if len(iframes) > 0:
            # iframe 전환
            driver.switch_to.frame('cafe_main') # ifame의 첫부분
            html = driver.page_source           # html 가져오고
            soup = BS(html, 'html.parser')      # html 파싱하라

            # 본문 추출
            try:
                a = soup.find('div', class_='article_viewer').get_text() # 값을 가져와라
            except:
                # 본문을 찾지 못할 경우
                a = 'null'

            # 좋아요 수 추출
            try:
                b = soup.find('em', class_='u_cnt _count').get_text()
            except:
                b = 'null'

            # 댓글 수 추출
            try:
                c = soup.find('strong', class_='num').get_text()
            except:
                c = 'null'

            # 댓글 추출
            try:
                d = "\n".join([t.get_text() for t in soup.find_all('span', class_='text_comment')])
            except:
                d = 'null'

            # 이미지 수 추출
            e = len(soup.find_all('img', class_='se-image-resource'))

            # 영상 수 추출
            f = len(soup.find_all('div', class_='pzp-ui-dimmed pzp-dimmed pzp-pc_dimmed'))

            # iframe에서 기본 컨텐츠로 전환
            driver.switch_to.default_content()
        else:
            a, b, c, d, e, f = 'null', 'null', 'null', 'null', 0, 0  # iframe이 없을 경우 기본값

        # 데이터 저장
        new_doc.append(a)
        like_cnt.append(b)
        comment_cnt.append(c)
        comment_list.append(d)
        img_cnt.append(e)
        div_cnt.append(f)

    except Exception as e:
        # 오류 발생 시 기본값 저장
        new_doc.append('null')
        like_cnt.append('null')
        comment_cnt.append('null')
        comment_list.append('null')
        img_cnt.append(0)
        div_cnt.append(0)
        print(f"Error occurred at index {i}")

    finally:
        # 현재 열린 탭 닫기
        driver.close()
        time.sleep(0.3)  # 0.3초 대기
        driver.switch_to.window(driver.window_handles[0])  # 첫 번째 탭으로 복귀

    # 매 10번째 URL마다 진행 상황 출력
    if (i + 1) % 10 == 0:
        print(f"진행 상황: {i + 1}/{len(url_list)}")


5. 

# 크롤링 데이터를 데이터프레임으로 변환
raw_data = pd.DataFrame()  # 초기화
raw_data['title'] = title_list  # 제목 리스트
raw_data['doc'] = new_doc  # 본문 리스트
raw_data['like'] = like_cnt  # 좋아요 수 리스트
raw_data['comment_cnt'] = comment_cnt  # 댓글 수 리스트
raw_data['comment_list'] = comment_list  # 댓글 리스트
raw_data['img'] = img_cnt  # 이미지 수 리스트
raw_data['div'] = div_cnt  # 영상 수 리스트
raw_data['ch'] = 'naver'  # 채널 정보
raw_data['ch2'] = 'cafe'  # 채널 정보 (세부)

# 데이터프레임을 pickle 파일로 저장
file_path = "C:/Users/jkl12/텍스트마이닝/"  # 슬래시 사용
with open(file_path + "노인부양cafe.pkl", "wb") as f:
    pickle.dump(raw_data, f)

# 크롬 드라이버 종료
driver.quit()

# 저장된 pickle 파일을 불러오기
with open(file_path + "노인부양cafe.pkl", "rb") as f:
    temp_file = pickle.load(f)

# 데이터프레임을 CSV 파일로 저장
temp_file.to_csv(file_path + "노인부양cafe.csv", index=False, encoding="utf-8-sig")

---

[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판