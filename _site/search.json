[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Home\n\n\nProfile\n\n\n\n\n\n\nStudyLog는 데이터 분석, 인공지능, 시스템 개발 전반의 학습과 실험을 기록하는 기술 아카이브입니다.\n\n\n이 블로그에서 다루는 내용은 다음과 같습니다.\n\n\n Data Analysis \nAI / ML / DL 프로그래밍 / 개발 컴퓨터 그래픽스 / 하드웨어 보안·네트워크 학업·연구 기타\n\n\n각 글은 학습 과정의 단계와 핵심 포인트를 구조화하여, 재현 가능성과 실무 적용 가능성을 동시에 확보합니다.\n\n\n또한 학습 과정에서 발견한 주요 인사이트와 흥미로운 관찰을 포함하여, 단순한 기록을 넘어 실무적 가치가 있는 기술 학습 자료로 활용할 수 있도록 구성합니다.\n\n\n\n&lt;p&gt;프로필 탭 내용&lt;/p&gt;\n\n\n\n\n이 웹사이트는 Quarto를 이용해 제작되었습니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "임시",
    "section": "",
    "text": "마지막 최종 테스트"
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "데이터분석입문 - 2024학년 1학기\n\n\n\n\n 02장 \n 03장 \n 04장 \n 05장 \n 06장 \n 07장 \n 08장 \n 09장 \n 10장 \n 11장 \n 12장 \n 13장 \n 14장 \n 15장 \n\n\n\n\n\n\n데이터분석심화 - 2024학년 2학기\n\n\n\n\n 06장 \n 07장 \n 08장 \n 08장 \n 08장 \n 09장 \n 09장 \n 10장 \n 10장 \n 10장 \n 10장 \n 11장 \n\n\n\n\n\n\n텍스트마이닝 이해 및 실습\n\n\n\n\n 02장 \n 03장 \n 03장 \n 04장 \n 05장 \n 06장 \n 06장 \n\n\n\n\n\n\n고급빅데이터분석실무\n\n\n\n\n 분석1 \n 분석2 \n 이론1 \n 이론2 \n 이론3"
  },
  {
    "objectID": "DAI02.html",
    "href": "DAI02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "Reporting Date: June. 9, 2024\n자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "DAI02.html#교재-출처-최하단에-표시",
    "href": "DAI02.html#교재-출처-최하단에-표시",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "교재 출처 최하단에 표시",
    "text": "교재 출처 최하단에 표시"
  },
  {
    "objectID": "DAI02.html#자-료-의-입-력",
    "href": "DAI02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "DAI02.html#도수분포표",
    "href": "DAI02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)"
  },
  {
    "objectID": "DAI02.html#막대-그래프",
    "href": "DAI02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 폰트 설정\nfont_path = r'C:\\Users\\jkl12\\Downloads\\NanumGothic.otf'\nfont_prop = font_manager.FontProperties(fname=font_path)\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontproperties=font_prop, fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontproperties=font_prop, fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontproperties=font_prop, fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0, fontproperties=font_prop)\nplt.show()"
  },
  {
    "objectID": "DAI02.html#원형-그래프",
    "href": "DAI02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n# 폰트 설정\nplt.rcParams['font.family'] = 'NanumGothic'  # 나눔고딕 폰트\nplt.rcParams['font.size'] = 12  # 기본 폰트 크기 설정\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "DAI02.html#파레토그림",
    "href": "DAI02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\nimport matplotlib.patches as mpatches\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontproperties=font_prop, fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontproperties=font_prop, fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', \n                           use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontproperties = font_prop, \n               fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right', \n           prop = font_prop)\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0, \n                   fontproperties = font_prop)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontproperties=font_prop, fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "DAI02.html#도수다각형",
    "href": "DAI02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14, fontproperties = font_prop)\nplt.ylabel('빈도수', fontsize = 14, fontproperties = font_prop)\nplt.title('음료의 히스토그램', fontsize = 16, fontproperties = font_prop)\nplt.show()\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI02.html",
    "href": "DAI/DAI02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "Reporting Date: June. 9, 2024\n자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "DAI/DAI02.html#자-료-의-입-력",
    "href": "DAI/DAI02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "DAI/DAI02.html#도수분포표",
    "href": "DAI/DAI02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "DAI/DAI02.html#막대-그래프",
    "href": "DAI/DAI02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_24476\\2514237286.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정"
  },
  {
    "objectID": "DAI/DAI02.html#원형-그래프",
    "href": "DAI/DAI02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "DAI/DAI02.html#파레토그림",
    "href": "DAI/DAI02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "DAI/DAI02.html#도수다각형",
    "href": "DAI/DAI02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI03.html",
    "href": "DAI/DAI03.html",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "",
    "text": "Reporting Date: June. 30, 2024\n연속형 자료가 어떤 값을 중심으로 분포되어 있는가를 나타내는 중심위치의 측도, 각 자료가 중심위치의 값으로부터 흩어진 정도를 나타내는 퍼진 정도의 측도 등을 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI03.html#numpy-모듈을-이용하여-계산할-수-있음.",
    "href": "DAI/DAI03.html#numpy-모듈을-이용하여-계산할-수-있음.",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "numpy 모듈을 이용하여 계산할 수 있음.",
    "text": "numpy 모듈을 이용하여 계산할 수 있음."
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-100.04125",
    "href": "DAI/DAI03.html#출력된-값-100.04125",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 100.04125",
    "text": "출력된 값 &gt; 100.04125\n3 . 중 앙 값 ( M e d i a n ) 전체 관측값을 “크기 순서로 배열” 했을 때, “가운데” 에 위치한 값.\n데이터의 개수가 “홀수” 일 경우, 중앙에 위치한 값이 중앙값이다. 데이터의 개수가 “짝수” 일 경우, 중앙에 위치한 “두 값의 평균” 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, “극단값의 영향을 받지 않는다.” 따라서, 평균과 값이 다를 수 있다."
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-100.05",
    "href": "DAI/DAI03.html#출력된-값-100.05",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 100.05",
    "text": "출력된 값 &gt; 100.05\n퍼진 정도의 측도\n4 . 분 산 ( V a r i a n c e ) 확률 분포나 데이터 집합의 산포도 ( 분포도 ) 를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 “얼마나 멀리 퍼져 있는지”를 나타내는 지표로 사용된다.\n관측값이 x₁​, x₂​, …, xn 이고, 표본평균이 x̄ 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평 균 제 곱 오 차 의 평 균 : 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n4 - 1 . 편 차 ( D e v i a t i o n ) : 각 관측값과 평균의 차이 편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 “상쇄” 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 “편차의 크기” 가 중요하므로,\n따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n4 - 2 . 자 유 도 ( D e g r e e s o f F r e e d o m ) : 위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다."
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-2.316125000000001",
    "href": "DAI/DAI03.html#출력된-값-2.316125000000001",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 2.316125000000001",
    "text": "출력된 값 &gt; 2.316125000000001"
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-2.287173437500001",
    "href": "DAI/DAI03.html#출력된-값-2.287173437500001",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 2.287173437500001",
    "text": "출력된 값 &gt; 2.287173437500001\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다.\n자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다.\n5 . 표 준 편 차 ( S t a n d a r d D e v i a t i o n ) 분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다."
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-1.521882058505192",
    "href": "DAI/DAI03.html#출력된-값-1.521882058505192",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 1.521882058505192",
    "text": "출력된 값 &gt; 1.521882058505192"
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-1.5123403841397614",
    "href": "DAI/DAI03.html#출력된-값-1.5123403841397614",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 1.5123403841397614",
    "text": "출력된 값 &gt; 1.5123403841397614\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다.\n6 . 범 위 ( R a n g e ) 관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다."
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-8.200000000000003",
    "href": "DAI/DAI03.html#출력된-값-8.200000000000003",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 8.200000000000003",
    "text": "출력된 값 &gt; 8.200000000000003\n7 . 사 분 위 수 범 위 ( Q u a r t i l e ) 전체 관측값을 “작은 순서로 배열” 하였을 때, 전체를 “사등분” 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n7 - 1 . 사 분 위 수 범 위 ( I n t e r q u a r t i l e R a n g e , I Q R )"
  },
  {
    "objectID": "DAI/DAI03.html#출력된-값-2.0250000000000057",
    "href": "DAI/DAI03.html#출력된-값-2.0250000000000057",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "출력된 값 > 2.0250000000000057",
    "text": "출력된 값 &gt; 2.0250000000000057\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\nimport pandas as pd"
  },
  {
    "objectID": "DAI/DAI03.html#자료의-입력",
    "href": "DAI/DAI03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "DAI/DAI03.html#평균-mean",
    "href": "DAI/DAI03.html#평균-mean",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n## 출력된 값 &gt; 100.04125 ##\n\n100.04125"
  },
  {
    "objectID": "DAI/DAI03.html#중앙값median",
    "href": "DAI/DAI03.html#중앙값median",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n## 출력된 값 &gt; 100.05 ##\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "DAI/DAI03.html#분-산-variance",
    "href": "DAI/DAI03.html#분-산-variance",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "DAI/DAI03.html#표준편차",
    "href": "DAI/DAI03.html#표준편차",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "DAI/DAI03.html#범위",
    "href": "DAI/DAI03.html#범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "DAI/DAI03.html#사분위수범위",
    "href": "DAI/DAI03.html#사분위수범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다.\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI04.html",
    "href": "DAI/DAI04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "Reporting Date: July. 4, 2024\n조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI04.html#자료의-입력",
    "href": "DAI/DAI04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "DAI/DAI04.html#표본상관계수",
    "href": "DAI/DAI04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "DAI/DAI04.html#산점도",
    "href": "DAI/DAI04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI05.html",
    "href": "DAI/DAI05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "Reporting Date: July. 8, 2024\n통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI05.html#사건의-확률",
    "href": "DAI/DAI05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "DAI/DAI05.html#근원사건",
    "href": "DAI/DAI05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "DAI/DAI05.html#확률의-법칙",
    "href": "DAI/DAI05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "DAI/DAI05.html#확률의-계산",
    "href": "DAI/DAI05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "DAI/DAI05.html#확률-법칙",
    "href": "DAI/DAI05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "DAI/DAI05.html#조건부-확률",
    "href": "DAI/DAI05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI06.html",
    "href": "DAI/DAI06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "Reporting Date: July. 11, 2024\n5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI06.html#확률변수",
    "href": "DAI/DAI06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "DAI/DAI06.html#이산확률분포",
    "href": "DAI/DAI06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "DAI/DAI06.html#이산확률변수의-평균과-표준편차",
    "href": "DAI/DAI06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "DAI/DAI06.html#두-확률분포의-결합분포",
    "href": "DAI/DAI06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "DAI/DAI06.html#공분산과-상관계수",
    "href": "DAI/DAI06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "DAI/DAI06.html#공분산",
    "href": "DAI/DAI06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "DAI/DAI06.html#상관계수",
    "href": "DAI/DAI06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "DAI/DAI06.html#두-확률변수의-독립성",
    "href": "DAI/DAI06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI07.html",
    "href": "DAI/DAI07.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "Reporting Date: July. 15, 2024 모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다.\n1 . 자 료 의 입 력 ## 교재 출처 최하단에 표시 ##"
  },
  {
    "objectID": "DAI/DAI07.html#출력된-값-0.8199789826230907",
    "href": "DAI/DAI07.html#출력된-값-0.8199789826230907",
    "title": "6장: 확률분포",
    "section": "출력된 값 > 0.8199789826230907",
    "text": "출력된 값 &gt; 0.8199789826230907"
  },
  {
    "objectID": "DAI/DAI07.html#해석-임의로-추출된-200명의-학생-중-색맹인-학생이",
    "href": "DAI/DAI07.html#해석-임의로-추출된-200명의-학생-중-색맹인-학생이",
    "title": "6장: 확률분포",
    "section": "해석: 임의로 추출된 200명의 학생 중 색맹인 학생이",
    "text": "해석: 임의로 추출된 200명의 학생 중 색맹인 학생이"
  },
  {
    "objectID": "DAI/DAI07.html#명-이하일-확률은-약-0.820-즉-약-82.0이다.",
    "href": "DAI/DAI07.html#명-이하일-확률은-약-0.820-즉-약-82.0이다.",
    "title": "6장: 확률분포",
    "section": "10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.",
    "text": "10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n3 - 1 . 이항분포의 기댓값과 표준편차 확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:\n4 . 초 기 하 분 포 ( Hypergeometric Distribution ) 유한한 모집단에서 비복원 추출 ( Sampling Without Replacement ) 을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인 ( FPC ) 이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다.\n결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다.\n5 . 포 아 송 분 포 ( Poisson Distribution ) 특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포.\n주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n119 구조대에 시간당 걸려오는 전화횟수 국내 발생하는 진도 4 이상 지진의 횟수\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\nfrom scipy import stats stats.poisson.cdf(10, 8)"
  },
  {
    "objectID": "DAI/DAI07.html#출력된-값-0.8158857925585467",
    "href": "DAI/DAI07.html#출력된-값-0.8158857925585467",
    "title": "6장: 확률분포",
    "section": "출력된 값 > 0.8158857925585467",
    "text": "출력된 값 &gt; 0.8158857925585467"
  },
  {
    "objectID": "DAI/DAI07.html#해석-임의로-추출된-200명의-학생-중-색맹인-학생이-1",
    "href": "DAI/DAI07.html#해석-임의로-추출된-200명의-학생-중-색맹인-학생이-1",
    "title": "6장: 확률분포",
    "section": "해석: 임의로 추출된 200명의 학생 중 색맹인 학생이",
    "text": "해석: 임의로 추출된 200명의 학생 중 색맹인 학생이"
  },
  {
    "objectID": "DAI/DAI07.html#명-이하일-확률은-약-0.816-즉-약-81.6이다.",
    "href": "DAI/DAI07.html#명-이하일-확률은-약-0.816-즉-약-81.6이다.",
    "title": "6장: 확률분포",
    "section": "10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.",
    "text": "10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\n참고용: Finite Population Correction Factor FPC 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI07.html#자료의-입력",
    "href": "DAI/DAI07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "DAI/DAI07.html#베르누이-시행",
    "href": "DAI/DAI07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "DAI/DAI07.html#이항분포",
    "href": "DAI/DAI07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "DAI/DAI07.html#초기하분포",
    "href": "DAI/DAI07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "DAI/DAI07.html#포아송분포",
    "href": "DAI/DAI07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI08.html",
    "href": "DAI/DAI08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "Reporting Date: July. 18, 2024\n6 장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI08.html#자료의-입력",
    "href": "DAI/DAI08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nRequirement already satisfied: numpy in c:\\users\\sinji\\anaconda3\\envs\\py310\\lib\\site-packages (2.2.5)"
  },
  {
    "objectID": "DAI/DAI08.html#연속확률분포",
    "href": "DAI/DAI08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "DAI/DAI08.html#확률밀도함수",
    "href": "DAI/DAI08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "DAI/DAI08.html#정규분포",
    "href": "DAI/DAI08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "DAI/DAI08.html#정규분포의-특성",
    "href": "DAI/DAI08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "DAI/DAI08.html#이항분포의-정규분포근사",
    "href": "DAI/DAI08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "DAI/DAI08.html#연속성수정",
    "href": "DAI/DAI08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "DAI/DAI08.html#정규분포가정의-조사",
    "href": "DAI/DAI08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI09.html",
    "href": "DAI/DAI09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "Reporting Date: July. 21, 2024\n주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI09.html#해석-c-d에서-동일한-난수들이-추출됨을-확인할-수-있다.",
    "href": "DAI/DAI09.html#해석-c-d에서-동일한-난수들이-추출됨을-확인할-수-있다.",
    "title": "9장: 표집분포",
    "section": "해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.",
    "text": "해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n( 2 ) 표본평균을 출력한다.\nimport numpy as np\nm = []\nnp.random.seed(1234) for i in range(100): sample = np.random.randint(0, 10, size = 5) m.append(np.mean(sample))\nm = np.array(m) print(m)\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\nimport matplotlib.pyplot as plt\nplt.hist(m, bins=7) plt.xlabel(‘m’) plt.ylabel(‘Frquency’) plt.title(‘Historam of m’)"
  },
  {
    "objectID": "DAI/DAI09.html#해석-정규분포와-유사한-종-모양-분포를-띄는-것을-통해",
    "href": "DAI/DAI09.html#해석-정규분포와-유사한-종-모양-분포를-띄는-것을-통해",
    "title": "9장: 표집분포",
    "section": "해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해",
    "text": "해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해"
  },
  {
    "objectID": "DAI/DAI09.html#정규분포에-가까우리라-예상할-수-있다.",
    "href": "DAI/DAI09.html#정규분포에-가까우리라-예상할-수-있다.",
    "title": "9장: 표집분포",
    "section": "정규분포에 가까우리라 예상할 수 있다.",
    "text": "정규분포에 가까우리라 예상할 수 있다.\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8 장에서 배운 정규확률그림을 그려본다.\nimport matplotlib.pyplot as plt import statsmodels.api as sm\nsm.qqplot(m, line=‘s’) plt.title(“Normal Q-Q plot”)"
  },
  {
    "objectID": "DAI/DAI09.html#해석-점들이-거의-직선상의-있으므로",
    "href": "DAI/DAI09.html#해석-점들이-거의-직선상의-있으므로",
    "title": "9장: 표집분포",
    "section": "해석: 점들이 거의 직선상의 있으므로",
    "text": "해석: 점들이 거의 직선상의 있으므로"
  },
  {
    "objectID": "DAI/DAI09.html#어느-정도-정규분포를-따른다고-할-수-있다.",
    "href": "DAI/DAI09.html#어느-정도-정규분포를-따른다고-할-수-있다.",
    "title": "9장: 표집분포",
    "section": "어느 정도 정규분포를 따른다고 할 수 있다.",
    "text": "어느 정도 정규분포를 따른다고 할 수 있다.\nMapo금빛나루 | | 공유 마당 (copyright.or.kr)\n참고용 블로그: 작은 숫자 특수문자 첨자 및 분수숫자 모음 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI09.html#자료의-입력",
    "href": "DAI/DAI09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "DAI/DAI09.html#통계량-s-t-a-t-i-s-t-i-c",
    "href": "DAI/DAI09.html#통계량-s-t-a-t-i-s-t-i-c",
    "title": "9장: 표집분포",
    "section": "2. 통계량 ( S t a t i s t i c )",
    "text": "2. 통계량 ( S t a t i s t i c )\n표본의 관측값들에 의하여 결정되는 양.\n표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데,\n이때 유념하여야 할 3가지 조건이 있다.\n( 1 ) 표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n( 2 ) 통계량의 값은 추출된 표본의 영향을 받는다:\n( 3 ) 다른 표본을 추출할 때마다 통계량의 값은 변한다:\n3 . 표 집 분 포 ( Sampling Distribution ) 통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n3 - 1 . 불 편 추 정 량 ( Unbiased Estimator ) 분포의 평균값이 추정하려는 모수와 일치하는 추정량.\n불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n3 - 2 . 편 의 추 정 량 ( Biased Estimator ) 분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다.\n이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다.\n따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n3 - 3 . 임 의 표 본 ( Random Sample ) 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn.\n위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n3 - 4 . 표 본 평 균 ( Sample Mean ) 모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:\n4 . 중 심 극 한 정 리 ( Central Limit Theorem, CLT ) 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이\n표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\nimport numpy as np\na = np.random.randint(0, 100, size=5) b = np.random.randint(0, 100, size=5)\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우 c = np.random.randint(0, 100, size=5)\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우 d = np.random.randint(0, 100, size=5)\nprint(“a :”, a) print(“b :”, b) print(“c :”, c) print(“d :”, d)"
  },
  {
    "objectID": "DAI/DAI09.html#통계량",
    "href": "DAI/DAI09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "DAI/DAI09.html#표집분포",
    "href": "DAI/DAI09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "DAI/DAI09.html#중심극한정리",
    "href": "DAI/DAI09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [18 79 31 65 75]\nb : [ 3  6  8 10 98]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI10.html",
    "href": "DAI/DAI10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "Reporting Date: July. 25, 2024\n추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "DAI/DAI10.html#자료의-입력",
    "href": "DAI/DAI10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "DAI/DAI10.html#통계적-추론",
    "href": "DAI/DAI10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "DAI/DAI10.html#모평균의-추정",
    "href": "DAI/DAI10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "DAI/DAI10.html#해석-p값이-크므로-귀무가설을-기각할-수-없음",
    "href": "DAI/DAI10.html#해석-p값이-크므로-귀무가설을-기각할-수-없음",
    "title": "10장: 통계적 추론",
    "section": "해석: p값이 크므로 귀무가설을 기각할 수 없음",
    "text": "해석: p값이 크므로 귀무가설을 기각할 수 없음"
  },
  {
    "objectID": "DAI/DAI10.html#평균키가-159cm와-통계적으로-유의한-차이가-없다고-본다",
    "href": "DAI/DAI10.html#평균키가-159cm와-통계적으로-유의한-차이가-없다고-본다",
    "title": "10장: 통계적 추론",
    "section": "→ 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다",
    "text": "→ 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고,\n이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\nxbar_n = np.mean(noise);print(xbar_n) # 평균\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\nn = noise.size;print(n) # 데이터 개수\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\npval = stats.norm.sf(np.abs(zval));print(pval) # p값"
  },
  {
    "objectID": "DAI/DAI10.html#해석-p값이-0.021로-0.05보다-작게-나왔으므로",
    "href": "DAI/DAI10.html#해석-p값이-0.021로-0.05보다-작게-나왔으므로",
    "title": "10장: 통계적 추론",
    "section": "해석: P—값이 0.021로 0.05보다 작게 나왔으므로",
    "text": "해석: P—값이 0.021로 0.05보다 작게 나왔으므로"
  },
  {
    "objectID": "DAI/DAI10.html#유의수준-5-에서-귀무가설을-기각할-수-있다.",
    "href": "DAI/DAI10.html#유의수준-5-에서-귀무가설을-기각할-수-있다.",
    "title": "10장: 통계적 추론",
    "section": "유의수준 5% 에서 귀무가설을 기각할 수 있다.",
    "text": "유의수준 5% 에서 귀무가설을 기각할 수 있다."
  },
  {
    "objectID": "DAI/DAI10.html#그러므로-평균교통소음정도가-60-보다-크다고-할-수-있다.",
    "href": "DAI/DAI10.html#그러므로-평균교통소음정도가-60-보다-크다고-할-수-있다.",
    "title": "10장: 통계적 추론",
    "section": "그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.",
    "text": "그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\nMapo금빛나루 | | 공유 마당 (copyright.or.kr) P Hat Symbol (p̂) (wumbo.net)\n참고용 블로그: 작은 숫자 특수문자 첨자 및 분수숫자 모음 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI10.html#모평균에-대한-검정",
    "href": "DAI/DAI10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "DAI/DAI10.html#모비율에-대한-추론",
    "href": "DAI/DAI10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI11.html",
    "href": "DAI/DAI11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "Reporting Date: July. 28, 2024\n표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI11.html#자료의-입력",
    "href": "DAI/DAI11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "DAI/DAI11.html#t-분포",
    "href": "DAI/DAI11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "DAI/DAI11.html#모평균에-대한-추론",
    "href": "DAI/DAI11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "DAI/DAI11.html#가설-검정",
    "href": "DAI/DAI11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "DAI/DAI11.html#신뢰구간과-양측검정의-관계",
    "href": "DAI/DAI11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "DAI/DAI11.html#모표준편차의-추론",
    "href": "DAI/DAI11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAI/DAI12.html",
    "href": "DAI/DAI12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "Reporting Date: July. 31, 2024\n두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "DAI/DAI12.html#통계용어",
    "href": "DAI/DAI12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "DAI/DAI12.html#두-개-의-독-립-표-본",
    "href": "DAI/DAI12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "DAI/DAI12.html#짝비교",
    "href": "DAI/DAI12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "DAI/DAI12.html#두-모비율의-차에-대한-추론",
    "href": "DAI/DAI12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다.\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "DAD/DAD6.html",
    "href": "DAD/DAD6.html",
    "title": "제 6장-두 모집단에 대한 비교",
    "section": "",
    "text": "Reporting Date: Setember. 18, 2024 두 모집단의 모평균, 모비율, 모분산의 차이에 대한 가설검증 문제를 다루고자 한다. (12장: 두 모집단의 비교와 이어지는 내용이다.)\n\n표본 평균을 추정하려면, 표본의 크기와 모분산을 고려해야 한다.\n[ 1 ] 두 모분산 σ12, σ 22 이 모두 알려져 있는 경우,\n두 모평균 차에 대한 “추정량” ⇨ “두 표본평균의 차” 통계적 추론을 위한 “준비물” ⇨ “추정량의 분포”\n이 분포는 다음과 같은 평균과 분산을 가진 정규분포를 따른다:\n표준화된 확률변수 Z는 표준정규분포 N(0, 1)를 따른다.\n[ 2 ] 두 모분산 σ12, σ 22 을 모두 모르는 경우, 표본의 크기를 고려하게 된다.\n표본의 크기가 충분히 큰 경우 ( 25 이상 )\n중심극한정리에 의해 모집단의 분포에 관계없이 x̄ 와 ȳ 가 근사적으로 정규분포를 따른다.\n두 모분산의 추정치인 표본분산 s₁², s₂² 를 고려한 통계량을 사용하여 검정을 수행한다.\n[ 3 ] 두 모집단이 알려져 있지는 않지만, 모분산이 동일한 것으로 가정할 수 있는 경우,\n다음과 같은 평균과 분산을 가지는 정규분포를 따른다:\n[1] 과 동일하다.\n공통분산 σ ² 의 합동추정량 (Pooled Variance)\n자유도 n ₁ + n ₂ – 2인 t-분포를 따른다.\n[ 4 ] 두 모분산이 서로 다른 경우, [3]번 식은 t-분포를 따르지 않는다.\n단, 아래와 같이 자유도를 수정할 경우, 근사적으로 t-분포를 따르게 된다.\n근사적으로 t-분포를 따르게 된다.\n수정된 자유도(df).\n독립표본에 의한 두 모평균의 비교:\n독 립 표 본 t – 검 정\n두 개의 서로 독립적인 집단의 평균을 비교하여 그 차이가 통계적으로 유의한지 판단하는 방법이다.\n사례 새로운 강의방식이 초등학생 독해력 향상에 도움이 되는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Reading.csv\"\nReading = pd.read_csv(url)\nReading.head()\n\n\n\n\n\n\n\n\nID\nGroup\nScore\n\n\n\n\n0\n1\nNew\n75\n\n\n1\n2\nNew\n80\n\n\n2\n3\nNew\n72\n\n\n3\n4\nNew\n77\n\n\n4\n5\nNew\n69\n\n\n\n\n\n\n\n가설검증을 결정하기 전에 데이터를 시각화한다.\n\nimport seaborn as sns  # 박스 플롯\nsns.boxplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n중위수와 같은 요인을 비교한 결과, 차이가 나타나므로 이를 근거로 검증을 진행할 수 있다.\n\n# 바이올린 플롯\nsns.violinplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n\n# 기술통계량\nReading.groupby('Group').Score.describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nGroup\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n8.0\n75.375\n4.373214\n69.0\n71.75\n76.0\n78.50\n81.0\n\n\nOld\n8.0\n69.125\n4.086126\n63.0\n67.25\n69.0\n71.25\n76.0\n\n\n\n\n\n\n\n양측검정 적용.\n\n# 그룹 나누기\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n# 양측검증:\n# 두 강의 방식에 차이가 있다. vs 차이가 없다.\n\nfrom scipy.stats import ttest_ind  # 독립 t-검정\nttest_ind(New.Score, Old.Score, equal_var = True)\n\n\n# T통계량: 그룹 간 평균 차이가 실제로 존재하는지를 나타내는 통계량.\n# 통계량이 클수록 차이가 있을 가능성이 높다.\n\n# [3]번 통계량: statistic=2.9536127902039953\n\n\n# 두 꼬리 검정에서의 p-값: pvalue=0.010470744188033123\n\n# 통상적으로 p-값이 0.05보다 작으면 귀무가설을 기각할 수 있다. \n# 즉, 두 강의 방식에 차이가 있다고 결론 내릴 수 있다.\n\nTtestResult(statistic=np.float64(2.9536127902039953), pvalue=np.float64(0.010470744188033123), df=np.float64(14.0))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\n# 단측검정:\n# 새로운 학습법이 더 효과적이다. vs 효과적이지 않다.\n\nstat, pval = ttest_ind(New.Score, Old.Score, equal_var = True)\nprint(\"P\", pval/2)\n\n# p-값이 0.0052로 유의수준 0.05보다 작으므로, 대립가설을 채택할 수 있다.\n\nP 0.005235372094016561\n\n\n단측검정과 등분산 가정 적용.\n\n# 단측검정\nfrom statsmodels.stats.weightstats import ttest_ind\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'pooled') # 등분산 가정 적용:\n          # 두 그룹 간의 분산이 동일하다고 가정\n\n(np.float64(2.9536127902039953),\n np.float64(0.005235372094016561),\n np.float64(14.0))\n\n\n단측검정과 이분산 가정 적용.\n\n# 단측검정\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'unequal') # 이분산 가정 적용:\n          # 두 그룹의 분산이 서로 다르다는 가정\n\n# [4]번 통계량: usevar= 'pooled' ⇨ 'unequal'\n# 14 ⇨ 13.935945095796395 (자유도가 실수로 바뀜)\n\n(np.float64(2.9536127902039953),\n np.float64(0.005256688626975243),\n np.float64(13.935945095796395))\n\n\n결론적으로, 새로운 강의방식이 초등학생 독해력 향상에 도움이 된다고 할 수 있다.\n대응표본에 의한 두 모평균의 비교:\n대응표본 t – 검정\n어떤 신발의 마모율을 비교할 때, 독립 표본 검정에 경우, 한 그룹의 사람이 왼쪽 신발을 신고, 다른 그룹의 사람이 오른쪽 신발을 신더라도 상관이 없다.\n하지만 대응 표본 검정은 동일한 사람이 왼쪽 신발과 오른쪽 신발을 모두 신어야 만 한다.\n각 쌍이 서로 연관되어 있으므로 두 신발을 신는 사람이 동일해야 하며, 표본의 수도 일치해야 한다.\n이는 마모율에 영향을 줄 수 있는 교락 요인(confounding factor), 즉 신발을 신는 사람의 특성 등을 배제하기 때문이다.\n그러므로, 대응 표본 검정은 같은 대상에 대한 실험 전후의 결과를 비교할 때 주로 사용된다.\n사례 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Paired.csv\"\nPaired = pd.read_csv(url)\nPaired.head()\n\n\n\n\n\n\n\n\nID\nPretest\nPosttest\n\n\n\n\n0\n1\n80\n82\n\n\n1\n2\n73\n71\n\n\n2\n3\n70\n95\n\n\n3\n4\n60\n69\n\n\n4\n5\n88\n100\n\n\n\n\n\n\n\n박스플롯 시각화 및 기술 통계량 출력.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Pretest와 Posttest에 대한 박스플롯 시각화\nsns.boxplot(data = Paired.iloc[:, [1, 2]], \n            orient = 'h') # 수평 방향\n\n# Pretest와 Posttest의 차이 계산 및 새로운 열(Diff) 추가\nPaired[\"Diff\"] = Paired.Pretest - Paired.Posttest \n                # = 교육 전 성적 - 교육 후 성적\n                # 교육이 효과가 있다면 교육 후 성적이 더 높을 것이므로\n                # 결과적으로는 변수 Diff의 값이 음수로 나와야 한다.\n\n\n\n\n\n\n\n\n두 변수에 대한 상자 그림\n\nPaired.iloc[:,1:4].describe()\n\n# 변수 Diff 평균(mean)이 -7.93이며\n# 실제로 그래프 상에서도 대부분의 개체에서 \n# 변수 Diff의 값이 0보다 작음을 볼 수 있다.\n\n\n# 표준편차(std)는 데이터의 산포도(변동성)를 측정하는 지표로, \n# 데이터가 평균으로부터 얼마나 떨어져 있는지를 나타낸다. \n\n# 표준편차는 항상 0 이상의 값을 가지며, 음수가 될 수 없다. \n# 이는 표준편차가 데이터 값의 차이를 제곱하여 계산하기 때문이다.\n\n\n\n\n\n\n\n\nPretest\nPosttest\nDiff\n\n\n\n\ncount\n15.000000\n15.000000\n15.000000\n\n\nmean\n70.266667\n78.200000\n-7.933333\n\n\nstd\n18.041487\n14.313829\n9.931671\n\n\nmin\n37.000000\n60.000000\n-25.000000\n\n\n25%\n59.500000\n67.000000\n-12.500000\n\n\n50%\n73.000000\n75.000000\n-7.000000\n\n\n75%\n82.000000\n90.500000\n-2.500000\n\n\nmax\n98.000000\n100.000000\n13.000000\n\n\n\n\n\n\n\n히스토그램 및 커널 밀도 추정(KDE) 시각화\n\nsns.distplot(Paired.Diff)\n\n# Seaborn의 최신 버전에서는 더 이상 지원되지 않으므로,\n# sns.histplot 또는 sns.kdeplot을 사용하는 것이 권장된다.\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_10492\\4004193321.py:1: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(Paired.Diff)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 히스토그램 그리기\nsns.histplot(Paired.Diff, \n             stat = 'density')  # y축을 밀도로 변경\n\n# KDE만 수정하기 위해 따로 그리기\nsns.kdeplot(Paired.Diff, \n            fill = True) # 음영 처리\n\nplt.xlim(-40, 30)     # x축 범위 설정\n\n\n\n\n\n\n\n\n양측검정 적용\n\n# ttest_rel에서 rel은 paired 또는 related를 의미한다.\n\n# 이 함수는 대응표본 t-검정을 수행하는 것으로, \n# 두 관련된 표본에 대한 평균의 차이를 비교하는 데 사용된다.\n\n\nfrom scipy.stats import ttest_rel\nttest_rel(Paired.Pretest, Paired.Posttest)\n\n# p-값이 0.0079(0.79%)로 0.05(5%)보다 작기 때문에 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다. \n# 이는 두 표본 간에 유의미한 차이가 있음을 의미한다.\n\nTtestResult(statistic=np.float64(-3.093705670004429), pvalue=np.float64(0.007930923229026533), df=np.int64(14))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\nstat, pval = ttest_rel(Paired.Pretest, Paired.Posttest)\nprint(\"one-sided p-value =\", pval/2)\n\n# 이 경우에도, p-값이 0.05보다 작으므로 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다.\n\none-sided p-value = 0.003965461614513267\n\n\n결론적으로 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있으며, 사후 테스트의 결과가 더 좋다고 할 수 있다.\n독립표본에 의한 두 모비율의 비교:\n피셔의 정확검정 Fisher’s Exact Test\n두 모비율에 대한 검정을 수행하기 위해 사용할 수 있는 대표적인 검정법은 두 독립된 이항분포의 비율에 대한 z-검정이다.\n사례: 현 정부에 대한 지지율이 성인 남녀별로 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Support.csv\"\nSupport = pd.read_csv(url)\nSupport.head()\n\n\n\n\n\n\n\n\nID\nGender\nYesNo\n\n\n\n\n0\n1\nMale\nNo\n\n\n1\n2\nFemale\nYes\n\n\n2\n3\nFemale\nNo\n\n\n3\n4\nFemale\nNo\n\n\n4\n5\nFemale\nNo\n\n\n\n\n\n\n\n이 데이터에 대한 2차원 분할표(빈도표) 작성하기.\n\nimport pandas as pd\nSupportTable = pd.crosstab(index = Support[\"Gender\"],\n                           columns = Support[\"YesNo\"])\n\nSupportTable\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n96\n104\n\n\nMale\n140\n110\n\n\n\n\n\n\n\n행 백분율 계산하기.\n\npd.crosstab(index=Support[\"Gender\"], columns=Support[\"YesNo\"],\n           normalize = \"index\") # 각 행의 합을 기준으로 비율을 계산\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n0.48\n0.52\n\n\nMale\n0.56\n0.44\n\n\n\n\n\n\n\n다음과 같은 교차 테이블(Cross Table)을 만들 수 있다.\n양측검증 적용.\n\nfrom scipy.stats import fisher_exact\nfisher_exact(SupportTable, \n             alternative = 'two-sided')\n             \n# 이 결과는 검정 통계량이 0.725이고 \n# p-값이 0.106(10.6%)이다.\n\n# 이는 일반적으로 사용되는 유의 수준 0.05(5%)에서 \n# 통계적으로 유의하지 않다는 것을 의미한다. \n\n# 결론적으로, 두 그룹(또는 변수) 간에 유의한 차이 또는 \n# 연관성을 찾지 못했다는 것을 나타낸다.\n\nSignificanceResult(statistic=np.float64(0.7252747252747253), pvalue=np.float64(0.10634531219761142))\n\n\n정규 근사 검정\n이항분포의 표본 크기 n이 충분히 크면, 이항분포는 정규분포로 근사할 수 있으며, 이를 정규 근사라고 한다. 일반적으로 n×p와 n×(1 − p)가 모두 5 이상이면, 정규분포로 근사할 수 있다고 간주한다.\n여기서 p 는 성공 확률이다.\n이러한 정규화된 변수를 제곱하면, 이는 카이제곱 분포를 따르게 된다.\n자유도가 1인 카이제곱 분포를 따른다.\n카이제곱검정(Chi-Square Test) 적용.\n\nfrom scipy.stats import chi2_contingency\nchi2_contingency(SupportTable)\n\n# 카이제곱 통계량: 2.54\n# 유의 수준이 일반적으로 0.05(5%)인 경우, \n# p-값이 0.111(11.1%)이므로 귀무가설을 기각할 수 없다.\n\n# 따라서 이 결과는 두 변수 간에 통계적으로 \n# 유의한 연관성이 없다고 결론지을 수 있다. \n\n# 즉, 이 교차표에 따르면 두 변수는 독립적이다.\n\nChi2ContingencyResult(statistic=np.float64(2.5395141968952935), pvalue=np.float64(0.1110289428837834), dof=1, expected_freq=array([[104.88888889,  95.11111111],\n       [131.11111111, 118.88888889]]))\n\n\n결론적으로, 현 정부에 대한 지지율이 성인 남녀별로 차이가 없다고 할 수 있다.\n대응표본에 의한 두 모비율의 비교:\n맥니머 검정\n맥니머 검정은 피셔의 정확검정이나 카이제곱 검정과 달리 대응 표본에 적용할 수 있는 검정이다. 이 검정은 대응 표본 t-검정과 유사하게 교락 효과를 제거하는 것이 중요하다.\n독립 표본의 경우, 한 사람이 A, B 제품 모두를 사용하지 않아도 무방하다. 그러나 대응 표본에서는 한 사람이 반드시 두 제품 모두를 사용해야 한다.\n사례: 정부에서 정책 발표 후 지지율에 변화가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Prepost.csv\"\nPrepost = pd.read_csv(url)\nPrepost.head()\n\n\n\n\n\n\n\n\nID\nPre\nPost\n\n\n\n\n0\n1\nYes\nYes\n\n\n1\n2\nNo\nNo\n\n\n2\n3\nYes\nNo\n\n\n3\n4\nNo\nNo\n\n\n4\n5\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nPrepostTable = pd.crosstab(index = Prepost[\"Pre\"], \n                           columns = Prepost[\"Post\"], \n                           margins = True, # 각 행과 열의 합계 추가\n                           margins_name = \"합계\")\nPrepostTable\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n18\n27\n45\n\n\nYes\n8\n67\n75\n\n\n합계\n26\n94\n120\n\n\n\n\n\n\n\n\npd.crosstab(index=Prepost[\"Pre\"], columns=Prepost[\"Post\"], \n            margins=True, margins_name=\"합계\", \n            normalize=\"all\") # 전체 데이터에 대한 비율 변환\n            \n# 정책 발표 이전 지지율(pre): 62.5%\n# 정책 발표 이후 지지율(post): 78.3%\n# 결과적으로 15.8%p가 상승하였음을 볼 수 있다.\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n0.150000\n0.225000\n0.375\n\n\nYes\n0.066667\n0.558333\n0.625\n\n\n합계\n0.216667\n0.783333\n1.000\n\n\n\n\n\n\n\n\n# pip install statsmodels\n\nfrom statsmodels.stats.contingency_tables import mcnemar\nprint(mcnemar(PrepostTable, \n              exact = True)) # 이항분포 기반의 정확 검정 방법\n              \n# 0.001(0.1%) &lt; 0.05(5%)\n\nprint(mcnemar(PrepostTable, \n              exact=False)) # 카이제곱분포를 사용한 근사 검정 방법\n              \n# 0.002(0.2%) &lt; 0.05(5%)\n\npvalue      0.0018782254774123432\nstatistic   8.0\npvalue      0.0023457869795667934\nstatistic   9.257142857142858\n\n\n결론적으로, 정부에서 정책 발표 전후 지지율에 변화가 있으며, 정책 발표 후에 지지율이 상승한 것으로 볼 수 있다.\n모분산의 동일성에 대한 검정:\nF – 검정 (F–test)\n가장 일반적인 검정 방법으로, 두 집단의 모분산이 동일한지 평가한다. 두 집단의 분산 비율을 계산하고, 이를 기반으로 F–분포를 사용하여 p–값을 구한다.\nReading 데이터의 모분산이 다른가?\n이전에 다루었던 Reading 데이터에 대해 분산의 동일성 검정을 위한 사용자 정의 함수를 작성하고, 가설검정을 수행하였다.\n\nimport pandas as pd\n\n# file_path = os.path.join('data', 'Reading.csv')\n# Reading = pd.read_csv(file_path)\n\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n\nimport numpy as np\nfrom scipy import stats\n\ndef F_test(x, y):\n    f = np.var(x, ddof = 1)/np.var(y, ddof = 1)\n    df1 = x.size -1 \n    df2 = y.size -1 \n    p = 2*(1-stats.f.cdf(f, df1, df2))\n    return f, p\n\nF_test(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\n(1.1454545454545453, np.float64(0.8624138071371459))\n\n\nBartlett’s Test\n\nfrom scipy import stats\nstats.bartlett(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nBartlettResult(statistic=np.float64(1.3291852026213666), pvalue=np.float64(0.24895022280539136))\n\n\nLevene’s Test\n\nstats.levene(New.Score, Old.Score)\n\n# 0.6(60%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nLeveneResult(statistic=np.float64(0.1978798586572438), pvalue=np.float64(0.6632376240724351))\n\n\n결론적으로, 두 집단의 모분산이 다르다고 말할 수 없다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD7.html",
    "href": "DAD/DAD7.html",
    "title": "제 6장-두 모집단에 대한 비교",
    "section": "",
    "text": "Reporting Date: Septemger. 28, 2024\n\n두 연속형 변수들 간의 연관성을 측정하는 데 사용되는 상관계수에 대해 다루고자 한다. (4장 두 변수 자료의 요약과 이어지는 내용이다.)\n두 변수의 공분산 구하는 과정\n\n각 데이터에서 평균을 빼서, 두 변수의 편차를 각각 구한다.\n\n각각의 편차 계산 과정\n\n두 변수 각각의 편차를 곱한 후 합산하는 방식이다.\n\n표준 공분산 계산 과정 더한 값에서 데이터 개수 n – 1 로 나눈 값이 표준 공분산이 된다.\n\n공분산의 값은 – ∞ ~ + ∞ 사이에 존재한다.\n\n공분산은 두 변수 간의 선형 관계를 나타내기 때문에, 그 값은 음의 무한대에서 양의 무한대까지의 범위를 가질 수 있다.\n공분산이 양수이면 두 변수는 같은 방향으로 움직이고, 음수이면 반대 방향으로 움직인다. 두 값의 편차를 구하고, 이를 공분산으로 계산하는 과정은 Z–분포와 일부 유사할 수 있다. Z–분포는 표준화를 기반으로 하고, 상관계수도 공분산을 표준화하는 과정이 있기 때문에 연결이 가능하다. 다만, Z–분포는 정규 분포의 표준화 개념을 나타내고, 상관계수는 두 변수 간의 관계를 표준화하여 설명하는 것이므로 서로 다른 개념이라는 점을 주의해야 한다. 공분산은 단위가 포함된 값이다.\n서로 다른 단위를 가진 변수들 간의 공분산은 크기나 해석에서 어려움이 있을 수 있다.\n예를 들어, 키와 몸무게의 공분산은 그 자체로 의미를 이해하기 어렵다. 이를 보다 쉽게 해석하기 위해, 공분산을 표준화하여 단위를 제거한 새로운 값인 상관계수를 사용한다.\n단위를 제거하기 위해 공분산을 각각의 표준편차로 나눈다.\n이 과정에서 각 변수의 단위가 없어지고, –1 ~ +1 사이의 범위를 갖는다. 결과적으로 이것을 피어슨 적률상관계수라고 한다.\n피어슨의 적률상관계수 (Pearson correlation coefficient)\n둘 다 피어슨 상관계수를 나타내지만, 일반적으로 상관계수는 피어슨의 적률상관계수를 뜻한다. 왼쪽은 표본을 통해 추정한 상관계수이고,오른쪽은 모집단의 상관계수를 의미한다. 표본상관계수 r을 이용하여,모상관계수 ρ에 대한 가설검증을 할 수 있다.\n귀무가설에 대한 검정통계량은 아래와 같은 통계량으로서 이는 귀무가설 하에서 자유도 n – 2인 T–분포를 따른다. 더 일반적인 상관계수에 대한 귀무가설 검정을 수행하려면, 피셔의 Z–변환을 이용할 수 있다.\n스피어만의 순위상관계수 (Spearman’s Rank Correlation Coefficient)\n두 변수 간의 비선형 관계를 측정하는 방법으로, 각 데이터의 순위를 사용하여 상관관계를 계산한다.\n이는 데이터가 서열형(순위) 혹은 비모수적일 때 유용하며, 피어슨 상관계수와 달리 데이터의 분포에 대해 가정하지 않는다.\n이 공식은 순위 차이의 크기가 클수록 상관관계가 약해진다는 점을 반영한다.\n두 순위 간 차이를 나타내므로 양수, 음수, 0 모두 가질 수 있다.\n사례: 소득과 지출 사이에는 상관관계가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Student.csv\"\nStudent = pd.read_csv(url)\nStudent.head()\n\n\n\n\n\n\n\n\nID\nAge\nIncome\nExpense\n\n\n\n\n0\n1\n25\n170\n67\n\n\n1\n2\n28\n177\n62\n\n\n2\n3\n20\n165\n53\n\n\n3\n4\n16\n150\n48\n\n\n4\n5\n19\n160\n58\n\n\n\n\n\n\n\n4개의 변수 중 Age(나이), Income(소득), Expense(지출) 변수를 사용하기로 한다.\n먼저, 산점도를 통해 데이터 분포의 전체적인 형태를 시각화한다.\n\nimport matplotlib.pyplot as plt\nplt.plot('Income', 'Expense', # 소득, 지출\n         'o', # 점의 형태: 원형\n         color = 'black', data = Student)\nplt.xlabel('Income')\nplt.ylabel('Expense')\n\nText(0, 0.5, 'Expense')\n\n\n\n\n\n\n\n\n\nplot 버전\n\nplt.scatter('Income', 'Expense', \n            data = Student)\nplt.xlabel('Income')\nplt.ylabel('Expense')\n\nText(0, 0.5, 'Expense')\n\n\n\n\n\n\n\n\n\nscatter 버전 위 산점도는 전반적으로 상승하는 경향을 보이며, 타원형 분포를 통해 양의 상관관계를 나타낸다. 신뢰구간, 회귀직선, 히스토그램이 추가된 산점도를 그린다\n\nimport seaborn as sns\nsns.jointplot(x = 'Income', y = 'Expense', \n              data = Student, kind = \"reg\")\n\n\n\n\n\n\n\n\n하늘색 영역은 신뢰구간이다.\n산점도 행렬 출력하기\n\nsns.pairplot(Student.iloc[:,1:4])\ng = sns.PairGrid(Student.iloc[:,1:4])\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n정방행렬의 형태로 피어슨의 상관계수 출력\nStudent.iloc[:,1:4].corr(method=‘pearson’)\n같은 행렬에서 상관계수가 1로 나오는 이유: 완벽한 양의 선형관계가 존재하기 때문이다. 전반적으로, 모든 변수들 간의 상관계수가 0.5 이상으로 비교적 높다.\n두 변수 수입과 지출 간의 모상관계수가 0인지를 검정한다.\n\nfrom scipy.stats import pearsonr\npearsonr(Student.Income, Student.Expense)\n\nPearsonRResult(statistic=np.float64(0.6812956535794542), pvalue=np.float64(0.0026006496946941993))\n\n\n사회과학 분야에서는 상관계수가 0.4 정도만 되어도 높은 값으로 간주된다. 표본 상관계수는 0.681로 비교적 높게 나타났으며, p–값은 0.002(0.2%)로 매우 낮아 귀무가설을 기각할 수 있다.\n따라서, 모상관계수가 0이 아니라고 결론지을 수 있다.\n상관계수와 p–값을 함께 출력한다.\n\n# !pip install pingouin\n\nimport pingouin as pg\nStudent.iloc[:,1:4].pairwise_corr(method='pearson').round(3)\n\n\n\n\n\n\n\n\nX\nY\nmethod\nalternative\nn\nr\nCI95%\np-unc\nBF10\npower\n\n\n\n\n0\nAge\nIncome\npearson\ntwo-sided\n17\n0.547\n[0.09, 0.81]\n0.023\n3.255\n0.653\n\n\n1\nAge\nExpense\npearson\ntwo-sided\n17\n0.530\n[0.07, 0.81]\n0.029\n2.735\n0.619\n\n\n2\nIncome\nExpense\npearson\ntwo-sided\n17\n0.681\n[0.3, 0.88]\n0.003\n19.505\n0.889\n\n\n\n\n\n\n\n상관계수행렬을 그래프로 그린다\n\ncorrMatrix = Student.iloc[:,1:4].corr(\n    method = 'pearson')\n\nimport seaborn as sns\nsns.heatmap(corrMatrix, \n            annot = True) # 그림 위 수치 표시\n\n\n\n\n\n\n\n\n편상관계수 (Partial Correlation Coefficient)\n두 변수 간의 상관관계를 분석할 때,특정 다른 변수의 영향을 통제(고정)한 후의 상관관계를 나타내는 지표. 이를 통해 변수들 간의 순수한 연관성을 파악할 수 있다.\n사례: 변수인 나이를 통제했을 때, 기능과 디자인에 대한 만족도 간에 상관관계가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satis.csv\"\nSatis = pd.read_csv(url)\nSatis.head()\n\n\n\n\n\n\n\n\nID\nAge\nSatis1\nSatis2\n\n\n\n\n0\n1\n28\n0\n70\n\n\n1\n2\n23\n0\n55\n\n\n2\n3\n26\n5\n65\n\n\n3\n4\n27\n5\n65\n\n\n4\n5\n25\n10\n60\n\n\n\n\n\n\n\n4개의 변수 중 ID를 제외한 나머지 변수를 사용하기로 한다.\n1 ~ 3번째 열까지의 상관계수를 구하고, 그 값을 소수점 셋째 자리로 반올림하여 출력\n\nSatis.iloc[:,1:4].corr().round(3)\n\n\n\n\n\n\n\n\nAge\nSatis1\nSatis2\n\n\n\n\nAge\n1.000\n0.698\n0.993\n\n\nSatis1\n0.698\n1.000\n0.703\n\n\nSatis2\n0.993\n0.703\n1.000\n\n\n\n\n\n\n\n모든 상관계수들이 0.6보다 큰 값을 가지고 있다.\n두 변수 간의 피어슨 상관계수와 p–값 출력\n\nfrom scipy.stats import pearsonr\npearsonr(Satis.Satis1, Satis.Satis2)\n\nPearsonRResult(statistic=np.float64(0.7030501468749641), pvalue=np.float64(0.0005449862495058279))\n\n\np–값이 0.0005(0.05%)이므로, 귀무가설을 기각할 수 있다.\n1 ~ 3열까지의 데이터에 대해 편상관계수 출력\n\nimport pingouin as pg\nSatis.iloc[:,1:4].pcorr().round(3)\n\n\n\n\n\n\n\n\nAge\nSatis1\nSatis2\n\n\n\n\nAge\n1.000\n-0.003\n0.987\n\n\nSatis1\n-0.003\n1.000\n0.115\n\n\nSatis2\n0.987\n0.115\n1.000\n\n\n\n\n\n\n\nAge – Satis2 (0.9): 매우 강한 양의 상관관계가 있으며, Age – Satis1 (-0.0): 0에 가까우므로, 거의 상관관계가 없다.\nSatis1 – Satis2 (0.1): 약한 양의 상관관계를 가지고 있다.\nAge를 공변량(covariate)으로 통제한 뒤, Satis1과 Satis2의 순수한 상관관계를 측정한다.\n\nSatis.partial_corr(x='Satis1', y='Satis2', covar='Age').round(3)\n\n\n\n\n\n\n\n\nn\nr\nCI95%\np-val\n\n\n\n\npearson\n20\n0.115\n[-0.36, 0.54]\n0.639\n\n\n\n\n\n\n\nCI95%는 상관계수 r에 대한 95% 신뢰구간이다. 즉, 실제 상관계수가 -0.36 ~ 0.54 사이에 있을 가능성이 95%라는 의미이다.\n상관계수가 음수와 양수일 가능성을 모두 포함하는 넓은 범위는 상관관계가 약하다는 것을 의미한다.\np–값은 0.6(60%)이므로, 두 변수 사이의 상관관계는 유의하지 않다고 결론지을 수 있다. 결론적으로, 나이가 많을수록 제품의 기능과 디자인에 대한 만족도가 모두 증가하는 경향이 있다. 앞서 나이가 중요한 요인으로 확인되었으므로, 이번에는 나이를 30세 이하와 30세 초과로 나누어 산점도를 그려보았다.\n\nSatis.loc[Satis.Age &lt; 30,\"AgeGroup\"] = \"Under30\"\nSatis.loc[Satis.Age &gt;= 30,\"AgeGroup\"] = \"Over30\"\nsns.jointplot(x = 'Satis1', y = 'Satis2', \n              data = Satis, hue = \"AgeGroup\")\n              # hue: 집단 변수(색을 구분하는 변수)\n\n\n\n\n\n\n\n\n연령대별로 구분할 경우, 두 변수의 상관성이 상당히 낮아짐을 볼 수 있다. 두 변수로 구분된 산점도가 전혀 겹치지 않고, 비선형적인 분포를 보이므로, 이들은 서로 연관성이 없다고 판단할 수 있다.\n연령대별에 대한 상관계수를 출력해본다.\n\nSatisUnder30 = Satis.loc[Satis.AgeGroup == \"Under30\",]\nSatisOver30 = Satis.loc[Satis.AgeGroup ==\"Over30\",]\n\nfrom scipy.stats import pearsonr\nprint(pearsonr(SatisUnder30.Satis1, SatisUnder30.Satis2))\nprint(pearsonr(SatisOver30.Satis1, SatisOver30.Satis2))\n\nPearsonRResult(statistic=np.float64(0.38797014489949266), pvalue=np.float64(0.38979066100659016))\nPearsonRResult(statistic=np.float64(0.17182016983164788), pvalue=np.float64(0.5746020827503785))\n\n\n상관계수의 값은 모두 낮고, 유의수준보다 p–값이 더 크므로, 두 변수 사이의 상관관계는 유의하지 않다고 결론지을 수 있다.\n신뢰도 분석 측정 도구가 일관되게 측정하고 있는지를 평가하는 과정.\nt와 e는 서로 독립(두 변수가 서로 대응되지 않는 것)이다.\n분산을 기반으로 두 변수 간의 관계 강도를 나타낸다.\n이 식은 피어슨 상관계수의 정의에서 유도된 것이며, 상관계수의 제곱을 나타내는 공식이다.\n식은 다음과 같이 분리된 형태로 표현된다.\n오차 e와 진정한 값 t가 독립적이라고 가정하면 Cov( e, t ) = 0이다.\n위 식을 대입한 뒤, 정리한다.\n따라서, 신뢰도는 참점수 t와 관측점수 X의 분산비(ratio of variance)로 해석할 수 있다.\n크론바흐의 알파 (Cronbach’s alpha)\n내적일치도(internal consistency)을 측정하기 위한 통계 지표. 여러 문항이 동일한 개념을 얼마나 잘 측정하는지를 나타낸다.\n위 계수는 0 ~ 1 사이의 값을 가지며, 값이 클수록 측정 도구의 신뢰도가 높다는 것을 의미한다.\n사례: 알파 계수 사용하여, 기업 구성원의 의식을 알아보기\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Ability.csv\"\nAbility = pd.read_csv(url)\nAbility.head()\n\n\n\n\n\n\n\n\nID\nQ01\nQ02\nQ03\nQ04\nQ05\nQ06\nQ07\nQ08\nQ09\nQ10\n\n\n\n\n0\n1\n4.0\n4.0\n4.0\n4.0\n4.0\n4\n3\n3.0\n3.0\n2.0\n\n\n1\n2\n4.0\n4.0\n3.0\n3.0\n4.0\n4\n2\n2.0\n4.0\n2.0\n\n\n2\n3\n5.0\n4.0\n3.0\n4.0\n4.0\n5\n2\n3.0\n2.0\n4.0\n\n\n3\n4\n3.0\n4.0\n2.0\n4.0\n4.0\n4\n2\n4.0\n2.0\n3.0\n\n\n4\n5\n4.0\n3.0\n4.0\n3.0\n2.0\n3\n2\n2.0\n3.0\n2.0\n\n\n\n\n\n\n\n위 설문조사는 10개의 문항이 존재하며, 대상자는 각 문항에 대해 1 ~ 5점까지의 값을 매길 수 있다.\n상사의 업무수행능력에 대한 신뢰도를 분석한다.\n\nAbility[[\"Q01\", \"Q02\", \"Q03\"]].corr()\n\nimport pingouin as pg\npg.cronbach_alpha(data = Ability[[\"Q01\", \"Q02\", \"Q03\"]])\n\n(np.float64(0.7351921832148215), array([0.697, 0.769]))\n\n\n알파 계수의 값이 0.735로서 내적일관성 신뢰도가 높다는 것을 알 수 있다. 상사와의 공적/사적 긴밀함에 대한 신뢰도 분석하기.\n\nAbility[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]].corr()\n\n\n\n\n\n\n\n\nQ04\nQ05\nQ06\nQ07\n\n\n\n\nQ04\n1.000000\n0.100787\n0.307348\n-0.351547\n\n\nQ05\n0.100787\n1.000000\n0.241432\n-0.273088\n\n\nQ06\n0.307348\n0.241432\n1.000000\n-0.350307\n\n\nQ07\n-0.351547\n-0.273088\n-0.350307\n1.000000\n\n\n\n\n\n\n\n4개의 문항에 대해 알파 계수를 계산하였다.\n\npg.cronbach_alpha(data = Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]])\n\n(np.float64(-0.2224494957901865), array([-0.386, -0.073]))\n\n\n알파 계수의 값이 –0.222로 계산되었으며, 이 값은 신뢰도가 음수라는 것을 나타낸다. 일반적으로 알파가 음수값을 가질 경우, 문항들이 서로 일관성이 없음을 의미한다. 이는 측정 도구가 잘못 설계되었거나, 각 문항이 서로 다른 개념을 측정하고 있음을 나타낼 수 있다.\n특히 문항 Q07이 나머지 문항들의 점수와 음의 상관을 가진다는 것을 알 수 있다.\n이 신뢰구간은 크론바흐의 알파 값의 신뢰성을 보여준다. 신뢰구간의 하한이 –0.386이고 상한이 –0.073인 것으로, 신뢰구간 내에 음수가 포함되어 있다.\n이는 크론바흐의 알파가 신뢰할 수 없는 상태임을 더욱 확증한다. 이 결과를 바탕으로 측정 도구의 문항을 재검토하거나 수정하는 것이 필요할 것으로 보인다.\nQ07 문항의 값을 6에서 빼는 방식으로 역전환을 시도한다.\n\nAbility[\"Q07_R\"] = 6 - Ability.Q07\nAbility[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]].corr()\n\n\n\n\n\n\n\n\nQ04\nQ05\nQ06\nQ07\n\n\n\n\nQ04\n1.000000\n0.100787\n0.307348\n-0.351547\n\n\nQ05\n0.100787\n1.000000\n0.241432\n-0.273088\n\n\nQ06\n0.307348\n0.241432\n1.000000\n-0.350307\n\n\nQ07\n-0.351547\n-0.273088\n-0.350307\n1.000000\n\n\n\n\n\n\n\n긍정적인 답변을 가진 경우, 다른 문항들과 반대되는 경향을 보여주기 때문이다.\n\npg.cronbach_alpha(data = Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07_R\"]])\n\n(np.float64(0.5876937990663734), array([0.532, 0.638]))\n\n\n알파 계수의 값이 0.587로서 비교적 만족스럽고, 각 문항들과 다음 문항들의 점수와의 상관계수도 모두 양의 부호를 가진다는 것을 알 수 있다.\n업무추진의 독자성에 대한 신뢰도 분석하기.\n\nAbility[[\"Q08\", \"Q09\", \"Q10\"]].corr()\n\n\n\n\n\n\n\n\nQ08\nQ09\nQ10\n\n\n\n\nQ08\n1.000000\n0.22047\n0.067734\n\n\nQ09\n0.220470\n1.00000\n-0.001290\n\n\nQ10\n0.067734\n-0.00129\n1.000000\n\n\n\n\n\n\n\n3개의 문항에 대해 알파계수를 계산한 것이다.\n\npg.cronbach_alpha(data = Ability[[\"Q08\", \"Q09\", \"Q10\"]])\n\n(np.float64(0.25112573908813685), array([0.144, 0.347]))\n\n\n알파 계수의 값이 0.251로 매우 낮게 나타나고 있다.\n따라서, 이들 문항이 하나의 동질적인 개념을 측정한다는 것을 신뢰할 수 없으며 세 문항의 합점수를 사용하는 것은 문제가 된다는 것을 알 수 있다. 사용자 정의 함수를 이용하여, 표준화 크론바흐 알파의 계산식을 구현하였다.\n\nimport numpy as np\ndef CronbachAlpha(df):\n    df_corr = df.corr()\n    N = df.shape[1]\n    rs = np.array([])\n    for i,col in enumerate(df_corr.columns):\n        sum_ = df_corr[col][i+1:].values\n        rs = np.append(sum_, rs)\n    mean_r = np.mean(rs)\n    cronbach_alpha = (N * mean_r) / (1 + (N-1) * mean_r)\n    return cronbach_alpha\nCronbachAlpha(Ability[[\"Q01\", \"Q02\", \"Q03\"]])\n\nnp.float64(0.7340559223240841)\n\n\n일반적으로 크론바흐의 알파가 0.7 이상이면 문항들 간의 일관성이 적절하다고 평가된다.\n\nCronbachAlpha(Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]])\n\nnp.float64(-0.25906347398614543)\n\n\n음수가 나온 경우, 문항들 간에 부정적인 상관관계가 있을 가능성이 높으며, 문항들이 제대로 된 일관성을 가지지 못하고 있음을 나타낸다.\n\nCronbachAlpha(Ability[[\"Q08\", \"Q09\", \"Q10\"]])\n\nnp.float64(0.24084556669287197)\n\n\n이 값은 상대적으로 낮은 문항들 간의 상관성을 나타낼 수 있으며, 이는 문항들이 일관성이 부족하다는 것을 의미한다.\n결과적으로 의미있는 신뢰도를 가진 문항은 Q01 ~ Q03 사이이다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD8.html",
    "href": "DAD/DAD8.html",
    "title": "제 8장-카이제곱 통계",
    "section": "",
    "text": "Reporting Date: October. 5, 2024\n질적변수의 분석에 널리 이용되는 다항분포, 카이제곱검정을 소개하고 교차표에 대한 기초적인 분석에 대해 다루고자 한다."
  },
  {
    "objectID": "DAD/DAD8.html#데이터의-분류",
    "href": "DAD/DAD8.html#데이터의-분류",
    "title": "제 8장-카이제곱 통계",
    "section": "01 데이터의 분류",
    "text": "01 데이터의 분류\n\n1. 양적 데이터\nQuantitative Data\n관찰 대상의 속성을 수치로 측정할 수 있는 데이터로, 덧셈과 뺄셈 등의 산술 연산이 가능하다.\n척도의 성격에 따라 구간 척도와 비율 척도로 구분된다.\n① 구간 척도 (Interval Scale)\n값들 간의 간격이 일정하다는 특성을 지니며, 절대적 영점이 존재하지 않는다.\n따라서 0은 속성의 부재(absence)를 의미하지 않으며, 덧셈과 뺄셈은 가능하지만 곱셈이나 나눗셈을 통한 비율 비교는 무의미하다.\n예: 섭씨(°C)와 화씨(°F) 온도. (0°C는 ’온도가 없음’을 의미하지 않음)\n② 비율 척도 (Ratio Scale)\n구간 척도의 모든 특성을 지니면서, 절대적 영점이 존재한다. 즉, 0이 속성의 완전한 부재를 의미하므로 사칙연산이 모두 가능하며 ‘두 배’, ’절반’과 같은 비율 비교가 가능하다.\n예: 연령, 키, 무게, 길이, 소득, 켈빈(K) 온도.\n\n\n2. 질적 데이터\nQualitative Data\n수치로 양을 측정하기보다는, 관찰 대상의 속성이나 범주를 나타내는 데이터이다.\n이러한 데이터는 주로 구분(classification)이나 서열(ordering)에 초점이 맞추어지며, 척도의 수준에 따라 명목척도와 순서척도로 구분된다.\n① 명목 척도 (Nominal Scale)\n범주 간에 순서가 존재하지 않으며, 단순한 구분만 가능하다. 각 범주는 상호 배타적이며, 수치적 크기나 간격 비교는 불가능하다.\n예: 성별, 출생지, 국적, 혈액형 등.\n명목척도 변수는 주로 카이제곱 검정(χ² test)이나 교차분석을 통해 변수 간의 독립성 또는 연관성을 분석한다.\n② 순서 척도 (Ordinal Scale)\n범주 간에 자연스러운 순서(rank)가 존재하지만, 인접한 범주 간 간격이 동일하다고 볼 수 없다.\n따라서 서열 비교는 가능하나, 차이의 크기를 정량적으로 해석하기는 어렵다.\n예: 만족도, 학점, 성적 등급 등.\n순서척도 변수 간의 연관성은 주로 순위상관계수(Spearman’s ρ, Kendall’s τ)를 이용해 평가할 수 있다."
  },
  {
    "objectID": "DAD/DAD8.html#카이제곱-분포",
    "href": "DAD/DAD8.html#카이제곱-분포",
    "title": "제 8장-카이제곱 통계",
    "section": "02 카이제곱 분포",
    "text": "02 카이제곱 분포\nChi-Square Distribution, χ²\n서로 독립인 표준 정규분포를 따르는 확률변수를 제곱한 뒤 그 합으로 정의되며, 항상 0 이상의 값을 갖는 확률분포이다. 분포 형태는 일반적으로 오른쪽 꼬리가 긴 비대칭을 보인다.\n\n1 . 자유도(df)\n작을수록 비대칭성이 크며, 평균, 중앙값, 최빈값의 차이가 뚜렷하게 나타난다. 커질수록 분포는 점점 대칭에 가까워지고 정규분포에 근사한다. 분포의 평균은 (df)와 같으며, 분산은 (2 × df)이다.\n\n\n2 . 통계적 활용\n카이제곱 분포는 주로 카이제곱 검정의 기반이 된다.\n예: 적합도 검정(goodness-of-fit test)과 독립성 검정(test of independence)에서, 귀무가설이 참일 경우 검정 통계량이 χ² 분포를 따른다고 가정한다.\n검정은 주로 분포의 오른쪽 꼬리 영역을 기준으로 수행되며, 계산된 통계량이 해당 영역에 속하면 귀무가설을 기각한다.\n따라서 χ² 분포는 데이터가 귀무가설로부터 얼마나 크게 벗어났는지를 평가하는 핵심 도구로 널리 활용된다."
  },
  {
    "objectID": "DAD/DAD8.html#카이제곱-적합도-검정",
    "href": "DAD/DAD8.html#카이제곱-적합도-검정",
    "title": "제 8장-카이제곱 통계",
    "section": "03 카이제곱 적합도 검정",
    "text": "03 카이제곱 적합도 검정\nChi-Square Goodness of Fit Test 피어슨의 카이제곱 통계량\n단일 범주형 변수에 대해, 실제로 관측된 빈도가 이론적으로 기대되는 빈도와 얼마나 일치하는지를 평가하기 위한 통계적 방법이다.\n즉, 주어진 표본이 특정한 이론적 분포인 균등분포, 정규분포, 이항분포 등을 얼마나 잘 따르는지를 검정하는 데 사용된다.\n이 검정은 동질성 검정이나 독립성 검정과 달리, 두 변수 간의 관계를 비교하는 것이 아니라\n한 변수의 분포 형태 자체가 이론적 가정과 일치하는지를 확인하는 데 초점을 둔다.\n로그 우도비 검정통계량 Log Likelihood Ratio Test Statistic\n우도비(Likelihood Ratio) 는 두 개의 통계 모형의 적합도를 비교하기 위한 지표이다.\n하나는 귀무가설(H₀)​하의 제한된 모형(restricted model), 다른 하나는 대립가설(H₁)하의 포괄적 모형(full model)이다.\n우도비 검정의 핵심은 두 모형의 최대우도(maximum likelihood)를 비교하여 대립가설이 데이터를 더 잘 설명하는지를 평가하는 것이며, 정의는 다음과 같다:\n(L(_0):) 귀무가설 H₀ 하에서의 최대우도 (L():) 대립가설 H₁ 하에서의 최대우도\n로그 우도의 개념 표본이 ({x_1, x_2, , x_n})이고, 모수 () 에 따른 확률밀도(또는 확률질량) 함수가 (f(x|))일 때, 우도 함수는 다음과 같이 정의된다.\n\\[L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta)\\]\n하지만 이 곱은 표본 크기가 커질수록 수치적으로 매우 작아지므로, 계산의 안정성을 위해 로그를 취한다.\n\\[\nl(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n\\]\n이를 로그우도(log-likelihood) 라고 하며, 개별 확률들의 로그값을 모두 더한 형태이다.\n로그 우도비 검정통계량 두 모형의 로그우도를 비교하여 검정통계량 (D)를 정의한다.\n\\[\nD = -2 \\log \\left( \\frac{L(\\theta_0)}{L(\\hat{\\theta})} \\right)\n\\]\n(D)의 값이 클수록 대립가설 하의 모형이 데이터를 더 잘 설명한다는 의미이며, 결과적으로 귀무가설을 기각할 가능성이 높아진다.\n분포 및 해석 표본의 크기가 충분히 크면, Wilks의 정리(Wilks’ Theorem) 에 따라 검정통계량 (D)는 df가 “두 모형의 추정 파라미터 개수 차이”인 카이제곱 분포(χ²) 를 따른다.\n따라서 유의수준 ()를 설정하고, 카이제곱 분포표에서 기준치(critical value)를 찾는다. 만약 계산된 (D)가 기준치보다 크면, 귀무가설(H₀)을 기각한다.\n\n1 . 멘델의 법칙\n멘델의 유전 법칙에 따르면, 특정 형질이 자손에게 전달될 때 일정한 분리비율(segregation ratio) 이 나타난다.\n단일 형질의 경우: 우성 대 열성 = 3 : 1, 두 형질이 동시에 관찰될 경우: 9 : 3 : 3 : 1의 기대비율(expected ratio) 이 예측된다. 그러나 실제 실험에서는 표본의 우연한 변동(sampling variation) 으로 인해 관찰된 비율이 이론적으로 기대되는 비율과 정확히 일치하지 않을 수 있다.\n이때, 관찰된 빈도와 기대빈도의 차이가 단순한 우연에 의한 것인지, 혹은 통계적으로 유의한 차이인지를 판단하는 방법이 바로 카이제곱(χ²) 적합도 검정이다.\n카이제곱 적합도 검정은 관찰빈도와 기대빈도의 차이를 정량적으로 평가하여, 그 차이가 유전법칙에서 제시된 이론적 비율과 통계적으로 일치하는지를 검정한다.\n따라서 이 검정은 멘델의 유전 법칙이 실제 실험 데이터와 부합하는지를 검증하는 핵심 통계적 도구로 사용된다.\n\n\n2 . 적합도 검정의 단계\n카이제곱 적합도 검정은 다음과 같은 절차로 수행된다.\n① 가설 설정 귀무가설(H₀): 관측된 비율은 이론적으로 기대되는 비율과 같다. 대립가설(H₁): 관측된 비율은 기대되는 비율과 다르다. 예: 완두콩의 우성:열성 비율이 3 : 1 이라는 멘델의 가설을 검정한다고 하자.\n② 기대도수 계산 총 표본 수에 각 범주의 기대 비율을 곱하여 기대도수를 구한다.\n예: 400개의 완두콩 샘플에서 3 : 1 비율을 예상한다면\n우성: (400 = 300) 열성: (400 = 100) ③ 카이제곱 통계량 계산 관측도수와 기대도수의 차이를 바탕으로 다음 공식을 사용한다.\n\\[\n\\chi^2 = \\sum \\frac{(관측도수 - 기대도수)^2}{기대도수}\n\\]\n④ 자유도(DF) 계산 자유도는 범주의 수 - 1 로 계산한다.\n예: 두 범주(우성, 열성)가 있을 때\n\\[DF = 2 - 1 = 1\\]\n⑤ 결과 해석 계산된 χ² 값을 자유도에 따른 임계값(critical value) 과 비교한다. 만약 (χ²_{} &gt; χ²_{})이면, 귀무가설을 기각한다. 반대로, 계산값이 임계값보다 작으면 귀무가설을 기각할 수 없으며, 이는 관측된 비율이 멘델의 법칙(3 : 1 비율)에 부합함을 의미한다.\n04 교차분석 이제 질적 변수 간의 연관성을 시각적·수치적으로 확인할 수 있는 도구인 교차표를 살펴본다.\n\n\n1 . 교차표\nContingency Table\n두 개 이상의 범주형 변수를 기준으로, 각 범주의 조합이 몇 번 나타나는지를 빈도로 정리한 표.\n예: 암의 종류와 성별을 교차하여 각 조합이 몇 번 발생하는지를 나타낼 수 있다.\n일반적으로 행(row)과 열(column)로 구성되며, 각 셀(cell)에는 해당 행과 열의 범주 조합에 해당하는 빈도수가 표시된다.\n차원은 행과 열의 범주 수에 따라 결정되며, (r)개의 범주를 가진 행과 (c)개의 범주를 가진 열이 있을 경우, 표의 크기는 (r×c) 가 된다.\n이러한 교차표는 변수 간 관계를 시각적으로 파악하고, 카이제곱 검정(χ² test)과 같은 통계적 방법을 통해 변수 간 독립성 여부를 평가하는 데 활용된다.\n교차표를 만드는 이유\n교차표는 두 개 이상의 범주형 변수 간 관계를 시각적으로 확인하는 도구이다. 이를 통해 각 범주 조합의 빈도를 정리하고, 변수 간 연관성을 탐색할 수 있다.\n변수 간 관계를 수치적으로 평가하기 위해 카이제곱 검정을 수행할 수 있다.\n카이제곱 검정은 관측 빈도(observed frequency)와 기대 빈도(expected frequency) 간의 차이를 기반으로, 변수 간 독립성 여부를 평가하는 통계적 방법이다.\n따라서 교차표는 단순히 빈도를 정리하는 역할을 넘어, 카이제곱 검정을 통해 변수 간 연관성을 정량적으로 검증하는 중요한 기초 자료로 활용된다.\n카이제곱 검정의 종류\n\n\n1 . 동질성 검정\nTest of Homogeneity\n서로 다른 그룹의 범주형 데이터 분포가 동일한지 여부를 평가하는 통계적 방법이다.\n예: 암 종류별로 성별 분포가 동일한지를 확인할 때 활용할 수 있다.\n가설 설정 귀무가설(H₀): 각 그룹의 분포는 서로 같다. 대립가설(H₁): 각 그룹의 분포는 서로 다르다.\n\n\n2 . 독립성 검정\nTest of Independence\n두 범주형 변수 간에 독립적인 관계가 성립하는지를 평가하는 통계적 방법이다.\n예: 암의 종류와 성별이 서로 독립적인지, 즉 암의 종류가 성별과 관련이 없는지를 확인할 때 활용할 수 있다.\n가설 설정 귀무가설(H₀): 두 변수는 서로 독립적이다. 대립가설(H₁): 두 변수는 서로 독립적이지 않다.\n\n\n3 . 검정 절차\n검정은 카이제곱 통계량을 기반으로 수행되며, 관측 빈도 (O_{ij}) 와 기대 빈도 (E_{ij}) 간의 차이를 비교하여 그룹 간 분포의 동일성을 평가한다.\n기대 빈도는 각 관측치가 행과 열의 비율대로 분포한다고 가정하여 계산된다.\n① 서로 다른 그룹의 범주형 데이터를 수집한다. ② 교차표를 작성한 뒤, 각 셀의 기대 빈도를 계산한다.\n③ 카이제곱 통계량을 다음과 같이 계산한다:\n\\[\n\\chi^2 = \\sum_{i=1}^{r}\\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\n(df = (r-1)(c-1)) ④ 설정한 유의수준 (α) 과 비교하여, 통계량이 임계값보다 크면 귀무가설(H₀)을 기각하고, 두 변수는 독립적이지 않다고 판단한다.\n그렇지 않으면 귀무가설을 채택하여, 두 변수가 독립적이라고 결론짓는다.\n동질성 검정은 독립성 검정과 유사하지만, 독립성 검정은 단일 표본에서 변수 간 관계를 평가하는 반면, 동질성 검정은 서로 다른 집단 간 분포의 동일성을 평가한다는 점에서 차이가 있다.\n관찰도수 Observed Frequency, O\n실제 데이터에서 측정된 빈도를 의미하며, 연구자가 수집한 자료를 기반으로 각 범주에 속하는 사례 수를 나타낸다.\n기대도수 Expected Frequency, E\n두 범주형 변수가 독립적일 경우, 각 셀에 기대되는 빈도를 의미한다.\n이는 관측된 데이터가 아닌 이론적 계산 값으로, 두 변수 간 관계가 없다고 가정할 때 각 범주에 속할 것으로 예상되는 사례 수를 나타낸다.\n기대도수는 다음 공식으로 계산할 수 있다:\n\\[\nE_{ij} = \\frac{(\\text{행 합계}_i) \\times (\\text{열 합계}_j)}{\\text{전체 합계}}\n\\]\n예: 음주 여부와 암 발생 관계 음주 여부와 암 발생 간의 관계를 조사할 때, 교차표의 각 셀은 다음과 같이 구성된다.\n음주자 중 암 발생자 수 음주자 중 암 미발생자 수 비음주자 중 암 발생자 수 비음주자 중 암 미발생자 수 관찰도수와 기대도수는 교차표 작성과 카이제곱 검정 수행에 활용되며, 변수 간 관계를 평가하는 핵심 자료가 된다.\n\n\n4 . 검정의 목적\n검정 통계량인 카이제곱 값(χ²)이 클수록 관측 빈도와 기대 빈도의 차이가 크다는 의미이며, 이는 두 변수 간 관계가 존재할 가능성을 시사한다.\n따라서 χ² 값이 유의미하게 크면 귀무가설(H₀)을 기각하고, 두 변수 간 관계가 있다고 판단한다.\n카이제곱 검정은 독립성 검정과 동질성 검정 모두에서 활용되며, 범주형 데이터의 관계를 분석하는 데 중요한 도구로 사용된다.\n\n\n5 . 충분조건\n카이제곱 검정을 적용하기 위해서는 다음과 같은 조건이 충족되어야 한다:\n① 기대도수\n각 셀의 기대도수는 5 이상이어야 한다. 기대도수가 5 미만인 셀이 전체 셀의 20%를 초과하지 않아야 한다. 이를 충족하지 못하면 검정 결과의 신뢰성이 저하될 수 있다.\n예: 연령대별 정치 선호도를 10점 척도로 분석할 경우, 일부 연령대에서 기대도수가 낮아 기준을 만족하기 어려울 수 있으며, 이 경우 연령을 상·중·하 등으로 그룹화해야 한다.\n② 표본 크기\n표본 크기가 지나치게 작으면 검정력(power)이 낮아져 제2종 오류(Type II error)의 발생 위험이 증가한다.\n따라서 실질적으로 의미 있는 차이를 높은 확률로 탐지하려면 충분히 큰 표본을 확보하는 것이 필요하다.\n③ 셀 간 균형\n범주 간 사례 수가 크게 불균형하면 일부 셀의 기대도수가 낮아지고, 검정 결과가 불안정해질 수 있다.\n조사 설계 단계에서 범주별 사례수를 균등하게 확보하는 것이 권장된다.\n④ 기대도수 부족 시 대처 방법\n범주 통합: 빈도가 낮은 범주를 합쳐 각 셀의 기대도수를 증가시킨다. 범주 재설정: 데이터를 적절히 재범주화하여 기대도수를 확보한다. 기타 범주 생성: 매우 작은 빈도를 가진 범주를 ’기타’와 같이 하나로 묶어 분석한다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD9.html",
    "href": "DAD/DAD9.html",
    "title": "제 9장 회귀분석",
    "section": "",
    "text": "Reporting Date: October. 15, 2024\n한 변수가 다른 변수에 의해 설명 및 예측 과정을 분석하는 회귀분석에 대해 논의하고자 한다."
  },
  {
    "objectID": "DAD/DAD9.html#기계-학습",
    "href": "DAD/DAD9.html#기계-학습",
    "title": "제 9장 회귀분석",
    "section": "01 기계 학습",
    "text": "01 기계 학습\nMachine Learning\n데이터 기반으로 패턴과 규칙을 학습하고, 그 학습된 모델을 활용해 미래 예측, 분류, 의사결정 등을 수행하는 알고리즘 및 방법론을 연구하는 인공지능(AI)의 하위 분야이다.\n기계학습은 3 가지의 유형으로 구분된다.\n\n1 . 지도 학습\nSupervised Learning\n입력 변수 X 와 정답에 해당하는 타깃 변수 Y 가 함께 주어졌을 때, X 로부터 Y 를 예측하는 모델을 학습하는 방법이다.\n학습 과정에서는 데이터에 포함된 정답을 활용하여 입력과 출력 간의 규칙을 찾고, 이를 기반으로 새로운 데이터에 대한 예측을 수행할 수 있다.\n크게 연속적인 값을 예측하는 회귀 문제와, 특정 범주를 예측하는 분류 문제로 나눌 수 있다.\n대표적인 알고리즘으로는 선형 회귀, 로지스틱 회귀, 결정 트리, K-최근접 이웃(KNN), 서포트 벡터 머신(SVM), 신경망 등이 있다.\n\n\n2 . 비지도 학습\nUnsupervised Learning\n정답(타깃 변수) 없이 입력 데이터 X 만을 가지고 데이터의 패턴, 구조, 분포를 학습하는 방법이다.\n주로 유사한 데이터들을 묶는 군집화, 데이터의 차원을 줄여 본질적인 특성을 추출하는 차원 축소기법이 사용된다.\n대표적인 예로는 장바구니 분석(연관 규칙 학습), 다차원 척도법(MDS), 주성분 분석(PCA), 요인 분석(FA) 등이 있다.\n\n\n3 . 강화 학습\nReinforcement Learning\n에이전트가 환경(Environment)과 상호작용하며, 각 상태(State)에서 취할 수 있는 행동(Action)을 선택한다.\n그 결과로 보상(Reward)을 받으면서, 장기적으로 누적 보상을 최대화할 수 있는 최적의 행동 방식을 학습하는 방법이다.\n정답 데이터가 주어지는 것이 아닌 시행착오를 통해 스스로 전략을 개선하는 것이 특징입니다.\n이러한 특성 덕분에 강화 학습은 로봇 제어, 게임 인공지능, 자율주행, 추천 시스템 등 순차적 의사결정 문제에 널리 활용된다.\n이 중 지도 학습에 속하는 회귀 모델을 중심으로 살펴보고자 한다.\n“회귀” 용어의 유래 19세기 후반, 영국의 통계학자 프랜시스 골턴은 유전학 연구에서 부모와 자녀의 키 상관관계를 조사하면서 처음으로 “회귀” 라는 용어를 사용하였다.\n그는 부모의 키가 평균보다 크거나 작은 경우, 자녀의 키가 평균값으로 되돌아가는 경향을 발견하였고, 이를 회귀라고 설명하였다. 즉, 자녀들의 키가 부모의 극단적인 키보다 평균에 가까워지는 현상을 의미한 것이다.\n이후 골턴의 연구를 바탕으로 칼 피어슨은 회귀 분석을 체계화하고 수학적으로 확장하였으며, 선형 회귀의 공식과 상관계수 개념을 발전시켜 오늘날 회귀는 통계학에서 중요한 분석 도구로 자리잡게 되었다."
  },
  {
    "objectID": "DAD/DAD9.html#회귀-모델의-정의",
    "href": "DAD/DAD9.html#회귀-모델의-정의",
    "title": "제 9장 회귀분석",
    "section": "02 회귀 모델의 정의",
    "text": "02 회귀 모델의 정의\n회귀 모델은 타겟 변수인 Y 를 예측하기 위해 입력 변수 X 간의 관계를 학습하는 지도 학습의 한 유형이다.\n회귀 모델은 목적과 특성에 따라 5 가지 유형으로 구분된다.\n\n1 . 종속 변수의 개수에 따른 분류\n일변량 회귀 Univariate Regression\n하나의 종속 변수 Y 를 예측하기 위해 입력 변수 X 와 Y 간의 관계를 모델링하는 방법이다.\n다변량 회귀 Multivariate Regression\n여러 개의 종속 변수 Y1, Y2, …, Ym 를 동시에 예측하며, 종속 변수 간의 상관관계와 입력 변수와의 관계를 함께 고려하여 고차원적 관계를 모델링하는 것을 목표로 한다.\n\n\n2 . 두 변수 간의 관계에 따른 분류\n선형 회귀 Linear Regression\n독립 변수와 종속 변수 사이의 관계가 선형적일 때 사용하는 모델로, 종속 변수를 독립 변수들의 선형 결합으로 표현하며 회귀선을 직선으로 모델링한다.\n비선형 회귀 Nonlinear Regression\n두 변수 간의 관계가 비선형일 때 사용되며, 다항 회귀나 곡선 형태의 수학적 함수를 통해 관계를 모델링한다.\n\n\n3 . 독립 변수의 개수에 따른 분류\n단순 회귀 Simple Regression\n하나의 독립 변수 X를 사용하여 종속 변수 Y 를 예측하는 방법이다.\n다중 회귀 Multiple Regression\n여러 독립 변수 X1, X2, …, Xp ​를 활용해 종속 변수 Y 를 예측하는 방법이다.\n현실적인 데이터 분석에서는 종속 변수에 영향을 미치는 요인이 여러 개 존재하는 경우가 많기 떄문에 다중 회귀가 널리 활용된다.\n\n\n4 . 분산과 공분산\n분산 Variance\n단일 변수의 산포 정도, 즉 값들이 평균으로부터 얼마나 흩어져 있는지를 나타낸다.\n회귀 분석에서 종속 변수 Y 의 분산은 데이터가 얼마나 퍼져 있는지를 보여주며, 분산이 작을수록 모델이 더 정밀한 예측을 할 가능성이 높다.\n공분산 Covariance\n두 변수 간의 선형적 관계를 나타내는 지표로, 두 변수가 함께 어떻게 변하는지를 측정한다.\n공분산이 양수이면 두 변수가 같은 방향으로 변하는 경향이 있고, 음수이면 반대 방향으로 변하는 경향이 있다.\n다만 공분산 값 자체는 단위의 영향을 받아 크기 비교가 어려워, 이를 표준화한 상관계수(Correlation Coefficient)가 자주 활용된다.\n\n\n5 . 로지스틱 회귀\nLogistic Regression\n종속 변수가 이진형(Binary)일 때 사용하는 회귀 기법이다. 이진형 변수의 예로는 성공/실패, 생존/사망, Yes/No 등이 있다.\n선형 회귀처럼 값을 직접 예측하는 것이 아니라, 특정 사건이 일어날 확률을 예측한다.\n예측된 확률은 0 과 1 사이의 값으로 표현되며, 이를 기준으로 특정 클래스로 분류할 수 있다.\n이를 위해 선형 회귀 모형을 기반으로 한 결과를 로그 오즈로 변환하고 로지스틱 함수를 적용하여 확률로 해석 가능하게 만든다."
  },
  {
    "objectID": "DAD/DAD9.html#단순-회귀분석",
    "href": "DAD/DAD9.html#단순-회귀분석",
    "title": "제 9장 회귀분석",
    "section": "03 단순 회귀분석",
    "text": "03 단순 회귀분석\nSimple Linear Regression\n하나의 독립 변수 X 와 하나의 종속 변수 Y 간의 선형적 관계를 분석하는 기법이다.\n주로 두 변수 사이의 직선적 관계를 파악하고, 이를 바탕으로 종속 변수를 예측하는 데 활용된다.\n예를 들어, 예약 건수와 판매량 간의 관계를 분석하거나, 공부 시간과 시험 성적 간의 관계를 분석할 때 적용할 수 있다.\n\n1 . 절편\n통계적으로 유의하지 않더라도 직선의 위치를 결정하는 수학적 요소이므로 회귀모형에는 일반적으로 포함하며, 실질적인 해석의 초점은 독립변수의 효과를 나타내는 기울기 계수에 맞추어진다.\n\n\n2 . 오차\n통계 분석에서 회귀모형은 항상 일정 수준의 오차를 전제로 한다. 만약 오차가 전혀 없다면 모든 데이터가 회귀직선 위에 위치하게 되지만, 실제 데이터에서는 관측값이 직선에서 벗어나기 때문에 오차가 발생한다.\n실무에서는 다중선형회귀와 로지스틱 회귀가 가장 많이 활용되며, 단순선형회귀는 주로 기초 개념 학습이나 기본 분석에 사용된다.\n단순선형회귀에서 중요한 점은 회귀계수를 추정하는 것으로, 추정된 값은 보통 헷(^) 기호를 붙여 나타낸다. 헷이 없는 기호는 모집단의 실제 모수를 의미한다.\n\n\n3 . 최소제곱법\nOrdinary Least Squares, OLS\n회귀분석에서 가장 널리 사용되는 회귀계수 추정 방법으로, 잔차(실제값과 예측값의 차이)의 제곱 합을 최소화하여 가장 적합한 회귀 직선을 구하는 기법이다.\n이를 통해 회귀 직선의 기울기(β₁)와 절편(β₀)을 추정할 수 있으며, 단순 선형 회귀의 경우 공식은 다음과 같다.\n기울기 공식\n절편의 공식\n\n\n4 . 회귀계수(기울기) 검정\n회귀 분석에서 중요한 요소는 기울기이다. 이는 독립 변수 X 가 종속 변수 Y 에 미치는 평균적 영향을 나타낸다. 이 계수가 통계적으로 유의미한지 검증하기 위해 가설 검정을 수행한다.\n가설 설정 Hypothesis Formulation\n귀무가설(H₀) : β = 0 (X 가 Y 에 영향을 주지 않는다) 대립가설(H₁) : β ≠ 0 (X 가 Y 에 유의한 영향을 준다)\n단순 선형 회귀에서는 회귀계수가 2 개(절편, 기울기)이므로 자유도는 n − 2 이다. 여기서 n 은 표본의 크기(데이터 포인트 수)이다.\n검정 통계량 Test Statistic\n회귀분석에서 검정 통계량(t - 통계량)은 회귀계수가 0 인지 여부를 검정하는 데 사용된다. 즉 독립변수와 종속변수 간의 관계가 유의미한지 검정할 수 있다.\n회귀분석에서 계수 추정치에 대한 검정은 주로 t-통계량을 활용하며, 이는 표본의 크기가 충분히 클 경우 정규분포에 근사하지만 원칙적으로는 자유도를 고려한 t - 분포를 따른다.\n이러한 검정을 위해 오차항은 평균이 0이고 분산이 일정하며, 서로 독립적으로 정규분포를 따른다는 가정이 필요하다.\n이 가정을 바탕으로 회귀계수 β 에 대한 가설검정과 신뢰구간이 설정되며, 연구자는 보통 귀무가설(계수=0)을 기각하고 대립가설(계수≠0)을 채택하기를 기대한다.\n기울기의 표준 오차 Standard Error of the Slope\n회귀계수(기울기) 추정치의 변동성 또는 신뢰도를 나타내는 값이다.\n즉, 동일한 데이터 표본에서 회귀분석을 반복할 경우, 추정된 기울기가 얼마나 변동할 수 있는지를 보여준다.\n표준 오차가 작을수록 기울기 추정치가 보다 정확하며, 회귀계수 검정과 신뢰구간 계산에 활용된다.\n잔차의 표준편차 Standard Error of the Estimate, σₑ\n회귀모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표로, 단순 선형 회귀에서는 추정할 회귀계수가 2 개(절편과 기울기)이므로 자유도는 n − 2 가 된다.\n이 값을 이용하여 회귀계수의 t − 통계량을 계산하고, 이를 기반으로 p − 값을 산출하여 귀무가설 기각 여부를 결정한다.\n일반적으로 p − 값이 0.05 보다 작으면 귀무가설을 기각하고, 독립변수가 종속변수에 유의미한 영향을 준다고 판단한다.\n단순 회귀분석을 통해 독립 변수와 종속 변수 간의 관계를 파악할 수 있으며, 최소제곱법(OLS)을 사용하여 회귀 직선을 추정한다.\n이후 회귀계수의 유의성을 검증하는 가설 검정과 자유도 계산을 통해, 모델이 통계적으로 신뢰할 만한지 판단하고 분석 결과를 해석할 수 있다.\n단순 회귀분석 사례 12개의 기업에 대해 1년 광고비와 매출액을 조사하여 얻은 것이다.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Sales.csv\"\nSales = pd.read_csv(url)\nSales.head()\n\n\n\n\n\n\n\n\nCompany\nAdver\nSales\n\n\n\n\n0\n1\n11\n23\n\n\n1\n2\n19\n32\n\n\n2\n3\n23\n36\n\n\n3\n4\n26\n46\n\n\n4\n5\n56\n93\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.jointplot(x = 'Adver', y = 'Sales',\n             data = Sales, kind = 'reg')\n\nimport statsmodels.formula.api as smf\nSalesFit = smf.ols(formula = 'Sales ~ Adver',\n                   data = Sales).fit()\nSalesFit.summary()\n\nC:\\Users\\sinji\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=12 observations were given.\n  return hypotest_fun_in(*args, **kwds)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.979\n\n\nModel:\nOLS\nAdj. R-squared:\n0.976\n\n\nMethod:\nLeast Squares\nF-statistic:\n455.5\n\n\nDate:\nThu, 04 Dec 2025\nProb (F-statistic):\n1.14e-09\n\n\nTime:\n13:05:43\nLog-Likelihood:\n-32.059\n\n\nNo. Observations:\n12\nAIC:\n68.12\n\n\nDf Residuals:\n10\nBIC:\n69.09\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2848\n2.889\n1.137\n0.282\n-3.153\n9.723\n\n\nAdver\n1.5972\n0.075\n21.343\n0.000\n1.430\n1.764\n\n\n\n\n\n\n\n\nOmnibus:\n0.879\nDurbin-Watson:\n2.470\n\n\nProb(Omnibus):\n0.644\nJarque-Bera (JB):\n0.379\n\n\nSkew:\n0.419\nProb(JB):\n0.828\n\n\nKurtosis:\n2.768\nCond. No.\n101.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n한편, 결정계수(R^2)는 회귀모형이 종속변수의 변동을 얼마나 설명하는지를 나타내지만, 값이 높다고 해서 항상 좋은 모델을 의미하는 것은 아니며 과적합 여부도 고려해야 한다.\n&lt;그림1&gt; 산점도와 회귀직선 데이터를 직관적으로 이해하기 위해 선형 관계를 가정하고 직선 형태의 모델을 사용하는 경우가 많지만, 실제 데이터가 비선형 관계를 보이면 비선형 모델을 적용하기도 한다. 데이터가 선형인지 비선형인지는 산점도를 통해 시각적으로 확인할 수 있으며, 이때 산점도는 독립변수와 종속변수가 모두 연속형일 때 의미가 있다. 한편, 범주형 변수는 산점도로 바로 표현할 수 없지만, 회귀모델에서는 더미 변수로 변환하여 포함시킬 수 있으므로, 범주형 변수 역시 분석에 반영할 수 있다.\n\nsns.lmplot(x = 'Adver', y = 'Sales', data = Sales)\nsns.regplot(x = 'Adver', y = 'Sales', data = Sales, lowess = True)\n\n\n\n\n\n\n\n\n반응변수에 대한 예측\n\npredictions = SalesFit.get_prediction()\npredictions.summary_frame(alpha = 0.05).round(3)\n\nSalesNew = pd.DataFrame({'Adver':[20, 30, 40]})\npredictions = SalesFit.get_prediction(SalesNew)\npredictions.summary_frame(alpha = 0.05).round(3)\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n35.228\n1.612\n31.636\n38.820\n25.961\n44.495\n\n\n1\n51.199\n1.185\n48.559\n53.840\n42.258\n60.140\n\n\n2\n67.171\n1.153\n64.601\n69.741\n58.251\n76.091"
  },
  {
    "objectID": "DAD/DAD9.html#잔차-분석",
    "href": "DAD/DAD9.html#잔차-분석",
    "title": "제 9장 회귀분석",
    "section": "04 잔차 분석",
    "text": "04 잔차 분석\nResidual\n실제 관측값과 회귀 모형에 의해 추정된 예측값의 차이.\n이는 데이터 점과 회귀직선 사이의 수직 거리를 의미한다. 회귀 모델이 데이터를 얼마나 잘 설명하는지를 나타내는 중요한 지표이며, 잔차가 작을수록 모델의 적합도가 높음을 의미한다.\n따라서 잔차 분석을 통해 회귀 직선이 데이터를 얼마나 잘 표현하는지, 그리고 모델 가정이 적절히 충족되는지를 평가할 수 있다.\n표준화잔차 standardized residuals\n표준화는 변수의 값을 평균 0 , 분산 1 로 변환하여 서로 다른 척도를 가진 변수를 비교 가능하게 만드는 과정이다.\n이를 통해 특정 변수가 값의 범위가 크다는 이유만으로 회귀분석에서 과도하게 영향을 미치는 문제를 완화할 수 있다.\n또한 잔차 분석 단계에서는 표준화 잔차나 학생화 잔차를 사용하여 이상치를 탐지 및 모형의 적합성을 진단하기도 한다.\n\nFitted = SalesFit.predict()\nResidual = SalesFit.resid # 순수한 자기 잔차\nRStandard = SalesFit.resid_pearson # 표준화잔차\npd.DataFrame({'Fitted':Fitted, \n              'Residual':Residual,\n              'RStandard':RStandard})\n\nfig, ax = plt.subplots(figsize = (10, 8))\nsns.scatterplot(x = Fitted, y = RStandard)\nax.axhline(y = 0)\n\n\n\n\n\n\n\n\n그러나 표준화는 잔차 간의 독립성을 보장하거나 분산 불균형 문제를 근본적으로 해결하지는 못한다.\n독립성 문제는 주로 데이터의 자기상관에서 기인하며, 이분산 문제는 가중회귀나 분산 안정화 변환과 같은 방법을 통해 보완해야 한다.\n따라서 표준화는 변수의 스케일을 맞추고 분석의 안정성을 높이는 데 중요한 역할을 하지만, 모든 회귀 진단 문제를 해결하는 방법은 아니라는 점을 유념해야 한다."
  },
  {
    "objectID": "DAD/DAD9.html#회귀-모델의-기본-가정",
    "href": "DAD/DAD9.html#회귀-모델의-기본-가정",
    "title": "제 9장 회귀분석",
    "section": "05 회귀 모델의 기본 가정",
    "text": "05 회귀 모델의 기본 가정\n회귀분석을 적용하기 위해서는 4 가지 기본 가정이 충족되어야 한다.\n\n1 . 선형성\nLinearity\n독립 변수와 종속 변수 간의 관계가 선형이어야 한다. 이는 산점도 등의 그래프를 통해 확인할 수 있다.\n\n\n2 . 정규성\nNormality\n오차의 분포가 정규분포를 따라야 한다. 이는 대부분의 데이터가 회귀선이라는 평균값 주변에 집중되어 있어야 함을 의미한다.\n\nimport numpy as np\nsns.distplot(RStandard, bins = 10)\n\nfrom scipy.stats import probplot\nprobplot(RStandard, plot = plt)\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_10776\\646685327.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(RStandard, bins = 10)\n\n\n((array([-1.58815464, -1.09814975, -0.78255927, -0.53069113, -0.30892353,\n         -0.101534  ,  0.101534  ,  0.30892353,  0.53069113,  0.78255927,\n          1.09814975,  1.58815464]),\n  array([-1.50086884, -1.04842015, -0.86297515, -0.42536736, -0.31286804,\n         -0.15710484,  0.07160403,  0.26692088,  0.31018475,  0.55989614,\n          1.15452596,  1.94447261])),\n (np.float64(1.014930164527946),\n  np.float64(3.64578479126661e-15),\n  np.float64(0.9869321023876501)))\n\n\n\n\n\n\n\n\n\n\n\n3 . 등분산성\nHomoscedasticity\n독립 변수의 값에 관계없이 오차의 분산이 일정해야 한다. 이 가정들이 만족되지 않으면,모델의 예측 성능이 저하되거나 편향된 결과를 초래할 수 있다. 따라서 회귀 분석을 수행할 때는 현실 문제에서 이러한 가정들이 만족되는지 먼저 확인해야 한다. 가정이 충족되면, 독립 변수와 종속 변수 간의 관계에 따라 적합한 회귀 모델 유형을 선택하여 분석을 진행하면 된다.\n\n\n4 . 독립성\nIndependence\n관측값들 간에는 독립성이 유지되어야 한다. 다중공선성일 경우, 각 독립 변수가 종속 변수에 미치는 영향을 개별적으로 평가하기가 어렵기 때문이다.\n\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(RStandard) # 기준: 표준화잔차\n\nimport statsmodels.api as sm\nfig = plt.figure(figsize = (12, 8))\nfig = sm.graphics.plot_regress_exog(SalesFit, 'Adver', fig = fig)\n\n\n\n\n\n\n\n\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD8.1.html",
    "href": "DAD/DAD8.1.html",
    "title": "제 8장-연관성의 측도",
    "section": "",
    "text": "Reporting Date: October. 5, 2025"
  },
  {
    "objectID": "DAD/DAD8.1.html#연관성의-측도",
    "href": "DAD/DAD8.1.html#연관성의-측도",
    "title": "제 8장-연관성의 측도",
    "section": "01 연관성의 측도",
    "text": "01 연관성의 측도\nAssociation Measures\n두 변수 간 관계의 정도와 강도를 파악할 때 사용되는 통계적 지표를 총칭한다.\n특히 명목형(범주형) 변수의 경우, 단순히 유의성 검정(p-value)만으로는 관계의 강도를 알 수 없으므로, 파이계수(ϕ), 분할계수(C), 크라머의 V 등 다양한 연관성 측도를 활용한다.\n연관성 척도 중 일부 지표는 순서형 변수에 특히 적합하다(예: 스피어만 상관계수).\n즉, 단순히 “관련이 있는가? 를 넘어”관련이 있다면, 그 강도는 어느 정도인가?“를 정량적으로 평가하는 것이 목적이다.\n앞서 코호트 및 사례 대조 연구에서는 오즈비(OR)를 통해 사건 발생과 노출 간의 연관성을 평가하였다.\n이번 장에서는 이를 확장하여, 범주형 변수 전반에서의 연관성 강도를 정량적으로 측정하는 대표적 지표들을 살펴본다.\n1 . 파이계수 Phi Coefficient\n두 명목형 변수 간의 관계의 강도를 측정한다. 주로 2×2 교차표에 사용되며, 상관계수(Pearson’s r)와 유사한 해석이 가능하다.\n값의 범위는 (-1 ≤ ϕ ≤ +1)이다.\n0 : 독립적 관계 +1: 완전한 양의 관계 −1: 완전한 음의 관계 \\[\\phi = \\frac{ad - bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\\]\n여기서, a, b, c, d는 교차표의 각 셀 값이다.\n2 . 분할계수 Contingency Coefficient, C\n2×2 이상 교차표에서 변수 간 연관성을 평가하는 지표로, 파이계수의 확장으로 이해 가능할 수 있다.\n값의 범위는 (0 ≤ C &lt; 1)이다.\n0: 독립적 관계 1에 가까울수록 강한 의존 관계(강한 연관성) 단, 교차표의 크기가 커질수록 C의 최대값이 1 보다 작아지므로 지표 간 비교 시 주의가 필요하다.\n3 . 크라머의 V Cramér’s V\nn×m 형태 교차표에서 변수 간 연관성을 표준화된 형태로 측정한다.\n파이계수의 일반화된 형태이며, 교차표의 크기 차이를 보정하여 표준화된 연관성 강도를 제공한다.\n값의 범위는 (0 ≤ V ≤ 1)이다.\n0: 독립적 관계 1: 완전한 연관 관계 \\[V = \\sqrt{\\frac{\\chi^2}{n \\cdot \\min(k-1, r-1)}}\\]\nχ²: 카이제곱 통계량 n: 총 샘플 수 k: 행 개수, r: 열 개수\n[요약 비교표] 지표 적용 교차표 형태 값의 범위 관계 방향 표준화 여부 비고 파이계수 (ϕ) 2×2 −1 ~ +1 가능 X 상관계수와 유사 분할계수 (C) 2×2 이상 0 ~ 1 미만 불가능 X 표 크기 커지면 1 미만 크라머의 V n×m 0 ~ 1 불가능 O 표준화된 강도 비교 가능\n02 명목형 변수들의 연관성 앞 장에서는 파이계수(ϕ), 분할계수(C), 크라머의 V 등을 통해 범주형 변수 간의 관계 강도를 정량적으로 측정하였다.\n이번 장에서는 그중에서도 명목형 변수 간의 예측력 기반 연관성을 나타내는 지표인 람다(Lambda)를 살펴본다.\n1 . 개념 개요 람다는 두 명목형 변수 간의 관계를, 한 변수를 이용해 다른 변수를 얼마나 정확히 예측할 수 있는가로 표현한다.\n즉, “A 변수를 알고 있을 때 B의 예측 오류가 얼마나 감소하는가” 를 통해 변수 간의 연관성을 측정한다.\n람다는 두 가지 형태로 구분된다.\n비대칭 람다 (Asymmetric Lambda): 한 변수를 기준으로 다른 변수를 예측할 때 사용 대칭 람다 (Symmetric Lambda): 두 변수의 관계를 방향성 없이 측정할 때 사용\n2 . 비대칭 람다 Asymmetric Lambda\n한 변수를 기준으로 다른 변수를 예측할 때의 오류 감소 비율을 계산한다. 즉, 기준 변수(행 또는 열)의 정보가 주어졌을 때 예측 오류가 얼마나 줄어드는가를 측정한다.\n\\[\\lambda = \\frac{E_1 - E_2}{E_1}\\]​​\n(E_1:)​ 기준 변수의 정보를 사용하지 않았을 때의 예측 오류 (E_2:)​ 기준 변수의 정보를 사용했을 때의 예측 오류 λ가 0이면 두 변수 간 연관성이 없으며, λ가 1에 가까울수록 기준 변수를 통해 다른 변수를 더 정확히 예측 가능하다는 의미다.\n3 . 대칭 람다 Symmetric Lambda\n두 변수 간의 관계를 방향성 없이, 즉 서로 동등하게 평가하는 방법이다. 비대칭 람다를 각각 계산한 후 평균을 취하여 구한다.\n\\[\\lambda_s = \\frac{(\\lambda_{A|B} + \\lambda_{B|A})}{2}\\]\n이 값이 클수록 두 변수는 상호 예측력이 높아 강한 연관성을 가짐을 의미한다.\n03 순서형 변수의 연관성 측정 MH 카이제곱을 사용하여 연관성을 분석할 수 있다. 두 순서형 변수 간의 관계를 측정하는 데 유용한 방법입니다.\n감마 계수\nGamma Coefficient\n순서형 변수에서 감마 계수를 사용하여 두 변수 간의 단조 관계를 분석할 수 있다. 감마 계수는 일치쌍과 비일치쌍의 수를 통해 계산됩니다.\n일치쌍: 지위가 높을 때 만족하는 경우 (내가 세운 가설) 비일치쌍: 지위가 높을 때 불만족하는 경우 (가설과 반대되는 것.)\n감마 계수는 -1에서 +1 사이의 값을 가진다. +1은 완전한 양의 일치, -1은 완전한 음의 일치를 의미합니다. 0은 관련성이 없음을 의미한다.\n코드: 학력의 따른 비율 상관계수 서로의 영향이 높지 않음, 약간의 영향 두 순서형 변수의 선형적 연관성 검정 수치적으로는 났지만 연관성은 있다. 감마계수 계산은 파이썬에서 제공을 하지 않는다.\n심프슨의 역설\nSimpson’s Paradox\n제3의 변수가 두 변수 간의 관계에 영향을 미칠 수 있다는 개념이다. 즉, 제3의 변수가 없을 경우에는 관계가 보였던 두 변수 간의 연관성이, 제3의 변수를 포함하면 역으로 나타날 수 있습니다.\n예를 들어, 흡연과 폐암 간의 관계를 분석할 때, 연령대라는 변수에 따라 두 변수 간의 관계가 다르게 해석될 수 있습니다.\n심프슨의 역설을 방지하기 위해서는 분석 시 제3의 변수를 항상 고려하는 것이 중요합니다.\n남여의 대학원 합격율이 전체에서 10% 차이가 남 이에 대해 버클리 대학원 관계자들은 합격율을 전공 별로 변수를 봄. 오히려 합격율이 남성이 더 높았음, 이는 지원자의 비가 8:1로 차이가 났기 때문.\nIQ와 알코올의 관계에서 저연령과 고령령 변수를 넣으니 고지능자가 덜 술을 마신다는 것.\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD8.2.html",
    "href": "DAD/DAD8.2.html",
    "title": "제 8장-오즈비",
    "section": "",
    "text": "Reporting Date: October. 5, 2024\n오즈비와 연구 설계에 대해 다루고자 한다."
  },
  {
    "objectID": "DAD/DAD8.2.html#오즈비",
    "href": "DAD/DAD8.2.html#오즈비",
    "title": "제 8장-오즈비",
    "section": "01 오즈비",
    "text": "01 오즈비\nOdds Ratio, OR\n앞서 다룬 카이제곱 검정과 로그 우도비 검정은 범주형 변수 간의 통계적 유의성을 판단하는 데 초점을 두었다.\n본 장에서는 그 관계의 크기(효과 크기) 를 수치화하는 지표인 오즈비(OR) 를 다룬다.\n두 집단 간 사건 발생 가능성의 상대적 비교를 제공하며, 특히 2×2 분할표 분석과 로지스틱 회귀분석에서 널리 사용된다. 위험비(Risk Ratio)와 유사하지만, 사건이 발생할 확률 대비 발생하지 않을 확률(odds)의 비율을 사용한다.\n\n1 . 정의 및 기본 공식\n두 집단(노출 vs 비노출)에 대해 사건(또는 성공)의 발생 여부를 2×2 분할표로 나타낼 수 있다.\n사건 발생 (Yes) 사건 미발생 (No) 노출 그룹 (Exposed) a b 비노출 그룹 (Non-exposed) c d 각 집단의 오즈(Odds)는 사건 발생 확률을 사건 미발생 확률로 나눈 값이다.\n노출 집단의 오즈: ()​ 비노출 집단의 오즈: ()​ 따라서, 오즈비는 두 오즈의 비율로 정의된다.\n\\[\n\\mathrm{OR} = \\frac{a/b}{c/d} = \\frac{a \\cdot d}{b \\cdot c}\n\\]\n해석 규칙 ( &gt; 1:) 노출이 사건 발생 확률을 증가시킴(위험 증가) ((OR&lt;1:) 노출이 사건 발생 확률을 감소시킴(보호 효과) ( = 1:) 노출과 사건 발생 간 차이 없음\n\n\n2 . 로지스틱 회귀에서의 의미\n로지스틱 회귀모형에서 종속변수가 이진일 때, 회귀계수 ()는 로그 오즈에 대한 영향력을 나타낸다.\n\\[\n\\log\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots\n\\]\n따라서 변수 (X_j)가 1 단위 증가할 때 오즈는 지수함수적으로 변하며, 그 비율이 바로 오즈비이다.\n\\[\n\\mathrm{OR} = e^{\\beta_j}\n\\]\n(e^{_j})​가 1 보다 크면 해당 변수의 증가가 사건의 오즈를 e^{_j}배로 증가시킨다.\n예: (_j = 0.69)이면 (e^{0.69})으로, 오즈가 2배로 증가함을 의미.\n주의: 로지스틱 회귀에서 보고되는 오즈비는 조건부(다른 공변량을 고정한 상태) 효과이며, 단변량 오즈비와 다를 수 있다.\n\n\n3 . 비교 연구에서의 활용 사례\n비교연구(예: 케이스-컨트롤, 코호트 연구)에서는 오즈비가 두 집단 간 상대적 위험을 평가하는 주요 지표로 사용된다.\n예: 흡연(노출)과 폐암(사건)의 관계를 조사할 때\n각 그룹의 사건·비사건 수를 집계하여 오즈를 계산한다. 오즈비로 비교하여 흡연과 폐암의 연관성 크기를 정량화한다. ( &gt; 1)이면 흡연자가 비흡연자보다 폐암 발생 오즈가 더 높음.\n코호트 연구에서는 비율(RR)과 오즈비가 다를 수 있음에 유의한다.\n사건 발생률이 충분히 낮을 때((&lt;10%)범위)에는 OR ≈ RR이지만, 발생률이 높을 경우 OR은 RR보다 과장된 효과를 나타낼 수 있다.\n\n\n4 . 수학적 범위와 해석상의 주의점\n오즈비의 값 범위: (0 &lt; &lt; )\n(=0:) 실무적으로는 사건이 전혀 발생하지 않는 경우(이론적 경계) ( :) 사건이 항상 발생하는 경우(이론적 경계) 중심값은 1이며, 1에서 멀어질수록 효과 크기가 큼을 의미한다.\n범주 순서에 따른 영향 분할표에서 기준 집단을 바꾸면 오즈비는 역수((1/))가 된다. 이는 관계의 방향성만 달라질 뿐, 강도 자체는 동일하다.\n노출 집단을 기준으로 계산한 OR이 3이라 가정 시, 비노출 집단을 기준으로 계산한 OR은 ()​이다. 따라서 결과 해석 시 기준 집단(레퍼런스)을 명확히 기술해야 한다.\n\n\n5 . 적용 범위의 일반화\n오즈비는 이진 결과 전반 (사건/비사건, 성공/실패, 긍정/부정)에 적용 가능하다.\n로지스틱 회귀뿐 아니라 교차표 분석, 환자군 비교, 약물효과 평가 등에서 표준화된 비교 지표로 활용되며,\n특정 집단이 다른 집단보다 성공할 오즈가 얼마나 큰가(혹은 작은가)가 주 관심사이다."
  },
  {
    "objectID": "DAD/DAD8.2.html#코호트-연구",
    "href": "DAD/DAD8.2.html#코호트-연구",
    "title": "제 8장-오즈비",
    "section": "02 코호트 연구",
    "text": "02 코호트 연구\nCohort Study; [전향적 연구]\n두 집단(예: 흡연자 vs 비흡연자)을 시간에 따라 추적하여 사건 발생(예: 질병) 여부를 비교하는 연구 설계이다.\n장점 신뢰성: 후향적 연구보다 자료의 신뢰성이 높다. 높은 정확도: 시간에 걸쳐 직접 데이터를 수집하므로 관찰 오차가 적다. 단점 긴 추적 시간: 연구 기간이 길고 참여자 추적이 어려울 수 있다. 선정 바이어스(Selection Bias): 특정 집단이 과대 또는 과소 대표될 수 있음 (예: 비흡연자가 모집 과정에서 과소 대표되는 경우, 결과 해석 시 주의 필요.)\n\n1 . 비율 차이 및 비교 지표\n코호트 연구에서 두 집단 간 사건 발생을 비교할 때 대표 지표는 다음과 같다.\n① 상대 위험비(RR, Relative Risk)\n두 집단의 사건 발생 확률을 직접 비교한 비율 사건 발생 확률을 직접 사용하므로 직관적 해석이 용이하다.\n\\[\n\\mathrm{RR} = \\frac{P(\\text{사건 | 노출})}{P(\\text{사건 | 비노출})}\n\\]\n예: 흡연자의 폐암 발생 확률 ÷ 비흡연자의 폐암 발생 확률\n② 오즈비(OR, Odds Ratio)\n사건 발생 오즈를 비교한 비율 상대 비율보다 큰 값이 나올 수 있으며, 사건 발생률이 높을수록 RR과 차이가 커진다.\n\\[\n\\mathrm{OR} = \\frac{a/b}{c/d}\n\\]​\n2×2 분할표 분석과 로지스틱 회귀에서 상대적 위험을 정량화할 때 유용하다.\n③ 상대 위험비와 오즈비의 비교\n지표 의미 특징 상대 비율(RR) 실제 사건 발생 비율 비교 사건 발생 확률을 직접 사용, 직관적 오즈비(OR) 사건 발생 오즈 비율 비교 상대 비율보다 큰 값이 나올 수 있음; 사건 발생 확률이 높을수록 RR과 차이 확대 코호트 연구에서는 RR과 OR을 함께 고려해야 연구 결과를 정확히 해석할 수 있다. 특히 로지스틱 회귀에서 계산되는 OR은 조정된(조건부) 상대 위험으로, 단순 RR과 다를 수 있음을 이해해야 한다.\n\n\n2 . 연구 활용 및 해석\n코호트 연구에서 흡연과 폐암 관계를 분석할 때, OR과 RR을 함께 고려하면 연구 결과를 보다 종합적으로 이해할 수 있다.\n연구 설계 및 분석 과정에서 오즈비를 적절히 해석하는 것이 중요하며, 상대 비율과 비율 차이와 함께 결과를 해석해야 정확한 결론을 도출할 수 있다."
  },
  {
    "objectID": "DAD/DAD8.2.html#사례-대조-연구",
    "href": "DAD/DAD8.2.html#사례-대조-연구",
    "title": "제 8장-오즈비",
    "section": "03 사례 대조 연구",
    "text": "03 사례 대조 연구\nCase-Control Study; [후향적 연구]\n이미 질병이 발생한 집단(사례군)과 그렇지 않은 집단(대조군)을 비교하여, 과거 노출 상태와 질병 발생 간의 관계를 분석한다.\n\n1 . 집단 정의\n구분 설명 예시 사례군 (Case Group) 연구에서 질병이 발생한 사람들 폐암 환자 대조군 (Control Group) 질병이 없는 사람들 폐암에 걸리지 않은 일반인 연구에서는 이들 집단의 과거 노출 상태(예: 흡연 여부)를 조사하여, 질병과 노출 간의 연관성을 평가한다.\n장점 과거 노출 상태를 조사하므로 전향적 연구보다 연구 기간이 짧음 사례군을 충분히 확보할 수 있어 효율적(희귀 질병 연구에 적합) 단점 후향적 설계 특성상 상대 위험도(RR) 산출 불가 선택 편의(Selection Bias): 사례군과 대조군 선정 방식에 따라 결과가 달라질 수 있음 회상 오차(Recall Bias): 참여자가 과거 행동을 정확히 기억하지 못해 정보가 왜곡될 수 있음\n\n\n2 . 오즈비 활용\n사례 대조 연구는 사건 발생률이 불명확하므로, 상대 위험비(RR) 대신 오즈비(OR)를 사용하여 두 집단 간 노출 상태를 비교하고 상대적 위험을 추정한다.\n2×2 분할표 예시 흡연 비흡연 사례군 (폐암 환자) a b 대조군 (일반인) c d\n\n\n3 . 연구 활용 및 해석\n사례 대조 연구는 질병 발생 원인과 노출 요인 간 관계 분석에 적합하며, 데이터 수집 시 선택 편의와 회상 오차를 최소화하는 절차 필요하다.\n오즈비를 통한 상대적 위험 추정이 핵심이며, 해석 시 코호트 연구의 RR과 달리 사건 발생률을 직접 반영하지 않음을 명확히 해야 한다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD09.1.html",
    "href": "DAD/DAD09.1.html",
    "title": "제 9장-다중 회귀",
    "section": "",
    "text": "Reporting Date: September. 07, 2025 다중 회귀분석에 대해 다루고자 한다."
  },
  {
    "objectID": "DAD/DAD09.1.html#다중-회귀분석",
    "href": "DAD/DAD09.1.html#다중-회귀분석",
    "title": "제 9장-다중 회귀",
    "section": "01 다중 회귀분석",
    "text": "01 다중 회귀분석\n현실의 사회적 또는 자연적 현상을 설명할 때, 반응 변수 (y) 의 변화를 단일 설명 변수만으로 충분히 설명할 수 있는 경우는 거의 없다.\n따라서 여러 개의 설명 변수를 적절히 선택하고, 이들의 함수로 반응 변수를 설명하면 보다 정확한 예측과 분석이 가능하다.\n이러한 상황을 다루기 위해 사용하는 방법이 다중 선형 회귀 모형이며, 일반적인 수식으로는 다음과 같이 표현된다:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\n\\]\n여기서 추정해야 할 모수(parameters)는 p + 1개의 회귀계수 _0, _1, , _p​이며, _i​는 오차항을 나타낸다.\n\n1 . 오차항 가정\n회귀분석의 타당성을 위해 오차항 ε 은 다음과 같은 가정을 만족해야 한다:\n\\[\n{E}[\\varepsilon] = 0, \\quad \\mathrm{Var}(\\varepsilon) = \\sigma^2\n\\]\n기댓값은 0이며, 분산은 일정(등분산성) 서로 다른 관측치의 오차항은 서로 독립적 반응 변수 y의 전체 변동은 회귀모형과 오차항으로 분해 가능 이러한 변동 분해를 제곱합의 분할이라고 하며, 식으로 표현하면 다음과 같다:\n\\[\n\\text{SST} = \\text{SSR} + \\text{SSE}\n\\]\n\n\n1 . 전체제곱합\nTSS, SST, Total Sum of Squares\n반응변수 y 의 총 변동을 나타낸다.\n이는 각 관측값이 전체 평균으로부터 얼마나 떨어져 있는지를 제곱하여 모두 합한 값이다. 따라서 y 의 전체 변동량을 측정하는 지표라고 할 수 있다. 전체제곱합의 자유도는 n − 1 이다.\n\n\n2 . 회귀제곱합\nSSR, Sum of Squares due to Regression SSM, Sum of Squares due to Model\n회귀모형에 포함된 설명변수들이 반응변수의 변동을 얼마나 설명하는지를 나타낸다. 즉, 평균으로부터의 변동 중에서 회귀모형이 설명할 수 있는 부분이다. 회귀제곱합의 자유도는 설명변수의 개수 p 이다.\n\n\n3 . 잔차제곱합\nSSE, Sum of Squares due to Error\n회귀모형이 설명하지 못하는 변동으로, 실제값과 회귀모형이 예측한 값 간의 차이를 제곱하여 합한 값이다.\n즉, 오차항에 의한 변동을 의미한다. 잔차제곱합의 자유도는 n − p − 1 이다.\n총제곱합(SST, Total Sum of Squares): 반응 변수 yyy의 전체 변동량을 나타내며, 각 관측값이 평균으로부터 얼마나 떨어져 있는지를 제곱하여 합한 값입니다. 자유도는 n−1n-1n−1입니다. 회귀제곱합(SSR, Sum of Squares due to Regression / SSM, Sum of Squares due to Model): 회귀모형이 반응 변수 변동 중 얼마를 설명할 수 있는지를 나타냅니다. 자유도는 설명 변수의 개수 p이다. 잔차제곱합(SSE, Sum of Squares due to Error): 회귀모형으로 설명되지 않는 변동으로, 실제값과 예측값 간의 차이를 제곱하여 합한 값입니다. 자유도는 n−p−1n - p - 1n−p−1입니다. 즉, 반응 변수 y의 총 편차는 회귀모형에 의해 설명된 편차와 오차항에 의한 편차로 나뉘게 되며, 이는 다음과 같이 해석할 수 있다:\n\\[\n\\underbrace{\\text{총 편차}}_{\\text{SST}} = \\underbrace{\\text{회귀로 설명된 편차}}_{\\text{SSR}} + \\underbrace{\\text{잔차}}_{\\text{SSE}}\n\\]\n또한 각 제곱합을 자유도로 나눈 값을 평균제곱합(Mean Squares, MS)이라 하고, 이 값들을 요약한 표를 분산분석표(ANOVA table)라고 한다.\nANOVA 테이블 변동 요인(Source) 제곱합(SS) 자유도(df) 평균제곱합(MS) F-값 회귀(Regression, 모형) SSR p = k-1 MSR = ​ F = ​ 오차(Error, 잔차) SSE n − p − 1 MSE = ​\n전체(Total) TSS n − 1\nF 통계량은 회귀모형이나 분산분석에서 모형이 설명하는 변동(설명된 변동)과 잔차 변동(설명되지 않은 변동)을 비교하여, 모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표이다. 값이 클수록 모형의 설명력이 상대적으로 높다고 해석할 수 있다.\n이는 전체 변동 중 모형이 설명하는 비율인 결정계수와도 연결된다. 일반적으로 1에 가까울수록 모형이 데이터를 잘 설명한다고 본다.\n다만, 단순히 F 통계량이나 R2 값의 크기만으로는 모형의 유의성을 단정할 수 없다. 자유도와 표본 크기, 변동의 구조에 따라 “얼마나 크면 충분히 큰가”라는 절대적 기준이 달라지기 때문이다.\n따라서 F 통계량과 함께 p-값을 확인하는 것이 필수적이다. p-값이 유의수준(예: 0.05)보다 작으면, “모든 집단의 평균이 같다” 혹은 “회귀계수가 모두 0이다” 라는 귀무가설을 기각할 수 있으며, 모형이 통계적으로 유의함을 의미한다.\n반대로 p-값이 크면 귀무가설을 기각할 증거가 부족하다는 뜻이다. 이 경우 반드시 모형을 “처음부터 다시” 만들어야 한다는 것은 아니며, 변수 선택, 데이터 수집, 가정 검토 등의 추가 작업이 필요할 수 있다.\n다중회귀분석에서는 p 개의 설명변수가 반응변수 y 를 설명하는 데 유의하게 기여하는지를 확인할 필요가 있다.\n이를 위해 귀무가설 H₀ : β₁ = β₂ = ⋯ = βp = 0 를 세우고, 다음과 같은 검정통계량을 사용한다:\n이 검정통계량은 귀무가설이 참일 때 자유도 (p, n − p − 1)를 갖는 F − 분포를 따른다.\n일원 분산분석 One-way ANOVA table\n다중 회귀분석의 ANOVA 표와 일원 분산분석(One-way ANOVA) 표는 기본적으로 동일한 통계적 원리 위에서 구성된다.\n두 분석 모두 전체 변동(총제곱합, SST)을 설명 가능한 부분과 설명 불가능한 부분으로 분해하고, 이를 토대로 F-검정을 수행하여 모형 또는 집단 간 차이가 유의한지 판단한다는 점에서 연결된다.\n다중 회귀에서는 총제곱합을 회귀제곱합(SSR)과 잔차제곱합(SSE)으로 분해하며, 이는 설명변수가 종속변수를 얼마나 잘 설명하는지 평가하는 과정이다.\n일원 분산분석에서는 총제곱합을 집단 간 제곱합(SSB)과 집단 내 제곱합(SSE)으로 분해하는데,\n이는 집단 평균 간 차이가 통계적으로 유의한지 검정하는 과정이다. 이러한 연결점은 일원 분산분석이 다중 회귀분석의 특수한 형태라는 점에서 명확해진다.\n즉, 범주형 독립변수(집단)를 더미 변수로 변환하여 회귀 모형에 포함하면, 일원 분산분석은 다중 회귀분석으로 표현될 수 있다.\n따라서 두 분석을 나란히 이해하는 것은 회귀분석과 분산분석의 수학적·논리적 일관성을 보여주며, 분석자가 다양한 데이터 구조를 동일한 틀에서 해석할 수 있도록 돕는다.\n그럼에도 불구하고 두 분석은 활용 목적과 해석에서 차이를 가진다.\n다중 회귀분석은 여러 개의 독립변수가 종속변수에 미치는 개별적·동시적 효과를 추정하고, 각 회귀계수에 대한 유의성 검정을 통해 변수가 기여하는 정도를 해석한다.\n반면, 일원 분산분석은 주로 집단 평균 차이라는 단일한 질문에 초점을 두며, 개별 집단 간 차이를 사후검정(Post-hoc test)을 통해 추가적으로 분석한다.\n결과적으로 회귀분석의 ANOVA 표는 모형 전체의 설명력을 평가하는 과정에서 필요하고, 일원 ANOVA 표는 집단 간 평균 차이가 존재하는지를 평가하는 과정에서 필요하다.\n즉, 두 방법론은 수학적 구조를 공유하지만, 회귀는 예측과 변수 효과 추정에 강점이 있고, ANOVA는 집단 비교에 특화되어 있다는 점에서 차별화된다.\n따라서 연구자가 어떤 질문을 던지는지에 따라 적절한 분석 방법을 선택해야 하는 것이 중요하다.\n변동 요인(Source) 제곱합(SS) 자유도(df) 평균제곱합(MS) F-값 Between Groups (SSB, 집단 간) SSB k − 1 MSB = SSB / (k − 1) F = MSB / MSE Within Groups (SSE, 집단 내) SSE N − k MSE = SSE / (N − k) Total (SST) SST N − 1\n다중 회귀분석 사례 700명의 고객을 대상으로 어떤 제품에 대해 조사하려 얻은 것이다.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satisfaction.csv\"\nSatisfaction = pd.read_csv(url)\nSatisfaction.head()\n\n\n\n\n\n\n\n\nID\nY\nX1\nX2\nX3\nX4\nGender\nAge\nGender1\nAge1\nAge2\nAge3\nAge4\n\n\n\n\n0\n1\n5\n5\n5\n6\n5\n1\n3\n1\n0\n0\n1\n0\n\n\n1\n2\n5\n5\n5\n5\n5\n1\n5\n1\n0\n0\n0\n0\n\n\n2\n3\n5\n5\n6\n5\n5\n2\n5\n0\n0\n0\n0\n0\n\n\n3\n4\n5\n6\n6\n5\n6\n1\n2\n1\n0\n1\n0\n0\n\n\n4\n5\n5\n5\n6\n5\n5\n1\n5\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nX1: 다자인 만족도 X2: 사용 편리성 만족도 X3: 성능 만족도 X4: 고장 및 견고성 만족도 Y: 구입 의향\n설문 데이터 분석에서의 접근 설문을 통해 얻은 만족도 및 구입 의향 데이터는 보통 순위형(ordinal) 데이터로 수집된다.\n예를 들어 ” 매우 만족 ~ 매우 불만족 ” 과 같은 리커트 척도는 순서가 존재하지만, 각 항목 간 간격이 동일하다고 보장되지 않는 특성이 있다.\n가전제품과 같은 대기업의 고객 만족 조사에서는 위과 같은 변수를 설정할 수 있다.\n이때 연구의 초점은 Y(구입 의향)을 종속변수로 두고, 네 가지 만족도 요인이 이에 얼마나 영향을 미치는지를 분석하는 것이다.\n분석 절차는 보통 다음과 같이 진행된다.\n① 상관계수 확인 각 독립변수(X1 ~ X4)와 종속변수(Y) 간의 관계를 파악한다. 데이터가 순위형일 경우 피어슨 상관계수보다는 스피어만 상관계수 같은 방법이 더 적절할 수 있다.\n\n# 피어슨의 상관계수\nSatisfaction.iloc[:, 1:6].corr()\n\nfrom scipy.stats import pearsonr\n\npearsonr(Satisfaction.Y, Satisfaction.X1)\npearsonr(Satisfaction.Y, Satisfaction.X2)\npearsonr(Satisfaction.Y, Satisfaction.X3)\npearsonr(Satisfaction.Y, Satisfaction.X4)\n\nPearsonRResult(statistic=np.float64(0.20687830951579425), pvalue=np.float64(3.321873797880896e-08))\n\n\n\n② 결정계수 확인\n단순 상관관계에서는 상관계수 r 의 제곱값 r² 이 결정계수로 해석될 수 있으며, 이는 종속변수의 변동 중 독립변수가 설명하는 비율을 의미한다.\n여러 독립변수가 있을 경우에는 다중회귀분석을 통해 전체 결정계수 R² 를 산출한다.\n\nimport statsmodels.formula.api as smf\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction).fit()\nprint(SatisfactionFit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.141\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     28.56\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           5.38e-22\nTime:                        16:29:03   Log-Likelihood:                -1174.4\nNo. Observations:                 700   AIC:                             2359.\nDf Residuals:                     695   BIC:                             2382.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.2918      0.347      3.719      0.000       0.610       1.974\nX1             0.2352      0.058      4.077      0.000       0.122       0.349\nX2             0.1666      0.066      2.543      0.011       0.038       0.295\nX3             0.2309      0.068      3.421      0.001       0.098       0.363\nX4             0.0585      0.050      1.168      0.243      -0.040       0.157\n==============================================================================\nOmnibus:                      147.199   Durbin-Watson:                   1.804\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              482.307\nSkew:                           0.987   Prob(JB):                    1.86e-105\nKurtosis:                       6.556   Cond. No.                         75.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n회귀분석 결과에서는 결정계수 R² = 0.141로, 전체 독립변수들이 구입 의향(Y)의 변동 중 약 14.1%만 설명하고 있다. 즉, 설명력이 높지는 않지만 전혀 의미 없는 수준도 아니다.\n전체 모형의 F 통계량 (F ≈ 28.56, p &lt; 0.001)은 “모형이 유의미하다”는 것을 보여준다.\n— 무작위로 요인들을 넣은 것보다, 적어도 일부 독립변수가 종속변수에 영향을 준다는 증거가 있음.\n개별 계수 검정에서는 X1, X2, X3이 유의하였고(X1, X3 특히 강함), X4는 유의하지 않음 (p ≈ 0.243)\n— 현재 모형 하에서는 고장·견고성 만족도가 구입 의향에 통계적으로 유의한 영향을 미친다고 보기 어렵다.\n결정계수가 낮은 원인으로는 가격 요소 등 중요한 변수가 빠져 있을 가능성, 또는 구입 의향과 관련된 여타 요인(브랜드 신뢰도, 마케팅, 사회적 영향 등)을 포함하지 않았을 가능성 등을 고려해야 한다.\n\n\n③ 표준화 회귀계수\n변수들이 서로 다른 단위를 가지고 있으므로, 회귀분석에서 비표준화 회귀계수만 보면 어떤 변수가 Y(구입 의향)에 더 큰 영향을 주는지 절대비교가 어렵다.\n따라서 표준화 회귀계수를 사용하면, 각 변수의 변화가 “자신의 표준편차 단위 하나 증가”했을 때 Y가 얼마나 표준편차 단위로 바뀌는지를 보여주므로, 변수들 간의 상대적 영향력 비교가 가능해진다.\n\n# 표준화 회귀계수\nfrom scipy import stats\nSatisfaction_z = Satisfaction.iloc[:,1:6].apply(stats.zscore)\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction_z).fit()\nSatisfactionFit.params.round(5)\n\nIntercept    0.00000\nX1           0.16435\nX2           0.11285\nX3           0.15935\nX4           0.04635\ndtype: float64\n\n\n예를 들어, 표준화 계수가 라면, X1과 X3이 구입 의향에 가장 큰 영향을 미치고, X4는 가장 영향이 작다는 해석이 가능하다.\n다만 표준화하면 절편은 일반적으로 의미가 0 근처가 되며, 변수의 원 단위 변화에 대한 실질적인 의미(예: 점수 1점의 변화)가 사라지므로, 연구 목적이 “영향력의 비교”인지 “실질적 예측”인지에 따라 비표준화 계수도 함께 살펴야 한다.\n편회귀계수(partial regression coefficient)\n다중회귀분석에서 각 독립변수가 종속변수에 미치는 순수한 영향력을 평가하기 위해 사용된다. 이는 다른 독립변수들의 영향을 고정한 상태에서 특정 독립변수의 기여도를 나타낸다.\n회귀계수는 모든 독립변수의 영향을 고려한 상태에서 특정 독립변수의 영향을 나타내며, 편회귀계수는 이러한 회귀계수의 일종으로 볼 수 있다.\n따라서 회귀계수와 유사하나 동일하지 않으며, 다중회귀분석에서 각 독립변수의 개별적인 영향을 평가할 때 중요한 지표로 활용된다."
  },
  {
    "objectID": "DAD/DAD09.1.html#변수-선택",
    "href": "DAD/DAD09.1.html#변수-선택",
    "title": "제 9장-다중 회귀",
    "section": "02 변수 선택",
    "text": "02 변수 선택\n풀모델(완전모델)은 모든 독립변수를 포함하여 종속변수에 영향을 줄 수 있는 요인을 전부 반영한 모델이다.\n그러나 실제로는 변수 과다로 인해 과적합이 발생할 수 있으므로, 변수를 일부만 선택하여 축소모델을 사용하는 것이 바람직하다. 이러한 과정이 바로 변수선택법이다.\n\nimport statsmodels.formula.api as smf\nSatisfactionFit1 = smf.ols(formula='Y~X1+X2+X3',\n                          data=Satisfaction).fit()\nprint(SatisfactionFit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.139\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     37.61\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           1.56e-22\nTime:                        16:29:03   Log-Likelihood:                -1175.1\nNo. Observations:                 700   AIC:                             2358.\nDf Residuals:                     696   BIC:                             2376.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.4020      0.334      4.193      0.000       0.746       2.059\nX1             0.2436      0.057      4.254      0.000       0.131       0.356\nX2             0.1763      0.065      2.712      0.007       0.049       0.304\nX3             0.2504      0.065      3.829      0.000       0.122       0.379\n==============================================================================\nOmnibus:                      140.914   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              452.399\nSkew:                           0.951   Prob(JB):                     5.79e-99\nKurtosis:                       6.449   Cond. No.                         62.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nX4를 제외하고 회귀분석을 다시 수행한 결과, 결정계수(R²)는 0.141에서 0.139로 약간 감소했다.\n이는 X4를 제거함으로써 모델의 전체 설명력이 조금 줄었음을 의미하며, X4가 종속변수에 미치는 직접적 영향은 미미하지만 다른 변수와의 상호작용 등을 통해 간접적 기여가 있을 수 있음을 시사한다.\nF-통계량은 37.61로 이전보다 증가했고, P-value는 매우 작아(1.56e-22) 모델 전체는 여전히 통계적으로 유의하다.\n개별 회귀계수 검정에서는 X1, X2, X3는 유의하지만, X4는 유의하지 않아 실질적인 영향력은 거의 없다고 볼 수 있다.\n변수선택법에는 크게 세 가지가 있다.\n① 전진선택법(Forward Selection) 가장 설명력이 높은 변수를 하나씩 추가하며, 선택된 변수는 이후 단계에서 제거되지 않는다.\n② 후진제거법(Backward Elimination) 모든 변수를 포함한 상태에서, 의미 없는 변수를 하나씩 제거하며, 제거 기준은 보통 p-value를 사용한다.\n③ 단계적 방법(Stepwise Selection) 전진선택법과 후진제거법을 혼합하여, 변수를 추가하면서 동시에 불필요한 변수가 없는지 검토하는 방식이다.\n모형 선택 시 결정계수(R²)만으로 평가해서는 안 되며, 조정된 결정계수(Adjusted R²), 평균제곱오차(MSE), AIC, BIC, 멜로우 C 등 여러 지표를 종합적으로 고려해야 한다.\n이러한 과정을 거친 후, 잔차 분석 등을 통해 모델 가정을 확인하는 것이 옳다.\n회귀분석에서 변수 선택 기능의 지원 정도 비교\nSAS PROC REG와 같은 절차를 통해 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다. 예를 들어, SELECTION=STEPWISE 옵션을 사용하여 단계적 선택법을 수행할 수 있다.\nR step() 함수를 사용하여 전진 선택법, 후진 제거법, 단계적 선택법을 수행할 수 있다. 이 함수는 AIC를 기준으로 변수 선택을 수행한다.\npython mlxtend: SequentialFeatureSelector를 제공하여 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.\nabess: 고속 최적 부분집합 선택을 위한 라이브러리로, 선형 회귀 및 분류 문제에 적용할 수 있다."
  },
  {
    "objectID": "DAD/DAD09.1.html#가변수",
    "href": "DAD/DAD09.1.html#가변수",
    "title": "제 9장-다중 회귀",
    "section": "04 가변수",
    "text": "04 가변수\nDummy Variable\n범주형 변수는 회귀 분석에 직접 사용할 수 없으므로, 이를 수치형 변수로 변환해야 한다.\n일반적으로 k 개의 범주를 가진 변수는 k - 1 개의 더미 변수로 변환하여 사용한다.\n이때, k - 1 개의 더미 변수 중 하나는 기준(reference) 범주로 설정되며, 나머지 범주들은 이 기준 범주와의 차이를 나타내는 변수로 해석된다.\n나이별 사는 지역 변수 처리\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Dummy.csv\"\nDummy = pd.read_csv(url)\nDummy.head()\n\n\n\n\n\n\n\n\nID\nY\nAge\nRegion\n\n\n\n\n0\n1\n46\n21\n1\n\n\n1\n2\n39\n21\n3\n\n\n2\n3\n62\n21\n3\n\n\n3\n4\n38\n21\n2\n\n\n4\n5\n39\n21\n3\n\n\n\n\n\n\n\n주어진 데이터에서 ‘Region’ 변수는 3개의 범주를 가집니다:\nRegion 1 Region 2 Region 3 이때, Region 3이 기준 범주(reference category)가 되며, 회귀식은 다음과 같이 표현된다:\n여기서:\nβ₁: Region 1에 해당하는 경우, Region 3과 비교하여 Y값의 차이 β₂: Region 2에 해당하는 경우, Region 3과 비교하여 Y값의 차이 β₀: Region 3에 해당하는 경우의 Y값 (기준 범주) 따라서, 회귀 계수 β₁과 β₂는 각각 Region 1와 Region 2이 Region 3에 비해 Y에 미치는 영향을 나타낸다.\nimport pandas as pd Dummy[‘D1’] = 0 Dummy.loc[Dummy[‘Region’]==1, ‘D1’]=1 Dummy[‘D2’] = 0 Dummy.loc[Dummy[‘Region’]==2, ‘D2’]=1 Dummy.head()\nimport statsmodels.formula.api as smf DummyFit = smf.ols(formula=‘Y~Age+D1+D2’, data=Dummy).fit() print(DummyFit.summary())\nimport statsmodels.formula.api as smf DummyFit1 = smf.ols(formula=‘Y~Age+C(Region)’, data=Dummy).fit() print(DummyFit1.summary())\nimport statsmodels.api as sm sm.stats.anova_lm(DummyFit1, typ=3)\n05 변수변환과 비선형 회귀분석 선형되지 않을 때 선형으로 바꿔주는 것. 예를 들면 로지스틱 회귀.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "DAD/DAD10.0.html",
    "href": "DAD/DAD10.0.html",
    "title": "제 10 장 분산분석: 일원분류",
    "section": "",
    "text": "Reporting Date: November. 5, 2024\n일원분류 분산분석(ANOVA)의 전반적인 내용에 대해 다루고자 한다."
  },
  {
    "objectID": "DAD/DAD10.0.html#유래와-역사적-배경",
    "href": "DAD/DAD10.0.html#유래와-역사적-배경",
    "title": "제 10 장 분산분석: 일원분류",
    "section": "1 . 유래와 역사적 배경",
    "text": "1 . 유래와 역사적 배경\n1935년, 통계학자 로널드 A. 피셔(Ronald A. Fisher)가 저서 The Design of Experiments에서 처음 체계적으로 제시하였다.\n그는 실험 설계법을 통해 통계적 추론의 근거를 마련하였으며, 이후 널리 알려진 붓꽃 데이터(Iris dataset)를 사용하여 그룹 간 평균 차이를 검정하는 절차를 구체적으로 설명하였다.\n이 연구는 실험의 무작위화(randomization), 통제(control), 반복(repetition)이라는 세 가지 원칙을 확립함으로써, 현대 통계학의 기초를 세우는 중요한 전환점이 되었다.\n피셔는 또한 귀무가설(null hypothesis)과 유의수준(significance level) 개념을 도입하여, 관찰된 차이가 단순한 우연인지 아닌지를 판단하는 통계적 검정의 표준 절차를 확립하였다."
  },
  {
    "objectID": "DAD/DAD10.0.html#홍차-실험",
    "href": "DAD/DAD10.0.html#홍차-실험",
    "title": "제 10 장 분산분석: 일원분류",
    "section": "2 . 홍차 실험",
    "text": "2 . 홍차 실험\nTea-tasting experiment\n피셔의 통계 사상을 상징적으로 보여주는 사례로, 임의화 실험(randomized experiment)의 효시로 평가된다.\n피셔의 비서는 홍차와 우유 중 어느 것을 먼저 넣었는지 맛만으로 구별할 수 있다고 주장했다.\n이를 검증하기 위해 피셔는 총 8잔의 홍차를 준비했으며, 이 중 4잔은 홍차를 먼저, 나머지 4잔은 우유를 먼저 넣은 뒤 무작위로 제시했다.\n비서가 모든 잔을 완벽히 맞출 필요는 없었고, 통계적으로 유의미한 비율로 구분할 수 있다면 그의 주장이 단순한 우연이 아님을 입증할 수 있었다.\n이 사례는 오늘날 우리가 사용하는 “귀무가설을 설정하고, 특정 유의수준에서 이를 기각하는” 가설 검정 절차의 시초로 간주된다.\n3 . 문화적 일화: 조지 오웰의 홍차론 흥미롭게도, 홍차에 대한 관점은 통계학적 접근 외에도 문화적으로 다양하게 전개되었다.\n영국의 작가 조지 오웰(George Orwell)은 1946년 에세이 A Nice Cup of Tea에서 “뜨거운 홍차를 먼저 따르고, 그 후 차가운 우유를 넣는 것이 가장 이상적이다”라고 주장했다.\n그는 이 방식이 홍차의 향과 우유의 부드러움을 동시에 살려준다고 설명했다.\n피셔가 실험을 통해 ’객관적 검증’을 중시했다면, 오웰은 경험과 감각을 바탕으로 ’주관적 완성’을 추구한 셈이다. 이 대비는 과학적 탐구와 문화적 감수성의 공존을 상징적으로 보여준다.\n02 실험 연구의 개념 Experimental Study, 디자인, 설계\n특정 요인이 결과 변수에 미치는 영향을 체계적으로 검증하기 위한 연구 설계 방법이다.\n이는 연구자가 요인을 의도적으로 조작(control)하고, 그에 따른 결과 변화를 관찰(observation)함으로써 인과관계를 명확히 밝히는 데 목적이 있다.\n예를 들어, 농작물의 수확량에 영향을 미치는 다양한 요소를 분석하기 위해 다음과 같은 가설을 설정할 수 있다.\n가설: 비료의 종류와 정화수의 온도에 따라 수확량의 차이가 존재하는가? 이 가설을 검증하기 위해 실험을 구성할 때는 주요 개념을 명확히 정의해야 한다.\n1 . 반응변수 Response Variable\n실험의 결과로 측정되는 변수로, 요인의 변화에 따라 값이 달라지는 종속변수이다.\n본 실험에서는 수확량(Yield)이 반응변수로 설정된다. 이는 각 실험 조건(비료와 온도 조합)에 따라 달라질 수 있으며, 연구의 주된 분석 대상이 된다.\n2 . 요인 Factor\n실험에서 반응변수에 영향을 줄 수 있는 독립변수를 의미한다. 연구자는 요인을 조작하여 각 수준별로 반응변수의 변화를 관찰한다.\n본 실험에서는 비료의 종류와 정화수의 온도가 두 개의 요인이다. 각 요인의 효과를 분리하여 비교함으로써, 어떤 요인이 수확량에 더 큰 영향을 미치는지 파악할 수 있다.\n3 . 수준 Level\n각 요인이 취할 수 있는 구체적인 값 또는 조건을 의미한다. 하나의 요인은 여러 수준을 가질 수 있으며, 수준의 조합을 통해 다양한 실험 처리가 구성된다.\n비료 요인: A, B, C → 3개 수준 정화수 온도 요인: 10 °C, 15 °C, 20 °C → 3개 수준 이처럼 요인의 수준 수가 많아질수록 가능한 처리 조합의 수도 기하급수적으로 증가한다.\n4 . 처리 또는 처치 Treatment\n요인들의 수준 조합에 의해 형성되는 구체적인 실험 조건을 의미한다. 즉, 각 처리는 서로 다른 조건하에서 실험이 수행되는 개별 실험 단위를 나타낸다.\n(비료 A, 온도 10 °C) (비료 B, 온도 15 °C) (비료 C, 온도 20 °C) 각 조합은 하나의 독립적인 처리이며, 처리별 수확량을 비교함으로써 요인의 효과를 통계적으로 분석할 수 있다.\n개념 예시 반응변수 실험의 결과로 관찰되는 값 수확량 요인 반응변수에 영향을 주는 독립변수 비료 종류, 정화수 온도 수준 요인의 구체적 조건 비료 A/B/C, 온도 10 / 15 / 20 °C 처리 요인 수준의 조합 (비료 A, 10 °C), (비료 B, 15 °C) 등\n실험 연구 과정\n03 실험 설계의 기본 원리 Principles of Experimental Design\n실험 설계의 목적은 요인의 효과를 정확하게 추정하고, 불필요한 오차를 최소화하는 것에 있다.\n이를 달성하기 위해 통계학자 로널드 A. 피셔(Ronald A. Fisher)는 세 가지 기본 원리 ― 반복화(Replication), 확률화(Randomization), 블록화(Blocking) ― 를 제시하였다.\n이 세 원리는 현대 모든 실험 설계의 기초로 사용되며, 분산 분석의 이론적 기반을 이룬다.\n1 . 반복화 Replication\n동일한 실험 처리를 여러 번 반복하여 수행하는 절차를 말한다.\n실험 단위는 완전히 동일하지 않기 때문에, 실험 오차(experimental error)가 항상 존재한다.\n따라서 동일한 조건하에서 실험을 반복함으로써 오차의 크기를 추정하고, 결과의 신뢰도를 높일 수 있다.\n각 반응값(response value)은 확률변수로 간주되므로, 반복 측정을 통해 변이(variation)를 관찰할 수 있다.\n이를 통해 평균 반응의 안정성과 분산의 크기를 평가할 수 있으며, 이는 F-검정의 분모에 해당하는 오차항(Mean Square Error) 추정에 직접적으로 기여한다.\n비료 A와 온도 15 °C의 조합을 세 번 반복 측정할 경우, 수확량의 평균뿐 아니라 변동성까지 고려한 신뢰성 있는 결론을 얻을 수 있다.\n2 . 확률화 Randomization\n실험 단위에 각 처리(조건)를 무작위로 배정(random assignment) 하는 절차이다.\n확률화를 통해 연구자의 의도나 외부 환경 요인에 의한 편향(bias)이 최소화된다. 즉, 각 실험 단위가 특정 처리에 배정될 확률이 동일하게 유지되므로, 관찰된 차이는 요인 자체의 효과로 해석할 수 있다.\n확률화는 통계적 추론의 전제인 독립성(independence)을 확보하고, 실험 결과에 대한 통계 검정의 정당성을 보장한다.\n농지의 위치, 토양의 질, 일조량 등 통제하기 어려운 요인이 존재할 때, 처리 배정을 무작위로 수행하면 이러한 외부 요인의 체계적 영향이 결과에 편향을 일으키는 것을 방지할 수 있다.\n3 . 블록화 Blocking\n비슷한 특성을 지닌 실험 단위를 묶어 동질적인 블록(block)을 형성한 후, 각 블록 내에서 실험 처리를 배정하는 방법이다.\n블록화의 목적은 요인 이외의 변동 요인을 통제하여 실험의 정밀도(precision)를 향상시키는 것이다.\n즉, 블록 내에서는 실험 단위 간 차이가 최소화되므로, 관찰된 차이는 요인 효과로 더 정확히 설명될 수 있다.\n블록은 하나의 보조 요인(nuisance factor)으로 간주되어, 분산 분석 시 오차 제곱합(Error Sum of Squares)을 줄이고 요인 효과의 추정 오차를 감소시킨다.\n농지의 일조량이 다를 경우, 일조량 수준이 유사한 구역을 하나의 블록으로 묶은 뒤, 각 블록 내에서 비료 종류를 무작위로 배정하면 요인 효과를 보다 정확히 평가할 수 있다.\n세 가지 원리는 서로 독립적이지만 상호보완적으로 작용한다.\n원리 목표 주요 효과 반복화 (Replication) 실험 오차 추정 및 결과의 신뢰성 확보 변이 분석 가능, 평균 안정화 확률화 (Randomization) 편향 제거 및 독립성 확보 결과의 타당성 향상 블록화 (Blocking) 외부 요인 통제 및 정밀도 향상 오차 감소, 요인 효과의 명확화\n04 일원분류 분산분석 One-Way ANOVA\n하나의 요인이 반응변수에 미치는 영향을 분석하기 위한 통계적 기법이다. 즉, 하나의 요인에 대해 여러 수준 또는 집단 간의 평균 차이가 존재하는지를 검정한다.\n예를 들어, 서로 다른 세 종류의 비료(A, B, C)가 작물의 수확량에 미치는 영향을 비교하고자 할 때,\n각 비료 그룹의 평균 수확량이 통계적으로 동일한지 여부를 판단하는 것이 일원분류 분산분석의 목적이다.\n1 . 가설의 설정 일원분류 ANOVA의 기본 가설은 다음과 같이 정의된다.\n귀무가설 (H₀): 모든 집단의 평균이 같다.\n\\[H_0 : \\mu_1 = \\mu_2 = \\mu_3 = \\dots = \\mu_k\\]\n대립가설 (H₁): 적어도 하나의 집단 평균은 다르다.\n\\[H_1 : \\text{적어도 한 } \\mu_i \\text{ 가 다름}\\]\n이때, 대립가설은 구체적으로 어느 집단 간의 차이가 있는지를 명시하지 않으며, 단지 ’평균 간 차이가 존재한다’는 전체적 효과를 검정한다.\n2 . ANOVA의 필요성 두 집단 간의 평균 비교는 t-검정으로 충분하다. 그러나 세 집단 이상을 비교할 때 t-검정을 반복 적용하면 누적된 유의수준(α) 문제가 발생한다.\n예를 들어, 유의수준 α = 0.05로 세 집단을 비교하는 경우, 서로 다른 세 쌍의 평균 비교가 필요하다.\n\\[1 - (1 - 0.05)^3 \\approx 0.14\\]\n즉, 전체적으로 약 14% 확률로 잘못된 기각(제1종 오류)이 발생할 수 있다. 이는 실제로 차이가 없음에도 불구하고 유의미한 차이가 있다고 잘못 판단할 위험을 높인다.\n3 . 분산분석의 원리 ANOVA는 단순히 평균 차이를 직접 비교하지 않고, 총 변동(Total Variation)을 다음 두 가지로 분해하여 비교한다.\n\\[\\text{총제곱합} = \\text{집단 간 제곱합 (SSB)} + \\text{집단 내 제곱합 (SSW)}\\]\n집단 간 제곱합 (Between-group variance): 각 집단 평균 간의 차이에서 기인한 변동, 즉 요인의 효과를 나타낸다.\n집단 내 제곱합 (Within-group variance): 같은 집단 내에서의 개별 오차로 인한 변동을 의미한다.\n이 두 변동의 비율을 이용해 계산되는 통계량이 F-통계량이다.\n\\[F = \\frac{\\text{집단 간 평균제곱 (MSB)}}{\\text{집단 내 평균제곱 (MSW)}}\\]\n이 값이 F분포 상에서 임계값보다 크면, 집단 간 평균 차이가 통계적으로 유의하다고 판단한다.\n4 . 해석의 의미 ANOVA의 결과, 귀무가설이 기각된다면 ” 적어도 하나의 집단 평균이 다르다 ” 는 결론을 내릴 수 있다.\n다만, 어떤 집단 간의 차이가 존재하는지는 알 수 없으므로, 그 이후에는 사후검정(Tukey HSD, Duncan, Scheffé 검정 등) 을 통해 집단 간 세부 비교를 수행해야 한다.\n05 분산분석표 해석\nANOVA Table\n비료는 본 실험에서 단일 요인이며, 이 요인은 총 k개의 처리 또는 수준으로 구성된다. 각 처리는 서로 다른 비료 종류를 의미하며, 이에 따라 반응변수(수확량)의 변화가 관측된다.\n1 . 반응값 구조와 표본평균 반응값 ( y )는 각 실험 단위에서 관측된 수확량을 의미한다. 비료 ( i )에 대해 ( n )번 반복 측정했다면, ( y_{i1}, y_{i2}, , y_{in} ) 이 수집된다.\n예: - ( y_{11} ): 1번 비료의 첫 번째 실험에서 얻은 수확량 각 비료에 대해 반복 측정된 반응값을 평균 내면 표본평균 ({y}_i)이 계산된다.\n2 . ANOVA의 귀무가설 일원분류 ANOVA의 귀무가설은 다음과 같다.\n\\[H_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\]\n즉, 비료 종류가 달라도 모평균 수확량은 동일하다고 가정한다. 이는 모든 비료 효과가 0이라는 의미에 해당한다. 반면, 어느 하나라도 다른 경우 대립가설 (H_1)이 성립한다.\n3 . 제곱합의 의미 Sum of Squares\n일원분류 분산분석(ANOVA)은 전체 변동을 처리 간 변동과 처리 내 변동으로 분해한다. 이는 요인(예: 비료 종류)이 반응변수(수확량)에 미치는 영향을 확인하기 위한 핵심 구조이다.\n\n총제곱합(TSS, Total Sum of Squares)\n\n\\[TSS = \\sum_{i=1}^{k}\\sum_{j=1}^{r} (y_{ij}-\\bar{y})^2\\]\n모든 관측값이 전체 평균에서 얼마나 벗어나는지 보여준다. 실험에서 관측된 전체 변동(total variability)을 의미하며, 이후 처리 효과와 오차 변동으로 분해된다. (2) 처리제곱합(SST, Treatment Sum of Squares)\n\\[SST = r\\sum_{i=1}^{k}(\\bar{y}_i - \\bar{y})^2\\]\n각 처리 평균이 전체 평균에서 얼마나 떨어져 있는지를 측정한다. 반복수 ( r )을 곱함으로써 각 처리 평균이 전체 변동에 기여하는 정도를 반영한다. 비료별 평균 차이가 클수록 SST가 커지며, 이는 처리 효과가 클 가능성을 의미한다. 해석:\nSST가 크면 → 처리 간(비료 간) 평균 수확량 차이가 크다 → 대립가설 지지 가능성 증가 SST가 작으면 → 비료 간 차이가 거의 없다 (3) 오차제곱합(SSE, Error Sum of Squares)\n\\[SSE = \\sum_{i=1}^{k}\\sum_{j=1}^{r}(y_{ij}-\\bar{y}_i)^2\\]\n동일한 처리 내에서 개별 관측값들이 처리 평균에서 벗어나는 정도를 나타낸다. 내부 변동(within-group variability), 즉 실험 통제로 해결되지 않는 자연적·비체계적 변동을 의미한다. SSE가 작을수록 처리군 내부의 일관성이 높아지고, 처리 효과 검정의 민감도가 상승한다. (4) 제곱합의 관계\n\\[TSS = SST + SSE\\]\n전체 변동이 처리 간 차이(SST) 처리 내 오차(SSE) 로 구성됨을 의미한다. 이는 일원분류 ANOVA 전체 구조의 토대이다.\n4 . 처리제곱합의 해석 관점 처리제곱합의 해석은 다음 원칙에 기반한다.\nSST가 작다면, 모든 처리 평균이 서로 유사하여 실험을 통해 구분할 만한 차이가 존재하지 않음을 의미한다.\nSST가 상대적으로 크고 SSE가 작다면, 처리 간 차이가 오차 변동보다 충분히 크다는 뜻이며, ANOVA에서 유의한 결과가 나올 가능성이 높아진다.\n최종 판단은 다음을 충족할 때 이루어진다. SST가 크고 SSE가 작으며 계산된 F 통계량이 유의수준에서 임계값을 초과할 때 이 경우 처리 간 평균 차이가 통계적으로 유의하다고 결론 내릴 수 있다.\n5 . 오차의 특성 ANOVA의 F 검정이 타당하려면 다음 오차 구조를 충족해야 한다.\n등분산성(homoscedasticity): 각 처리 집단의 오차 분산이 동일 정규성(normality): 오차항이 정규분포를 따름 독립성(independence): 관측값 간 종속 구조가 없음 이 조건이 충족될 때, 계산된 F-통계량은 이론적 F-분포와 부합한다.\n6 . F-검정의 구조 ANOVA의 목적은 다음 귀무가설을 검정하는 것이다.\n\\[H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_k = 0\\]\n즉, 모든 처리 평균이 동일하다는 가설이다.\n평균제곱(MS)을 기반으로 F 통계량을 계산한다.\n\\[F = \\frac{MS_{treatment}}{MS_{error}}\\]\n처리 평균제곱: (MS_{treat} = ) 오차 평균제곱: (MS_{error} = ) F-값이 클수록 처리평균이 동일하다는 귀무가설을 기각하고 처리 효과가 존재한다는 결론을 얻는다.\n7 . 추가 설명 반복수 ( r )이 동일하지 않아도(불균형 설계), 제곱합과 자유도 계산만 조정되며 분석의 논리는 변하지 않는다. F-분포는 오른쪽이 긴 비대칭 분포이며, 제곱항 기반이므로 음수 값이 존재하지 않는다.\n06 모형 설정의 목적과 구조\n일원분류 ANOVA를 기준으로 한다.\n1 . 모델을 세우는 이유 비료 종류가 수확량에 어떤 영향을 주는지를 정량적으로 파악하기 위해서는 관측된 수확량을 처리 효과(비료의 영향)와 오차(통제 불가능한 변동)로 명확히 분리할 필요가 있다.\n모형화(Modeling)는 다음 두 가지 목적을 가진다.\n처리 효과와 오차를 구분하여 변동의 원인을 구조적으로 이해한다. 비료 간 차이가 통계적으로 유의한지 검정할 수 있는 기반을 제공한다. 즉, 모델을 세움으로써 각 비료가 수확량에 미치는 효과를 체계적으로 설명하고 추론할 수 있다.\n2 . 모형의 기본 구조 일원분류 ANOVA에서는 개별 수확량 ( y_{ij} )를 다음과 같이 분해한다.\n\\[y_{ij} = \\mu_i + \\epsilon_{ij}\\]\n여기서\n( i = 1, 2, , k ): 비료 종류(처리 수준) ( j = 1, 2, , n ): 반복 실험 번호 ( i ): i번째 비료의 모평균 (각 처리에 대한 모평) ( {ij} ): 개별 수확량의 오차항(Error term)\n3 . 처리 평균의 분해: 전체 평균과 처리 효과 처리별 모평균 ( _i )다시 다음과 같이 표현된다.\n\\[\\mu_i = \\mu + \\tau_i\\]\n( ) : 전체 모평균(overall mean) (모든 비료에 대한 평균) ( _i ) : i번째 처리(비료)의 효과(treatment effect) 그리스 알파벳 타우(τ)로 표기 (19번째 글자) 전체 평균에서 벗어난 정도를 의미 즉, 각 처리의 평균은 전체 평균 위에 얼마나 효과가 더해졌는가를 나타낸다.\n4 . 최종 모형의 형태 위 식을 결합하면 개별 수확량은 다음과 같이 분해된다.\n\\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\]\n이 모형은 다음의 세 구성요소로 수확량을 설명한다.\n전체 모평균(μ) 비료의 처리 효과(τᵢ) 설명되지 않는 변동(오차, εᵢⱼ)\n5 . 오차항의 가정 오차항 ( _{ij} )는 ANOVA의 핵심 가정에 따라 다음을 만족한다.\n\\[\\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n평균 0 분산 ( ^2 ) 정규분포를 따르며 서로 독립 이러한 가정이 충족되어야 F-검정이 타당하게 동작한다.\n6 . 모델 설정의 의의 이 모형을 사용하면 다음이 가능해진다.\n각 비료의 효과(τᵢ)가 0인지 검정하여 비료 종류가 수확량에 유의한 차이를 만드는지 평가 처리 효과와 오차가 분리되므로 변동의 원인을 구조적으로 해석 분산분석표(ANOVA Table)에서 제곱합 → 평균제곱 → F값 으로 이어지는 분석 구조 확립 결과적으로, 이 모형은 비료 간 평균 차이가 단순한 우연인지, 실제 효과인지 판단할 수 있는 통계적 기반을 형성한다.\n필요하면 이어서 모형의 제약조건(∑τᵢ = 0), 모수추정(OLS 관점), F 검정 유도 과정도 전문적으로 정리해드립니다. (흠…)\n64 = 72 + (-8) = 전체 모평균 + 오\n①②③④⑤⑥⑦⑧⑨ ⋅ ⌎\n[출처]"
  },
  {
    "objectID": "DAD/DAD10.0.html#문화적-일화-조지-오웰의-홍차론",
    "href": "DAD/DAD10.0.html#문화적-일화-조지-오웰의-홍차론",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 문화적 일화: 조지 오웰의 홍차론",
    "text": "3 . 문화적 일화: 조지 오웰의 홍차론\n흥미롭게도, 홍차에 대한 관점은 통계학적 접근 외에도 문화적으로 다양하게 전개되었다.\n영국의 작가 조지 오웰(George Orwell)은 1946년 에세이 A Nice Cup of Tea에서 “뜨거운 홍차를 먼저 따르고, 그 후 차가운 우유를 넣는 것이 가장 이상적이다”라고 주장했다.\n그는 이 방식이 홍차의 향과 우유의 부드러움을 동시에 살려준다고 설명했다.\n피셔가 실험을 통해 ’객관적 검증’을 중시했다면, 오웰은 경험과 감각을 바탕으로 ’주관적 완성’을 추구한 셈이다. 이 대비는 과학적 탐구와 문화적 감수성의 공존을 상징적으로 보여준다."
  },
  {
    "objectID": "DAD/DAD10.0.html#반응변수",
    "href": "DAD/DAD10.0.html#반응변수",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반응변수",
    "text": "1 . 반응변수\nResponse Variable\n실험의 결과로 측정되는 변수로, 요인의 변화에 따라 값이 달라지는 종속변수이다.\n본 실험에서는 수확량(Yield)이 반응변수로 설정된다. 이는 각 실험 조건(비료와 온도 조합)에 따라 달라질 수 있으며, 연구의 주된 분석 대상이 된다."
  },
  {
    "objectID": "DAD/DAD10.0.html#요인",
    "href": "DAD/DAD10.0.html#요인",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . 요인",
    "text": "2 . 요인\nFactor\n실험에서 반응변수에 영향을 줄 수 있는 독립변수를 의미한다. 연구자는 요인을 조작하여 각 수준별로 반응변수의 변화를 관찰한다.\n본 실험에서는 비료의 종류와 정화수의 온도가 두 개의 요인이다. 각 요인의 효과를 분리하여 비교함으로써, 어떤 요인이 수확량에 더 큰 영향을 미치는지 파악할 수 있다."
  },
  {
    "objectID": "DAD/DAD10.0.html#수준",
    "href": "DAD/DAD10.0.html#수준",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 수준",
    "text": "3 . 수준\nLevel\n각 요인이 취할 수 있는 구체적인 값 또는 조건을 의미한다. 하나의 요인은 여러 수준을 가질 수 있으며, 수준의 조합을 통해 다양한 실험 처리가 구성된다.\n비료 요인: A, B, C → 3개 수준 정화수 온도 요인: 10 °C, 15 °C, 20 °C → 3개 수준 이처럼 요인의 수준 수가 많아질수록 가능한 처리 조합의 수도 기하급수적으로 증가한다."
  },
  {
    "objectID": "DAD/DAD10.0.html#처리-또는-처치",
    "href": "DAD/DAD10.0.html#처리-또는-처치",
    "title": "제 10장 분산분석: 일원분류",
    "section": "4 . 처리 또는 처치",
    "text": "4 . 처리 또는 처치\nTreatment\n요인들의 수준 조합에 의해 형성되는 구체적인 실험 조건을 의미한다. 즉, 각 처리는 서로 다른 조건하에서 실험이 수행되는 개별 실험 단위를 나타낸다.\n(비료 A, 온도 10 °C) (비료 B, 온도 15 °C) (비료 C, 온도 20 °C) 각 조합은 하나의 독립적인 처리이며, 처리별 수확량을 비교함으로써 요인의 효과를 통계적으로 분석할 수 있다.\n개념 예시 반응변수 실험의 결과로 관찰되는 값 수확량 요인 반응변수에 영향을 주는 독립변수 비료 종류, 정화수 온도 수준 요인의 구체적 조건 비료 A/B/C, 온도 10 / 15 / 20 °C 처리 요인 수준의 조합 (비료 A, 10 °C), (비료 B, 15 °C) 등\n실험 연구 과정\n03 실험 설계의 기본 원리 Principles of Experimental Design\n실험 설계의 목적은 요인의 효과를 정확하게 추정하고, 불필요한 오차를 최소화하는 것에 있다.\n이를 달성하기 위해 통계학자 로널드 A. 피셔(Ronald A. Fisher)는 세 가지 기본 원리 ― 반복화(Replication), 확률화(Randomization), 블록화(Blocking) ― 를 제시하였다.\n이 세 원리는 현대 모든 실험 설계의 기초로 사용되며, 분산 분석의 이론적 기반을 이룬다."
  },
  {
    "objectID": "DAD/DAD10.0.html#반복화",
    "href": "DAD/DAD10.0.html#반복화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반복화",
    "text": "1 . 반복화\nReplication\n동일한 실험 처리를 여러 번 반복하여 수행하는 절차를 말한다.\n실험 단위는 완전히 동일하지 않기 때문에, 실험 오차(experimental error)가 항상 존재한다.\n따라서 동일한 조건하에서 실험을 반복함으로써 오차의 크기를 추정하고, 결과의 신뢰도를 높일 수 있다.\n각 반응값(response value)은 확률변수로 간주되므로, 반복 측정을 통해 변이(variation)를 관찰할 수 있다.\n이를 통해 평균 반응의 안정성과 분산의 크기를 평가할 수 있으며, 이는 F-검정의 분모에 해당하는 오차항(Mean Square Error) 추정에 직접적으로 기여한다.\n비료 A와 온도 15 °C의 조합을 세 번 반복 측정할 경우, 수확량의 평균뿐 아니라 변동성까지 고려한 신뢰성 있는 결론을 얻을 수 있다."
  },
  {
    "objectID": "DAD/DAD10.0.html#확률화",
    "href": "DAD/DAD10.0.html#확률화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . 확률화",
    "text": "2 . 확률화\nRandomization\n실험 단위에 각 처리(조건)를 무작위로 배정(random assignment) 하는 절차이다.\n확률화를 통해 연구자의 의도나 외부 환경 요인에 의한 편향(bias)이 최소화된다. 즉, 각 실험 단위가 특정 처리에 배정될 확률이 동일하게 유지되므로, 관찰된 차이는 요인 자체의 효과로 해석할 수 있다.\n확률화는 통계적 추론의 전제인 독립성(independence)을 확보하고, 실험 결과에 대한 통계 검정의 정당성을 보장한다.\n농지의 위치, 토양의 질, 일조량 등 통제하기 어려운 요인이 존재할 때, 처리 배정을 무작위로 수행하면 이러한 외부 요인의 체계적 영향이 결과에 편향을 일으키는 것을 방지할 수 있다."
  },
  {
    "objectID": "DAD/DAD10.0.html#블록화",
    "href": "DAD/DAD10.0.html#블록화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 블록화",
    "text": "3 . 블록화\nBlocking\n비슷한 특성을 지닌 실험 단위를 묶어 동질적인 블록(block)을 형성한 후, 각 블록 내에서 실험 처리를 배정하는 방법이다.\n블록화의 목적은 요인 이외의 변동 요인을 통제하여 실험의 정밀도(precision)를 향상시키는 것이다.\n즉, 블록 내에서는 실험 단위 간 차이가 최소화되므로, 관찰된 차이는 요인 효과로 더 정확히 설명될 수 있다.\n블록은 하나의 보조 요인(nuisance factor)으로 간주되어, 분산 분석 시 오차 제곱합(Error Sum of Squares)을 줄이고 요인 효과의 추정 오차를 감소시킨다.\n농지의 일조량이 다를 경우, 일조량 수준이 유사한 구역을 하나의 블록으로 묶은 뒤, 각 블록 내에서 비료 종류를 무작위로 배정하면 요인 효과를 보다 정확히 평가할 수 있다.\n세 가지 원리는 서로 독립적이지만 상호보완적으로 작용한다.\n원리 목표 주요 효과 반복화 (Replication) 실험 오차 추정 및 결과의 신뢰성 확보 변이 분석 가능, 평균 안정화 확률화 (Randomization) 편향 제거 및 독립성 확보 결과의 타당성 향상 블록화 (Blocking) 외부 요인 통제 및 정밀도 향상 오차 감소, 요인 효과의 명확화"
  },
  {
    "objectID": "DAD/DAD10.0.html#anova의-필요성",
    "href": "DAD/DAD10.0.html#anova의-필요성",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . ANOVA의 필요성",
    "text": "2 . ANOVA의 필요성\n두 집단 간의 평균 비교는 t-검정으로 충분하다. 그러나 세 집단 이상을 비교할 때 t-검정을 반복 적용하면 누적된 유의수준(α) 문제가 발생한다.\n예를 들어, 유의수준 α = 0.05로 세 집단을 비교하는 경우, 서로 다른 세 쌍의 평균 비교가 필요하다.\n\\[\n1 - (1 - 0.05)^3 \\approx 0.14\n\\]\n즉, 전체적으로 약 14% 확률로 잘못된 기각(제1종 오류)이 발생할 수 있다. 이는 실제로 차이가 없음에도 불구하고 유의미한 차이가 있다고 잘못 판단할 위험을 높인다."
  },
  {
    "objectID": "DAD/DAD10.0.html#분산분석의-원리",
    "href": "DAD/DAD10.0.html#분산분석의-원리",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 분산분석의 원리",
    "text": "3 . 분산분석의 원리\nANOVA는 단순히 평균 차이를 직접 비교하지 않고, 총 변동(Total Variation)을 다음 두 가지로 분해하여 비교한다.\n\\[\n\\text{총제곱합} = \\text{집단 간 제곱합 (SSB)} + \\text{집단 내 제곱합 (SSW)}\n\\]\n집단 간 제곱합 (Between-group variance): 각 집단 평균 간의 차이에서 기인한 변동, 즉 요인의 효과를 나타낸다.\n집단 내 제곱합 (Within-group variance): 같은 집단 내에서의 개별 오차로 인한 변동을 의미한다.\n이 두 변동의 비율을 이용해 계산되는 통계량이 F-통계량이다.\n\\[\nF = \\frac{\\text{집단 간 평균제곱 (MSB)}}{\\text{집단 내 평균제곱 (MSW)}}\n\\]\n이 값이 F분포 상에서 임계값보다 크면, 집단 간 평균 차이가 통계적으로 유의하다고 판단한다."
  },
  {
    "objectID": "DAD/DAD10.0.html#해석의-의미",
    "href": "DAD/DAD10.0.html#해석의-의미",
    "title": "제 10장 분산분석: 일원분류",
    "section": "4 . 해석의 의미",
    "text": "4 . 해석의 의미\nANOVA의 결과, 귀무가설이 기각된다면 ” 적어도 하나의 집단 평균이 다르다 ” 는 결론을 내릴 수 있다. 다만, 어떤 집단 간의 차이가 존재하는지는 알 수 없으므로, 그 이후에는 사후검정(Tukey HSD, Duncan, Scheffé 검정 등) 을 통해 집단 간 세부 비교를 수행해야 한다."
  },
  {
    "objectID": "DAD/DAD10.0.html#반응값-구조와-표본평균",
    "href": "DAD/DAD10.0.html#반응값-구조와-표본평균",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반응값 구조와 표본평균",
    "text": "1 . 반응값 구조와 표본평균\n반응값 ( y )는 각 실험 단위에서 관측된 수확량을 의미한다. 비료 ( i )에 대해 ( n )번 반복 측정했다면, ( y_{i1}, y_{i2}, , y_{in} ) 이 수집된다.\n예: - ( y_{11} ): 1번 비료의 첫 번째 실험에서 얻은 수확량 각 비료에 대해 반복 측정된 반응값을 평균 내면 표본평균 ({y}_i)이 계산된다."
  },
  {
    "objectID": "DAD/DAD10.0.html#anova의-귀무가설",
    "href": "DAD/DAD10.0.html#anova의-귀무가설",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . ANOVA의 귀무가설",
    "text": "2 . ANOVA의 귀무가설\n일원분류 ANOVA의 귀무가설은 다음과 같다.\n\\[\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k\n\\]\n즉, 비료 종류가 달라도 모평균 수확량은 동일하다고 가정한다. 이는 모든 비료 효과가 0이라는 의미에 해당한다. 반면, 어느 하나라도 다른 경우 대립가설 (H_1)이 성립한다."
  },
  {
    "objectID": "DAD/DAD10.0.html#제곱합의-의미",
    "href": "DAD/DAD10.0.html#제곱합의-의미",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 제곱합의 의미",
    "text": "3 . 제곱합의 의미\nSum of Squares\n일원분류 분산분석(ANOVA)은 전체 변동을 처리 간 변동과 처리 내 변동으로 분해한다. 이는 요인(예: 비료 종류)이 반응변수(수확량)에 미치는 영향을 확인하기 위한 핵심 구조이다.\n\n총제곱합(TSS, Total Sum of Squares)\n\n\\[\nTSS = \\sum_{i=1}^{k}\\sum_{j=1}^{r} (y_{ij}-\\bar{y})^2\n\\]\n모든 관측값이 전체 평균에서 얼마나 벗어나는지 보여준다. 실험에서 관측된 전체 변동(total variability)을 의미하며, 이후 처리 효과와 오차 변동으로 분해된다. (2) 처리제곱합(SST, Treatment Sum of Squares)\n\\[\nSST = r\\sum_{i=1}^{k}(\\bar{y}_i - \\bar{y})^2\n\\]\n각 처리 평균이 전체 평균에서 얼마나 떨어져 있는지를 측정한다. 반복수 ( r )을 곱함으로써 각 처리 평균이 전체 변동에 기여하는 정도를 반영한다. 비료별 평균 차이가 클수록 SST가 커지며, 이는 처리 효과가 클 가능성을 의미한다. 해석:\nSST가 크면 → 처리 간(비료 간) 평균 수확량 차이가 크다 → 대립가설 지지 가능성 증가 SST가 작으면 → 비료 간 차이가 거의 없다 (3) 오차제곱합(SSE, Error Sum of Squares)\n\\[\nSSE = \\sum_{i=1}^{k}\\sum_{j=1}^{r}(y_{ij}-\\bar{y}_i)^2\n\\]\n동일한 처리 내에서 개별 관측값들이 처리 평균에서 벗어나는 정도를 나타낸다. 내부 변동(within-group variability), 즉 실험 통제로 해결되지 않는 자연적·비체계적 변동을 의미한다. SSE가 작을수록 처리군 내부의 일관성이 높아지고, 처리 효과 검정의 민감도가 상승한다. (4) 제곱합의 관계\n\\[\nTSS = SST + SSE\n\\]\n전체 변동이 처리 간 차이(SST) 처리 내 오차(SSE) 로 구성됨을 의미한다. 이는 일원분류 ANOVA 전체 구조의 토대이다.\n4 . 처리제곱합의 해석 관점 처리제곱합의 해석은 다음 원칙에 기반한다.\nSST가 작다면, 모든 처리 평균이 서로 유사하여 실험을 통해 구분할 만한 차이가 존재하지 않음을 의미한다.\nSST가 상대적으로 크고 SSE가 작다면, 처리 간 차이가 오차 변동보다 충분히 크다는 뜻이며, ANOVA에서 유의한 결과가 나올 가능성이 높아진다.\n최종 판단은 다음을 충족할 때 이루어진다. SST가 크고 SSE가 작으며 계산된 F 통계량이 유의수준에서 임계값을 초과할 때 이 경우 처리 간 평균 차이가 통계적으로 유의하다고 결론 내릴 수 있다.\n5 . 오차의 특성 ANOVA의 F 검정이 타당하려면 다음 오차 구조를 충족해야 한다.\n등분산성(homoscedasticity): 각 처리 집단의 오차 분산이 동일 정규성(normality): 오차항이 정규분포를 따름 독립성(independence): 관측값 간 종속 구조가 없음 이 조건이 충족될 때, 계산된 F-통계량은 이론적 F-분포와 부합한다.\n6 . F-검정의 구조 ANOVA의 목적은 다음 귀무가설을 검정하는 것이다.\n\\[H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_k = 0\\]\n즉, 모든 처리 평균이 동일하다는 가설이다.\n평균제곱(MS)을 기반으로 F 통계량을 계산한다.\n\\[F = \\frac{MS_{treatment}}{MS_{error}}\\]\n처리 평균제곱: (MS_{treat} = ) 오차 평균제곱: (MS_{error} = ) F-값이 클수록 처리평균이 동일하다는 귀무가설을 기각하고 처리 효과가 존재한다는 결론을 얻는다.\n7 . 추가 설명 반복수 ( r )이 동일하지 않아도(불균형 설계), 제곱합과 자유도 계산만 조정되며 분석의 논리는 변하지 않는다. F-분포는 오른쪽이 긴 비대칭 분포이며, 제곱항 기반이므로 음수 값이 존재하지 않는다.\n06 모형 설정의 목적과 구조\n일원분류 ANOVA를 기준으로 한다.\n1 . 모델을 세우는 이유 비료 종류가 수확량에 어떤 영향을 주는지를 정량적으로 파악하기 위해서는 관측된 수확량을 처리 효과(비료의 영향)와 오차(통제 불가능한 변동)로 명확히 분리할 필요가 있다.\n모형화(Modeling)는 다음 두 가지 목적을 가진다.\n처리 효과와 오차를 구분하여 변동의 원인을 구조적으로 이해한다. 비료 간 차이가 통계적으로 유의한지 검정할 수 있는 기반을 제공한다. 즉, 모델을 세움으로써 각 비료가 수확량에 미치는 효과를 체계적으로 설명하고 추론할 수 있다.\n2 . 모형의 기본 구조 일원분류 ANOVA에서는 개별 수확량 ( y_{ij} )를 다음과 같이 분해한다.\n\\[y_{ij} = \\mu_i + \\epsilon_{ij}\\]\n여기서\n( i = 1, 2, , k ): 비료 종류(처리 수준) ( j = 1, 2, , n ): 반복 실험 번호 ( i ): i번째 비료의 모평균 (각 처리에 대한 모평) ( {ij} ): 개별 수확량의 오차항(Error term)\n3 . 처리 평균의 분해: 전체 평균과 처리 효과 처리별 모평균 ( _i )다시 다음과 같이 표현된다.\n\\[\\mu_i = \\mu + \\tau_i\\]\n( ) : 전체 모평균(overall mean) (모든 비료에 대한 평균) ( _i ) : i번째 처리(비료)의 효과(treatment effect) 그리스 알파벳 타우(τ)로 표기 (19번째 글자) 전체 평균에서 벗어난 정도를 의미 즉, 각 처리의 평균은 전체 평균 위에 얼마나 효과가 더해졌는가를 나타낸다.\n4 . 최종 모형의 형태 위 식을 결합하면 개별 수확량은 다음과 같이 분해된다.\n\\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\]\n이 모형은 다음의 세 구성요소로 수확량을 설명한다.\n전체 모평균(μ) 비료의 처리 효과(τᵢ) 설명되지 않는 변동(오차, εᵢⱼ)\n5 . 오차항의 가정 오차항 ( _{ij} )는 ANOVA의 핵심 가정에 따라 다음을 만족한다.\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n평균 0 분산 ( ^2 ) 정규분포를 따르며 서로 독립 이러한 가정이 충족되어야 F-검정이 타당하게 동작한다.\n6 . 모델 설정의 의의 이 모형을 사용하면 다음이 가능해진다.\n각 비료의 효과(τᵢ)가 0인지 검정하여 비료 종류가 수확량에 유의한 차이를 만드는지 평가 처리 효과와 오차가 분리되므로 변동의 원인을 구조적으로 해석 분산분석표(ANOVA Table)에서 제곱합 → 평균제곱 → F값 으로 이어지는 분석 구조 확립 결과적으로, 이 모형은 비료 간 평균 차이가 단순한 우연인지, 실제 효과인지 판단할 수 있는 통계적 기반을 형성한다.\n필요하면 이어서 모형의 제약조건(∑τᵢ = 0), 모수추정(OLS 관점), F 검정 유도 과정도 전문적으로 정리해드립니다. (흠…)\n64 = 72 + (-8) = 전체 모평균 + 오\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "IDA/IDA11.html",
    "href": "IDA/IDA11.html",
    "title": "11장: 정규모집단에서의 추론",
    "section": "",
    "text": "Reporting Date: July. 28, 2024\n표본의 크기가 작을 경우에는 일반적인 통계적 추론 방법을 적용하기 어려울 수 있다. 이 경우, 정규분포 대신 t – 분포를 이용한 통계적 추론 방법에 대해 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA11.html#자료의-입력",
    "href": "IDA/IDA11.html#자료의-입력",
    "title": "11장: 정규모집단에서의 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 9: 예제 4에 주어져 있는 자료를 가지고 파이썬을 이용하여 \n# μ 에 대한 90% 신뢰구간을 구하고 예제 4에서 실시한 검정도 \n# 파이썬을 이용하여 다시 시행한 후 그 결과를 비교하라. (p.346)\n\n# ------------------------------------------------------------------------------\n\n# 예제 4: 어느 도시의 보건복지과에서는 \n# 그 도시의 상수원인 어느 호수의 수질에 관심이 있다고 한다. \n\n# 수질을 나타내는 하나의 수치로 단위부피당 평균 세균수가 있는데, \n# 그 수가 200 이상이면 상수원으로 적합 하지 않다고 한다. \n\n# 호수의 열 군데에서 물을 떠서 조사한 결과 단위부피당 세균수가 다음과 같이 나타났다. \n# 이 자료로부터 호수의 단위부피당 평균 세균수(μ)가 200보다 적다고 주장할 수 있겠는가? (p.330)\n\nimport numpy as np \nbacteria = np.array([175, 190, 215, 198, 184, 207, 210, 193, 196, 180])\n\n#_______________________________________________________________________________\n\n# 예제 10: 다음에 주어진 자료를 파이썬을 이용하여 분석하고자 한다. (p.348)\n\nx = np.array([31, 35, 37, 38, 38, 38, 39, 40, 40, 41, 42, 43, 44, 44, 46, 48])"
  },
  {
    "objectID": "IDA/IDA11.html#t-분포",
    "href": "IDA/IDA11.html#t-분포",
    "title": "11장: 정규모집단에서의 추론",
    "section": "2. t 분포",
    "text": "2. t 분포\nStudent’s t Distribution 통계학에서 모집단의 표본 평균이 정규분포를 따르지 않는 경우에도 사용가능한 분포. 모집단의 분포가 N ( μ, σ2 ) 일 때 크기가 n 인 표본의 평균 x̄ 의 분포는 정확하게 N ( μ, σ2 / n ) 이다.\n이를 표준화한 것이 아래와 같다:\n일반적으로 σ 는 미지수이므로 이를 표본의 표준편차 s 로 추정하여 사용한다. 표본의 크기가 큰 경우, s 로 대체하여도 그 분포가 큰 영향을 받지 않는다.\n그러나 표본의 크기가 작은 경우, 대체하게 되면 표준화된 확률변수의 분포는 표준정규분포와 달라지게 되며, 이를 t 분포 라고 한다.\n정규모집단 N ( μ, σ2 ) 으로부터 임의추출된 표본을 X 1, …, X n 이라고 할 때, 표본 평균과 표본 분산을 아래와 같이 정의한다:\n위 정의가 성립할 때 아래 식이 성립한다:\n자유도가 (n – 1)인 t 분포를 따르고, 이를 기호로써 t (n – 1)로 표현한다.\n표준정규분포와의\n공통점 : 0 을 중심으로 대칭 & 종모양 분포 차이점 : 양 꼬리부분에 상대적으로 많은 확률이 존재 → 더 두꺼운 꼬리를 갖는다. 이때 자유도가 증가하면, t 분포의 꼬리는 표준정규분포의 꼬리에 가까워진다:\ndf = 1 : 자유도가 1인 경우, df = 5 : 자유도가 5인 경우"
  },
  {
    "objectID": "IDA/IDA11.html#모평균에-대한-추론",
    "href": "IDA/IDA11.html#모평균에-대한-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3. 모평균에 대한 추론",
    "text": "3. 모평균에 대한 추론\n표본의 크기가 작거나 모집단이 정규분포를 따르지 않는 경우, t – 분포를 사용한다. 이때는 Z α / 2​ 대신 t α / 2 (n − 1) ​을 사용하여 신뢰구간을 계산해야 한다:\n위 식을 μ 에 대해 정리하면:\n10장: 통계적 추론 → 3 - 5 → (2)번의 식은 위와 같이 수정되어야 한다.\n표본 크기가 작을수록\nt – 분포의 임계값이 커지므로, 신뢰구간이 넓어져 표본표준편차의 불확실성을 반영한다. t – 분포를 사용하여 신뢰구간을 더 넓게 잡으므로, 정확한 추정에 도움이 된다. 표본 크기가 충분히 크다면, t – 분포와 정규분포가 거의 같아지므로 이 경우에만, Z – 분포를 사용할 수 있다.\n따라서:\n또는 아래와 같이 나타낼 수 있다:"
  },
  {
    "objectID": "IDA/IDA11.html#가설-검정",
    "href": "IDA/IDA11.html#가설-검정",
    "title": "11장: 정규모집단에서의 추론",
    "section": "3-1. 가설 검정",
    "text": "3-1. 가설 검정\n검정통계량은 H 0 가 맞을 때 자유도가 ( n − 1 ) 인 t – 분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n검정통계량이 t 분포를 따르는 경우의 검정을 t 검정이라고 한다."
  },
  {
    "objectID": "IDA/IDA11.html#신뢰구간과-양측검정의-관계",
    "href": "IDA/IDA11.html#신뢰구간과-양측검정의-관계",
    "title": "11장: 정규모집단에서의 추론",
    "section": "4. 신뢰구간과 양측검정의 관계",
    "text": "4. 신뢰구간과 양측검정의 관계\nμ 에 대한 100 ( 1 − α ) % 신뢰구간은 아래와 같다:\nH 0 : μ = μ 0 에 대한 양측검정에서의 기각역은 유의수준이 α 일 때 아래와 같다:\n위 기각역의 여집합인 H 0 를 기각하지 못하고 받아들이는 영역을 ’ 채택영역 ’ 이라고 할 때\n이 채택영역은 다음과 같다:\n이를 μ 0 를 중심으로 풀어쓰면 다음과 같다:\n위 과정을 바탕으로 다음과 같은 결론을 내릴 수 있다:\n모수 θ 에 대한 100 ( 1 − α ) % 신뢰구간이 ( L, U ) 로 구해졌을 때, 가설 H 0 : θ = θ 0 대 H 1 : θ ≠ θ 0 에 대하여 유의수준 α 로 검정을 시행할 때의 결론을 의미한다."
  },
  {
    "objectID": "IDA/IDA11.html#모표준편차의-추론",
    "href": "IDA/IDA11.html#모표준편차의-추론",
    "title": "11장: 정규모집단에서의 추론",
    "section": "5. 모표준편차의 추론",
    "text": "5. 모표준편차의 추론\n모표준편차의 추정과 검정에서는 정규성 가정이 중요한 역할을 한다. 이 가정이 충족되지 않으면, 신뢰구간 계산이나 가설 검정의 결과를 신뢰할 수 없다.\n모표준편차 σ 를 추정하는 과정은 모분산 σ 2 에 대한 추정에서 출발하며, 이 모분산 σ 2 에 대한 추정에서 사용되는 것이 표본분산이다:\n\n5-1. 점추정\nPoint Estimation 모집단의 모수를 단일 값으로 추정하는 방법이다. 모표준편차 σ 의 경우, 점추정은 표본표준편차를 사용하는 방식이다:\n\n\n5-2. 구간추정\nInterval Estimation 모집단의 모수를 특정 신뢰수준에서 포함할 것으로 예상되는 구간을 제공하는 방법이다. 모표준편차 σ 에 대한 구간추정은 모분산에 대한 신뢰구간을 기반으로 하여 계산되며, 이때 모분산 s 2 의 신뢰구간을 구하기 위해 카이제곱 분포를 사용한다.\n\n\n5-3. 카이제곱 분포\nChi-Square Distribution 표본 분산을 모집단 분산과 비교하거나 범주형 변수 간의 독립성을 검정할 때 유용한 분포.\n자유도에 따라 그 형태가 달라지며, 자유도가 커질수록 정규 분포와 유사해진다. 아래 수식은 표본 분산을 모집단 분산으로 표준화한 것이다:\n자유도가 (n – 1)인 x2 분포를 따르고, 이를 기호로써 x2 (n – 1)로 표현한다. 모집단 σ 2 의 신뢰구간을 계산하기 위해 카이제곱 통계량을 사용한다: 위 분포로부터 구한 신뢰구간은 아래 식과 같다:\n위 식에서 괄호 안에 있는 부등식을 σ 2 을 중심으로 풀어 쓰면 다음과 같다:\n따라서 이 식으로부터 σ 2 의 100 ( 1 − α ) % 신뢰구간을 구하면 다음과 같다:\n표준편차 σ 는 σ 2 의 양의 제곱근이므로,그의 신뢰구간은 σ 2 의 신뢰구간의 경곗값의 제곱근을 취하여 얻을 수 있다: 결론적으로, σ 에 대한 100 ( 1 − α ) % 신뢰구간은 위와 같다.\n검정통계량은 H 0 이 맞을 때 자유도 df 인 카이제곱분포를 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 9의 ( 1 ) bacteria 에 대한 요약 통계량 계산하기.\n\nxbar_b = np.mean(bacteria);print(xbar_b) # 평균\n\nvar_b = np.var(bacteria, ddof=1);print(var_b) # 분산 (자유도 1 사용)\n\nsd_b = np.std(bacteria, ddof=1);print(sd_b) # 표준편차 (자유도 1 사용)\n\nmedian_b = np.median(bacteria);print(median_b) # 중앙값\n\n194.8\n172.62222222222226\n13.138577633146681\n194.5\n\n\n\nmin_b = np.min(bacteria);print(min_b) # 최솟값\n\nmax_b = np.max(bacteria);print(max_b) # 최댓값\n\nsum_b = np.sum(bacteria);print(sum_b) # 합계\n\nn = bacteria.size;print(n) # 데이터 개수\n\n175\n215\n1948\n10\n\n\n예제 9의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수 t – 분포의 백분위수 함수\n\nfrom scipy import stats \n\nse_b = stats.sem(bacteria); print(se_b) # 표본표준오차\n\n# 유의수준 0.1에 해당하는 t-분포의 임계값 \nt_alpha = stats.t.ppf(1 - 0.1 / 2, n - 1); print(t_alpha)\n\ninterval = t_alpha * se_b;print(interval) # 신뢰구간의 범위를 계산\n\nCI = [xbar_b - interval, xbar_b + interval]; print(CI) # 신뢰구간\n\n4.154783053568769\n1.8331129326536335\n7.6161865478670645\n[np.float64(187.18381345213294), np.float64(202.41618654786708)]\n\n\n90 % 신뢰구간은 194.8 ± 7.616 임을 알 수 있다.\n예제 9의 ( 3 ) 검정하고자 하는 가설은 H 0 : μ = 200 대 H 1 : μ &lt; 200 이며, 표본의 크기는 10 이다.\n\ntval = (xbar_b - 200) / se_b;print(tval) \n\n# 단측검정: 귀무가설 μ=200, 대립가설 μ&lt;200\npval = stats.t.cdf(tval, n - 1);print(pval)\n\n## 해석: P–값이 0.1211 로 유의수준 5% 에서 귀무가설을 기각할 수 없으므로\n## 주어진 10 개의 자료로부터 호수의 단위 부피당 평균세균수가 200 보다 \n## 적다고 안심할 수 없다.\n\n-1.2515695604210733\n0.12113884687382763\n\n\n예제 10의 ( 1 ) x 에 대한 요약 통계량 계산하기.\n\nxbar_x = np.mean(x);print(xbar_x) # 평균\n\nvar_x = np.var(x, ddof=1);print(var_x) # 분산 (자유도 1 사용)\n\nsd_x = np.std(x, ddof=1);print(sd_x) # 표준편차 (자유도 1 사용)\n\nmedian_x = np.median(x);print(median_x) # 중앙값\n\nmin_x = np.min(x);print(min_x) # 최솟값\n\nmax_x = np.max(x);print(max_x) # 최댓값\n\nsum_x = np.sum(x);print(sum_x) # 합계\n\nn = x.size;print(n) # 데이터 개수\n\n40.25\n18.2\n4.266145801540309\n40.0\n31\n48\n644\n16\n\n\n예제 10의 ( 2 ) x 에 대한 정규확률그림 그리기.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 정규확률그림 그리기\nsm.qqplot(x, line='s')\nplt.title(\"Normal Q-Q Plot\")\n\nText(0.5, 1.0, 'Normal Q-Q Plot')\n\n\n\n\n\n\n\n\n\n예제 10의 ( 3 ) 모평균 μ 에 대한 95% 신뢰구간을 구한다.\n\nfrom scipy import stats \n\nse = stats.sem(x);print(se) # 표준편차\n\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n - 1);print(t_alpha) # 95% 신뢰구간을 위한 t값 \n\ninterval = t_alpha * se;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_x - interval, xbar_x + interval];print(CI) # 신뢰구간\n\n1.0665364503850772\n2.131449545559323\n2.2732686324957263\n[np.float64(37.97673136750427), np.float64(42.52326863249573)]\n\n\n95 % 신뢰구간은 40.25 ± 2.273 임을 알 수 있다.\n예제 10의 ( 4 ) 검정하고자 하는 가설은 H 0 : μ = 38 대 H 1 : μ &gt; 38 이며, 표본의 크기는 16 이다.\n\ntval = (xbar_x - 38) / se;print(tval) # 표본표준오차\n\npval = 1 - stats.t.cdf(tval, n - 1);print(pval) # p값\n\n## 해석: P–값이 0.026 이므로 유의수준 5% 에서 귀무가설을 기각하게 된다.\n## 따라서, 평균이 38 보다 크다고 할 수 있다.\n\n2.109632539223229\n0.026050840503660355\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA09.html",
    "href": "IDA/IDA09.html",
    "title": "9장: 표집분포",
    "section": "",
    "text": "Reporting Date: July. 21, 2024\n주어진 표본을 통해 모집단의 성격을 알아내는 과정을 추론 ( Inference )이라 한다. 그 통계적 추론에서 모집단의 특성을 추정하거나 가설 검정을 수행할 때, 사용되는 표집분포에 대해 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA09.html#자료의-입력",
    "href": "IDA/IDA09.html#자료의-입력",
    "title": "9장: 표집분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 4:  0 ~ 9 까지  10 개의 정수값을 균등하게 갖는 모집단이 있다.\n# (위 모집단의 분포를 이산균등분포라고 한다.) \n\n# 예시: 전화번호 끝자리 번호의 분포\n\n# (1) 여기에서 크기가  5 인 표본을  100 번 뽑아서 \n# (2) 매번 추출된 표본에서 표본평균을 구하고, \n# (3) 그 평균들을 가지고 히스토그램을 그려라. (p.268)"
  },
  {
    "objectID": "IDA/IDA09.html#통계량",
    "href": "IDA/IDA09.html#통계량",
    "title": "9장: 표집분포",
    "section": "2. 통계량",
    "text": "2. 통계량\nStatistic\n표본의 관측값들에 의하여 결정되는 양. 표본을 이용하여 모수에 대해 추론할 때, 표본에서 이용되는 적절한 양을 의미한다.\n위 표본평균은 표본 X₁, …, Xn의 관측값에 의하여 결정되므로 “통계량” 이다.\n마찬가지로 표본상관계수나 표본표준편차도 표본으로부터 계산하므로 통계량이다. 표본에서 계산한 통계량은 모수의 값을 추측하는데 쓰이게 되는데, 이때 유념하여야 할 3가지 조건이 있다.\n\n표본은 단지 모집단의 한 부분에 불과하다. 그러므로, 표본으로부터 계산된 통계량의 값은 모수의 참 값과는 일반적으로 같지 않다:\n통계량의 값은 추출된 표본의 영향을 받는다:\n다른 표본을 추출할 때마다 통계량의 값은 변한다:"
  },
  {
    "objectID": "IDA/IDA09.html#표집분포",
    "href": "IDA/IDA09.html#표집분포",
    "title": "9장: 표집분포",
    "section": "3. 표집분포",
    "text": "3. 표집분포\nSampling Distribution\n통계량의 확률분포.\n여러 표본이 있을 경우, 통계량의 값들은 표집분포에 따라 변화하게 된다. 그리고 그 통계량이 편향 추정량인지 불편 추정량인지에 따라 표집분포의 성질이 달라진다.\n\n3-1. 불편추정량\nUnbiased Estimator 분포의 평균값이 추정하려는 모수와 일치하는 추정량. 불편추정량의 경우, 표집분포의 평균 (중심) 은 모수와 같다:\n위 식이 성립할 때, 아래 식이 성립한다.\n통계량의 분포는 모집단의 분포 ( 분산 ) 와 표본의 크기 n 에 영향을 받는다:\n표집분포에서 사용되는 표본 크기 n 은 통계량의 분산과 추정의 정확성에 중점을 둔다. 아래에서 설명할 중심극한정리의 n 과는 다른 역할을 가진다.\n\n\n3-2. 편의추정량\nBiased Estimator\n분포의 평균값이 추정하려는 모수와 일치하지 않는 추정량.\n편의추정량의 경우, 표집분포의 평균은 모수와 다르다:\n위 식이 성립할 때, 아래 식이 성립한다. 이 경우, 편향 ( Bias ) 도 고려해야 한다:\n표본 크기를 늘리는 것만으로는 (불편추정량과 같은) 정확한 추정을 보장할 수 없다. 따라서, 추정량의 편향을 최소화하는 것이 중요하다.\n\n\n3-3. 임의표본\nRandom Sample 일반적으로 크기가 큰 모집단으로부터 임의추출된 크기가 n 인 표본 X₁, …, Xn. 위 표본은 서로 독립이고, 모두 모집단의 분포와 동일한 분포를 갖는 것으로 간주된다.\n\n\n3-4. 표본평균\nSample Mean\n모평균 ( 모집단의 평균 ) 에 대한 추론에서 중요한 역할을 한다. 모평균은 모집단의 중심을 나타내는 수치로서 가장 많이 사용된다.\n표본평균은 다음과 같이 정의된다:\n표본 평균의 기댓값 (평균) , 분산, 표준편차는 아래 식과 같다:"
  },
  {
    "objectID": "IDA/IDA09.html#중심극한정리",
    "href": "IDA/IDA09.html#중심극한정리",
    "title": "9장: 표집분포",
    "section": "4. 중심극한정리",
    "text": "4. 중심극한정리\nCentral Limit Theorem, CLT 표본의 크기 n 이 충분히 클 때, ( 보통 30 이상 ) 모집단 분포 ( 연속이든 이산이든, 대칭이든 비대칭이든 ) 와 관계없이 표본 평균의 분포가 정규분포에 가까워진다.\n이 정리는 많은 통계적 방법의 이론적 기초가 된다.\n중심극한정리는 다음과 같이 정의된다:\nZ : 표준 정규분포를 따르는 변수로, 표본 평균의 표준화된 형태를 나타낸다.\n중심극한정리에서 사용되는 표본 크기 n 은 표본 평균의 분포가 정규분포에 근사하게 되는 성질에 중점을 둔다.\n예제 4 ( 1 ) 크기가 5 인 표본을 100 번 뽑는다:\n\nimport numpy as np \n\na = np.random.randint(0, 100, size=5) \nb = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nc = np.random.randint(0, 100, size=5) \n\nnp.random.seed(1) # seed의 숫자를 1로 고정한 경우\nd = np.random.randint(0, 100, size=5) \n\nprint(\"a :\", a) \nprint(\"b :\", b) \nprint(\"c :\", c)\nprint(\"d :\", d)\n\n## 해석: c, d에서 동일한 난수들이 추출됨을 확인할 수 있다.\n\na : [76 96 89 53 48]\nb : [77  2 23 31 64]\nc : [37 12 72  9 75]\nd : [37 12 72  9 75]\n\n\n( 2 ) 표본평균을 출력한다.\n\nimport numpy as np \n\nm = [] \n\nnp.random.seed(1234) \nfor i in range(100): \n    sample = np.random.randint(0, 10, size = 5) \n    m.append(np.mean(sample))\n    \nm = np.array(m)\nprint(m)\n\n[5.2 6.4 4.4 3.  5.  1.8 3.  3.4 5.4 7.8 3.4 4.8 4.8 4.8 6.4 4.8 4.4 6.\n 2.8 4.8 4.2 4.4 6.8 1.8 6.  4.6 3.2 2.4 2.8 6.2 3.2 6.8 5.8 5.8 4.4 5.\n 4.4 6.  3.6 4.8 4.8 4.  4.4 5.6 5.2 6.2 1.8 3.8 1.4 6.4 3.8 5.2 4.4 4.4\n 4.6 0.4 3.  5.8 2.2 4.  4.4 3.6 5.2 1.6 3.2 5.8 6.8 3.8 6.2 2.6 2.  4.8\n 2.6 6.  7.6 5.6 6.  3.6 4.2 3.8 5.2 3.4 8.2 4.6 6.8 3.8 4.2 3.8 4.6 2.4\n 4.  4.4 6.  4.4 2.6 3.4 6.  4.6 4.6 2.8]\n\n\n( 3 ) 그 평균을 가지고 히스토그림을 그려라.\n\nimport matplotlib.pyplot as plt \n\nplt.hist(m, bins=7)\nplt.xlabel('m') \nplt.ylabel('Frquency') \nplt.title('Historam of m')\n\n## 해석: 정규분포와 유사한 종 모양 분포를 띄는 것을 통해\n## 정규분포에 가까우리라 예상할 수 있다.\n\nText(0.5, 1.0, 'Historam of m')\n\n\n\n\n\n\n\n\n\n( 4 ) 데이터의 정규성을 보다 정확하게 평가하기 위해, 8장에서 배운 정규확률그림을 그려본다.\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nsm.qqplot(m, line='s')\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 점들이 거의 직선상의 있으므로\n## 어느 정도 정규분포를 따른다고 할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA07.html",
    "href": "IDA/IDA07.html",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "",
    "text": "Reporting Date: July. 15, 2024\n모집단의 구성원들이 두 그룹으로 나누어져 있는 경우의 표본추출에서 광범위하게 쓰이는 확률모형과 그의 특징 및 관련된 다른 확률모형들을 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA07.html#자료의-입력",
    "href": "IDA/IDA07.html#자료의-입력",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7 :  어떤 초등학교에서  10 년간 조사결과\n# 평균적으로  4 % 의 학생이 색맹인 것으로 나타났다고 한다. (p.213)\n\n# 올해에도 색맹인 학생의 비율이 예년과 같다고 할 때,\n# 임의로 추출된  200 명의 학생 중 색맹인 학생이  10 명 이하일 확률은 얼마인가?"
  },
  {
    "objectID": "IDA/IDA07.html#베르누이-시행",
    "href": "IDA/IDA07.html#베르누이-시행",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "2. 베르누이 시행",
    "text": "2. 베르누이 시행\nBernoulli Distribution\n모집단의 각 구성원이 두 그룹 중 하나에 속하는 경우, 각각의 구성원이 특정 그룹에 속할 확률 p 와 속하지 않을 확률 1 − p 를 따르는 이산 확률 분포.\n시 행 ( Trial ) : 매번 반복되는 추출( 실험 )\n2 개의 가능한 결과 중, 하나는 성공 ( Success, S ), 다른 하나는 실패 ( Failure, F ) 로 이름을 붙인다.\n이는 시행의 결과가 2 개 뿐임을 강조하며, 보통 우리가 관심이 있는 결과에 성공이란 이름을 붙인다.\n각 시행은 독립으로, 각 시행의 결과가 다른 시행의 결과에 영향을 미치지 않는다.\n일반적으로 복원 추출 ( Sampling With Replacement )로 간주된다. 각 시행 후 다시 원래의 상태로 복귀하여, 다음 시행에 영향을 주지 않는다.\n예제 7에 경우, 추출되는 학생은 색맹 ( S ) 또는 정상 ( F )으로 나눌 수 있다."
  },
  {
    "objectID": "IDA/IDA07.html#이항분포",
    "href": "IDA/IDA07.html#이항분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "3. 이항분포",
    "text": "3. 이항분포\nBinomial Distribution\n각 시도가 성공 또는 실패 두 가지 결과 중 하나를 가지는 독립적인 시행이 n 번 반복될 때, 성공 횟수를 나타내는 확률분포.\n성공할 확률이 p 인 베르누이 시행을 n 번 반복할 때 일어나는 성공의 횟수가 X 라면, 이 확률변수 X 는 모수가 ( n, p )인 이항분포를 따른다.\n모수(Parameter): 우리가 관심을 가지는 수치\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, x = 0, 1, …, n에 대하여 확률질량함수( PMF ) 는 다음과 같다:\n이 항 계 수 ( Binomial Coefficient )\n주어진 수의 집합에서 특정한 수의 원소를 선택하는 방법의 수. 조 합 ( Combination ) 으로도 알려져 있다.\n예제 7에서 주어진 정보를 바탕으로, 아래와 같이 이항 분포의 모수 ( 파라미터 ) 를 설정할 수 있다:\n성공 확률: p = 0.04 ( 학생이 색맹일 확률 ) 시도 횟수: n = 200 ( 추출된 학생 수 ) 성공 횟수: k ≤ 10 ( 색맹인 학생 수 )\n즉, 확률변수 X 를 200 명 중 색맹인 학생의 수 라고 하면, X는 모수가 ( n, p ) = ( 200, 0.04 ) 인 이항분포를 따르게 된다.\n이때, k ≤ 10 인 경우의 확률을 구해야 하므로, 누적 분포 함수 ( CDF ) 를 사용해야 한다:\n원하는 확률을 계산하려면, 다음 식을 계산해야 한다:\n다만, 위 식을 직접 계산하기에는 번거로운 면이 있다. 부록의 이항분포표에서도 n = 25 까지가 최대이기 때문이다.\n이럴 경우, 아래에서 설명할 포아송분포로 근사하여 계산할 수 있다.\n\nfrom scipy import stats\nstats.binom.cdf(10, 200, 0.04)\n\n## 출력된 값 &gt; 0.8199789826230907\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.820, 즉 약 82.0%이다.\n\nnp.float64(0.8199789826230907)\n\n\n\n3-1. 이항분포의 기댓값과 표준편차\n확률변수 X 의 분포가 X ~ Bin ( n, p ) 일 때, 기댓값, 분산, 표준편차는 아래와 같다:"
  },
  {
    "objectID": "IDA/IDA07.html#초기하분포",
    "href": "IDA/IDA07.html#초기하분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "4. 초기하분포",
    "text": "4. 초기하분포\nHypergeometric Distribution\n유한한 모집단에서 비복원 추출 ( Sampling Without Replacement )을 통해 성공과 실패를 구분하는 확률 분포이다.\n이는 이항 분포와 유사하지만, 추출이 비복원 방식이라는 점에서 차이가 있다.\n이 경우 각 추출과정이 서로 영향을 주게 되므로, 베르누이 시행의 독립성을 충족하지 못한다.\n유한한 모집단에서 비복원 추출을 하는 경우, 성공의 횟수를 X 라고 할 때, 확률변수 X 의 분포를 초기하분포라고 한다.\n그의 확률질량함수 ( PMF ) 는 아래와 같다:\n이때, n은 D 혹은 ( N − D ) 보다 작거나 같은 수로 가정한다.\nN : 모집단의 크기 D : 모집단에서의 성공 횟수 n : 추출된 표본의 크기 X : 표본에서의 성공 횟수\n위 초기하분포식에서 이항분포처럼 성공의 확률 p 를 다음과 같이 정의하면:\n초기하분포의 기댓값, 분산, 표준편차는 아래와 같다:\n초기하분포와 이항분포의 기댓값 ( 평균 ) 은 같은 형태 ( np ) 를 가진다.\n분산과 표준편차에서는 모집단 크기와 표본 크기에 따른 조정이 포함된다는 점에서 차이가 있다.\n이러한 조정을 유한모집단의 수정요인(FPC)이라고 한다. 이것은 통계적 추정에서 사용되는 보정 요소이다.\n표본이 모집단에 비해 상대적으로 작을 때 (일반적으로 표본 크기가 전체 모집단의 5% 미만인 경우) 발생할 수 있는 추정의 오차를 줄이기 위해 이러한 수정요인을 사용한다. 결과적으로 베르누이 시행을 근사하게 따른다고 가정하고 이항분포와 동일하게 취급하게 된다."
  },
  {
    "objectID": "IDA/IDA07.html#포아송분포",
    "href": "IDA/IDA07.html#포아송분포",
    "title": "7장: 이항분포와 그에 관련된 분포들",
    "section": "5. 포아송분포",
    "text": "5. 포아송분포\nPoisson Distribution\n특정 시간 동안 혹은 특정 공간에서 발생하는 사건의 수를 모델링하는 확률 분포. 주로 일어나는 사건의 횟수에 대한 확률을 예측하는 데 사용된다.\n즉, 매 순간 사건 발생이 가능하지만, 매 순간 사건 발생의 확률이 매우 작은 경우를 의미한다.\n\n119 구조대에 시간당 걸려오는 전화횟수\n국내 발생하는 진도 4 이상 지진의 횟수\n\n포아송분포를 적용하기 위해서는 3 가지 가정을 만족해야 한다.\n람다(Λ, λ: 그리스어 알파벳의 11번째 글자) 1 . 주어진 구간에서 사건의 평균 발생횟수의 확률분포는 구간의 시작점에는 관계가 없고, 구간의 길이에만 영향을 받는다.\n2 . 한 순간에 2 회 이상의 사건이 발생할 확률은 거의 0 에 가깝다.\n3 . 한 구간에서 발생한 사건의 횟수는 겹치지 않는 다른 구간에서 발생하는 사건의 수에 영향을 받지 않는다.\n확률변수 X 가 평균이 m ( λ ) 인 포아송분포를 따른다고 하면 확률질량함수 ( PMF ) 는 아래와 같다:\n위 x값의 범위는 사건 발생 횟수를 사전에 정확하게 알 수 없다는 점을 반영한다.\n예제 7의 X 는 근사적으로 평균 m = 8 인 포아송분포를 따르게 된다. ( m ( λ ) = np = 200 × 0.04 = 8 )\n부록의 포아송분포표에서 m = 8 열과 c = 10 행을 찾아보면, 원하는 확률인 P ( X ≤ 10 ) = 0.816 을 찾을 수 있다.\n\nfrom scipy import stats\nstats.poisson.cdf(10, 8)\n\n## 해석: 임의로 추출된 200명의 학생 중 색맹인 학생이 \n## 10명 이하일 확률은 약 0.816, 즉 약 81.6%이다.\n\nnp.float64(0.8158857925585467)\n\n\n\n참고용: Finite Population Correction Factor FPC 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA05.html",
    "href": "IDA/IDA05.html",
    "title": "5장: 확률",
    "section": "",
    "text": "Reporting Date: July. 8, 2024\n통계적인 추론을 통해서도 모집단에 대한 다양한 정보를 얻을 수 있다. 그 통계적 추론의 기초가 되는 확률이론에 대해 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA05.html#사건의-확률",
    "href": "IDA/IDA05.html#사건의-확률",
    "title": "5장: 확률",
    "section": "1. 사건의 확률",
    "text": "1. 사건의 확률\n동일 조건하에서 한 가지 실험을 반복할 때, 전체 실험 횟수에서 그 사건이 일어나리라고 예상되는 횟수의 비율을 말한다. \n사건을 \\(A\\) 라고 하면, 사건 \\(A\\) 의 확률은 \\(P(A)\\) 로 표시한다.\n\n1-1. 표본공간\nSample Space: Ω\n한 실험에서 나올 수 있는 모든 결과들의 모임. 유한표본공간 ( Finite Sample Space )\n주사위 던지기의 표본공간\n연속표본공간 ( Continuous Sample Space )\n0과 1 사이의 모든 실수"
  },
  {
    "objectID": "IDA/IDA05.html#근원사건",
    "href": "IDA/IDA05.html#근원사건",
    "title": "5장: 확률",
    "section": "1-2. 근원사건",
    "text": "1-2. 근원사건\nElementary Outcomes: ω ₁ , ω ₂ … \n표본공간을 구성하는 개개의 결과.\n주사위 던지기의 근원사건\n\n1-3. 사건\nEvent: A, B, …\n표본공간의 부분집합으로, 어떤 특성을 갖는 결과들의 모임 (=근원사건들의 집합)\n주사위 던지기의 사건 A, B"
  },
  {
    "objectID": "IDA/IDA05.html#확률의-법칙",
    "href": "IDA/IDA05.html#확률의-법칙",
    "title": "5장: 확률",
    "section": "2. 확률의 법칙",
    "text": "2. 확률의 법칙\n위 정의로부터 확률의 특성을 유추할 수 있다.\n사건 A 가 일어날 확률은, 사건 A 에 속하는 근원사건이 일어날 확률의 “합” 과 같다. Ω 를 하나의 사건이라고 하면, 이 사건은 “반드시” 일어나므로 확률은 “1” 이 되어야 한다."
  },
  {
    "objectID": "IDA/IDA05.html#확률의-계산",
    "href": "IDA/IDA05.html#확률의-계산",
    "title": "5장: 확률",
    "section": "3. 확률의 계산",
    "text": "3. 확률의 계산\n\n3-1. 균일 확률\n주사위 던지기에서, 각 숫자가 나올 확률 Ω 가 k 개의 원소로 이루어져 있고, 각 근원사건이 일어날 가능성이 “동일” 하다고 가정할 때, 근원사건 중 하나가 일어날 확률은 1 / k 로 주어진다.\n또 사건 A 가 m 개의 근원사건으로 이루어져 있다면, 사건 A 가 일어날 확률은 위와 같다.\n\n\n3-2. 상대도수 수렴치로서의 확률\n주사위를 60번 던져 10번 2가 나왔을 때, 2가 나올 확률 동일한 실험 N 회를 반복할 때, 사건 A 의 상대도수는 위와 같이 표현된다.\nN 이 증가함에 따라 상대도수가 “일정한 값으로 수렴” 한다면, 그 값으로 사건 A 가 일어날 확률 P ( A ) 를 추정한다."
  },
  {
    "objectID": "IDA/IDA05.html#확률-법칙",
    "href": "IDA/IDA05.html#확률-법칙",
    "title": "5장: 확률",
    "section": "4. 확률 법칙",
    "text": "4. 확률 법칙\n여 사 건 ( Complementary Event ): 사건 A 가 일어나지 않는 사건 합 사 건 ( Sum Event ): 사건 A 또는 B 가 일어나는 사건 곱 사 건 ( Product Event ): 사건 A 또는 B 가 동시에 일어나는 사건 배 반 사 건 ( Exclusive Event ): 두 사건이 동시에 일어날 수 없는 경우"
  },
  {
    "objectID": "IDA/IDA05.html#조건부-확률",
    "href": "IDA/IDA05.html#조건부-확률",
    "title": "5장: 확률",
    "section": "5. 조건부 확률",
    "text": "5. 조건부 확률\nConditional Probability\n한 사건의 결과가 다른 사건의 발생에 영향을 미치는 확률.\n사건 B가 발생했을 때, 사건 A가 발생할 확률:\n곱 사 건 의 확 률 법 칙 ( Multiplication Rule for Probability ) 두 사건이 동시에 발생할 확률을 계산하는 방법. 이 법칙은 두 사건이 독립적인지 여부에 따라 다르게 적용된다:\n\n5-1. 표본공간의 분할\nPartition\n사건 A₁, A₂, …, An 이 서로 배반사건이고, Ω = A₁ ∪ … ∪ An 일 때, 사건 A₁, A₂, …, An 을 Ω 의 분할이라고 한다:\n이때, 각각의 부분집합은 서로 겹치지 않는다:\n예시: 주사위를 던지는 실험에서, Ω 를 두 개의 부분집합으로 나눌 수 있다.\nA₁ ​: 홀수가 나오는 경우 { 1, 3, 5 } A₂ ​: 짝수가 나오는 경우 { 2, 4, 6 }\n이 경우, Ω 는 다음과 같이 표현된다:\n\n\n5-2. 총확률의 법칙\nLaw of Total Probability\n표본공간의 분할 개념을 사용하여 전체 확률을 계산하는 방법.\n사건 A₁, A₂, …, An 이 표본공간의 분할일 때, 임의의 사건 B 의 확률 “P(B)” 는 다음과 같이 계산할 수 있다:\n이를 조건부 확률 ( 종속사건 ) 을 사용하여 다시 쓰면:\n예시: 사건 B 를 “주사위 눈이 4 이하인 사건” 으로 정의하면,\nP(B) = P({ 1, 2, 3, 4 }) = ⅔ P(A₁)​ = P({ 1, 3, 5 }) = ½ P(A₂)​ = P({ 2, 4, 6 }) = ½\n이 경우, P ( B ) 는 다음과 같이 표현된다:\n각 확률 값을 대입해보면:\n\n\n5-3. 베이즈 정리\nBayes’ rule\n사건 A₁, A₂, …, An 이 Ω 의 분할일 때, 임의의 사건 B 에 대하여 다음 식이 성립한다:\n예시: 사건 B 가 발생한 후, 사건 A₁ ​ 이 발생할 확률:\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA03.html",
    "href": "IDA/IDA03.html",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "",
    "text": "Reporting Date: June. 30, 2024\n연속형 자료가 어떤 값을 중심으로 분포되어 있는가를 나타내는 중심위치의 측도, 각 자료가 중심위치의 값으로부터 흩어진 정도를 나타내는 퍼진 정도의 측도 등을 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA03.html#자료의-입력",
    "href": "IDA/IDA03.html#자료의-입력",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제13: 정량 100인 음료수 80병을 임의로 추출하여 그 내용물의 실제 측정된 양을 잰 자료이다.(p.42)\n\nimport numpy as np\n\n# 변수 drink에 NumPy 배열을 할당\ndrink = np.array([98, 99, 100, 99, 99.4, 101.7, 98.8, 101.8, 101.5, \n                 101.8, 102.6, 101, 98.8, 101.4, 99.7, 99.7, 99.7, \n                 100.9, 98.6, 101.4, 102.1, 102.9, 100.8, 101.8, \n                 100, 101.2, 100.5, 101.2, 100.1, 101.6, 101.3, 99.9, \n                 99.4, 99.3, 99.4,101.6, 96.1, 100, 99.7, 99.1, 100.7, \n                 100.8, 100.8, 95.5,100.1, 100.5, 98.9, 99.9, 96.8, \n                 102.4, 100, 103.7, 101.4,99.7, 97.4, 99.5, 97.5, \n                 99.9, 100.3, 100.2, 101.5, 99.4, 99.7, 98.2, 100.3, \n                 100.2, 100.5, 100.4, 101.5, 98.4, 101.4, 98.8, 100.9, \n                 101.1, 100.9, 98.1, 98.7, 99.2, 98.1, 97.2])\n\n중심위치의 측도"
  },
  {
    "objectID": "IDA/IDA03.html#평균-mean",
    "href": "IDA/IDA03.html#평균-mean",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "2. 평균 (Mean)",
    "text": "2. 평균 (Mean)\n총 자료의 개수 \\(n\\) 을 모든 관측값 \\(x_1, x_2, \\dots , x_n\\) 의 합으로 나눈 값. 이를 “산술 평균” 이라고도 하며, 공식으로 표현하면 다음과 같다:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n표본 평균은 관측값의 산술평균이며, “극단값” 에 영향을 받는다.\n\n표본 평균: 전체 데이터인 “모집단” 에서 추출한 일부 데이터의 평균을 의미한다. (예시1) 예제13 에서 추출된 80 병의 음료수는 표본에 해당된다.\n극단값(Outlier): 데이터 집합에서 다른 값들과 현저히 다른 값들을 의미한다. (예시2) 데이터 집합 { 11, 12, 13, 300 } 에서 극단값은 300 이다.\n\n\n# 2장의 예제 13에서 주어진 음료수 한 병의 부피 데이터를 기반으로 \n# 평균, 중앙값, 분산, 표준편차, 범위, 사분위수범위를 파이썬을 이용하여 계산하라.(p.83)\n\n## numpy 모듈을 이용하여 계산할 수 있음. ##\n\n# 평균\nprint(np.mean(drink))\n\n100.04125"
  },
  {
    "objectID": "IDA/IDA03.html#중앙값median",
    "href": "IDA/IDA03.html#중앙값median",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "3. 중앙값(Median)",
    "text": "3. 중앙값(Median)\n전체 관측값을 크기 순서로 배열했을 때, 가운데에 위치한 값.\n데이터의 개수가 홀수 일 경우, 중앙에 위치한 값이, 짝수 일 경우, 중앙에 위치한 두 값의 평균 이 중앙값이 된다.\n항상 중앙의 값을 채택하므로, 극단값의 영향을 받지 않는다. 따라서, 평균과 값이 다를 수 있다.\n\n# 중앙값 계산\nprint(np.median(drink))\n\n100.05\n\n\n퍼진 정도의 측도"
  },
  {
    "objectID": "IDA/IDA03.html#분-산-variance",
    "href": "IDA/IDA03.html#분-산-variance",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "4. 분 산 (Variance)",
    "text": "4. 분 산 (Variance)\n확률 분포나 데이터 집합의 산포도(분포도)를 나타내는 통계적 측도. 주어진 데이터의 개별 값들이 평균으로부터 얼마나 멀리 퍼져 있는지를 나타내는 지표로 사용된다.\n관측값이 \\(x_1, x_2, \\dots , x_n\\) 이고, 표본평균이 \\(\\bar{x}\\) 일 때, 표본분산은 다음과 같다:\n계산과정 평 균 과 의 거 리 : 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지를 계산한다. (예시1) xi – x̄ = 98 – 100.04 = – 2.04 제 곱 : 거리를 제곱하여 모든 값을 양수로 만들고, 거리의 상대적 크기를 더 잘 식별하게 한다. (예시2) – 2.04² = 4.1616 OR 2601/625\n평균제곱 오차의 평균: 제곱한 값을 모두 더하고, 데이터의 개수로 나눈 값이다.\n\n4-1. 편차(Deviation):\n각 관측값과 평균의 차이\n편차의 합은 항상 0 이므로, 편차의 평균도 항상 0 이다. 그 이유는, 양수 편차들의 합과 음수 편차들의 합이 항상 상쇄 되기 때문이다. 퍼진 정도를 측정함에 있어, 멀리 떨어진 정도를 나타내는 편차의 크기가 중요하므로,  따라서, 편차에서 부호를 없애는 방법으로 “제곱” 을 택한 것이다.\n\n\n4-2. 자유도\nDegrees of Freedom\n위키백과: 통계적 추정을 할 때, 표본자료 중 모집단에 대한 정보를 주는 “독립적인 자료의 수”. 나무위키: 추정해야 할 미지수의 개수를 내가 가진 정보의 수에서 뺀 값.\n모집단의 분산인 경우: “모든 데이터가 사용” 되므로, 자유도를 고려할 필요없이 n 으로 나누면 된다.\n표본의 분산인 경우: 표본은 모집단에서 추출된 “상대적으로 적은 규모의 집단” 이다.\n이 작은 규모의 집단을 바탕으로 계산된 편차의 제곱합은 모집단을 바탕으로 한 것과 “차이” 가 있을 수 밖에 없다.\n우리는 편차의 크기를 나타내는 편차의 제곱합을 “정확히 구하는 것이 목적” 이므로, 표본 분산이 모집단의 분산을 “과소 추정하지 않도록” 보정하여, “통계적 추정의 정확성” 을 높이기 위해 n 이 아닌 n – 1 을 사용하는 것이다.\n\n# 분산 계산 (표본의 분산, 자유도를 1로 설정 = n-1)\nprint(np.var(drink, ddof=1))\n\n# 분산 계산 (모집단의 분산, 자유도 고려X = n)\n# print(np.var(drink, ddof=0))\nprint(np.var(drink))\n\n2.316125000000001\n2.287173437500001\n\n\n자유도를 고려할 때, 표본의 분산이 더 “크게” 나타나며, 이는 표본 내의 “변동성” 을 보다 정확하게 반영하고 있음을 보여준다. 자유도를 1로 설정하지 않는 것은 n – 1 로 나누지 않는 것과 같으므로, 이점을 주의한다."
  },
  {
    "objectID": "IDA/IDA03.html#표준편차",
    "href": "IDA/IDA03.html#표준편차",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "5. 표준편차",
    "text": "5. 표준편차\nStandard Deviation\n분산의 제곱근으로, 데이터의 변동성을 “원래 데이터의 단위” 로 나타낸 값. 이는 분산의 계산과정 중 편차의 제곱합으로 인해 증가된 값을 바로잡아 준다.\n\n# 표준편차 계산 (표본의 표준편차, 자유도를 1로 설정)\nprint(np.std(drink, ddof=1))\n\n# 표준편차 계산 (모집단의 표준편차)\nprint(np.std(drink))\n\n1.521882058505192\n1.5123403841397614\n\n\n표준편차에서도 분산과 동일한 이유로 자유도가 주어진다."
  },
  {
    "objectID": "IDA/IDA03.html#범위",
    "href": "IDA/IDA03.html#범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "6. 범위",
    "text": "6. 범위\nRange\n관측값에서 가장 큰 값과 가장 작은 값의 차이.\n장점: 간편하게 구할 수 있고, 해석이 용이하다. 단점: 양 끝점에 의해서만 결정되므로, 중간에 위치한 관측값들이 “전혀 반영되지 않는다.” 그러므로, “극단값” 에 영향을 받는다.\n\n# 범위 계산 (최대값 - 최소값)\nprint(np.max(drink) - np.min(drink))\n\n8.200000000000003"
  },
  {
    "objectID": "IDA/IDA03.html#사분위수범위",
    "href": "IDA/IDA03.html#사분위수범위",
    "title": "3장: 수치를 통한 연속형 자료의 요약",
    "section": "7. 사분위수범위",
    "text": "7. 사분위수범위\nQuartile\n전체 관측값을 작은 순서로 배열 하였을 때, 전체를 사등분 하는 값.\n사분위수 제1 사분위수: Q ₁ = 제25 백분위수 = 25% 제2 사분위수: Q ₂ = 제50 백분위수 = 50% = ” 중 앙 값 ” 제3 사분위수: Q ₃ = 제75 백분위수 = 75%\n계산방법\n\n7-1. 사분위수범위\nInterquartile Range, IQR\n\n# 사분위수 범위 계산 (IQR, Interquartile Range)\nq1, q3 = np.percentile(drink, [25, 75])  # 25%와 75% 사분위수 계산\nprint(q3 - q1)\n\n2.0250000000000057\n\n\npandas 모듈의 DataFrame 객체 내의 멤버 함수인 describe() 함수를 이용하면, 위의 통계량을 한 번에 계산할 수 있다.\n\nimport pandas as pd\n\n# 데이터프레임 생성 및 요약 통계량 계산\ndrink_df = pd.DataFrame(drink)  # 데이터를 pandas의 DataFrame으로 변환\nsummary = drink_df.describe()  # 기술 통계량 요약 계산\nprint(summary)\n\n                0\ncount   80.000000\nmean   100.041250\nstd      1.521882\nmin     95.500000\n25%     99.175000\n50%    100.050000\n75%    101.200000\nmax    103.700000\n\n\ndescribe() 함수에서의 std 는 자동적으로 n – 1 로 나누어져서 계산된다.\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "ADA/ADA11.0.html",
    "href": "ADA/ADA11.0.html",
    "title": "제 11장 비모수",
    "section": "",
    "text": "Reporting Date: August. 5, 2024\n\n1 . 자 료 의 입 력 추정과 검증의 정의\n모집단의 가정이 잘못되었거나 없다면, 중심극한정리 개념을 사용하거나, 표본이 많다면 가능하다. 그러나 소표본에 경우에는 어떻게 해야 하는 건가?\n그렇다면 빅데이터 시대에 소표본이 있을까? 측정이나 관측데이터에서는 의외로 데이터를 수집하기 쉽지 않다. 생물학적인 표본을 얻기 위해서 몇 년간 걸린다.\n주로 우리가 보는 빅데이터란 SNS나 이미지와 같은 것이다. 상당히 많은 연구 분야에서는 소표본이 매우 많다.\n모수에 대해서 어떤 추정도 할 수 없는 경우, 분포 무관방법을 사용한다. 부호와 순위를 가지고 표현하므로, 직관적이고, 정규분포를 따를 때에도 효율은 크게 떨어지지 않으며 이를 벗어날 때에는 적극적으로 사용되는 방법이다.\n\n1 . 자 료 의 입 력 파라메트릭(계산식)을 사용 기댓값, 추정, 가설 검정을 시도\n표본이 작고, 단일 모집단에 대한 t검정 사인과 랭크 테스트 시도하기!\nW+는 양의 부호를 가진 수의 합, E(W) 은 먼저, E(Ri) =n으로 볼 수 있으므로, 모든 랭크를 더한다고 했을 때, n+1/2으로 말할 수 있고, 귀무가설이 맞다면 전체 E(W) 중 E(W+) 은 절반의 양인 n (n+1) / 4로 볼 수 있다.\n정규분포를 따를 땐 가능했던 그 연산(기댓값, 분산)을 여기서도 할 수 있다는 것. E(Ri+)라는 건 귀무가설 하에 전체 Ri 중 전반이 나와야 한다.\nVar(Ri+) = E(Ri+)^2 + [E(Ri+)]^2 이 공식에서 E(Ri+)은 안다. 문제는 E(Ri+)^2인데, (Ri+)^2 / 2 이와 같다.\n따라서, 2(Ri+)^2 / 2 - (Ri+)^2 / 4 = (Ri+)^2 / 4가 된다. 이때, (Ri+)^2은 등차수열 공식을 사용하여, 최종적으로 n(n+1)(2n+1) / 24의 값이 나온다.\n이는 정규표준화로 표현할 수 있으며, 정규화를 할 수 있다. 또한, 순수한 비모수로서 그대로 비교해도 무방.\n두 개의 집단이 독립 표본\n귀무가설은 두 개의 분포가 같다. 표본의 개수가 같지 않아도 됨, 표본의 개수가 더 작거나 W 통계량이 작을 때\n과제로 증명하기: 기댓값과 분산 입증하기. Wx = R(i)\n생존을 연장시키는 약을 생쥐에 투여 전후가 아니므로 사인(양수, 음수)이 나올 수가 없음\n순위합검정에서 통계량이 음수값으로 나올 수 있는가?\n전체 모집단에 분산을 모를 때, 서로 분산의 가정을 한 것\n6장에서 했었던 모든 것들 비모수로 진행해보기 표본이 작다고 해서 t검정을 사용하는 것은 위험하다 분포에 대한 정규 가정이 들어가므로, 분포를 모를 때에는 비모수를 진행하는 것이 타당하다.\n비모수 상당히 많이 쓰임\n척도모수\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA10.2.html",
    "href": "ADA/ADA10.2.html",
    "title": "제 10장 분산분석: 이원분류 실습",
    "section": "",
    "text": "Reporting Date: August. 5, 2024 이원분류 분산분석에 대해 다루고자 한다.\n\n\n01 반복이 없는 이유\n1 . 반복이 없는 경우 # 반복을 하지 않는 것도 가능하다. # 처치수준의 2개면 교오작용은 일어날 수 없다. # 각각의 순수자기자신의 효과만을 볼 수 있다. # 각각의 기각역이 생긴다.\n예를 들면, 자동차이기 때문에 많이 반복하질 못했다.\n일반적인 이원 분산 분석은 교오 작용 식이 존재한다. 그러나 임의화완전블록설계에선 교오 작용 식이 존재하지 않는다.\n고객이 각각의 제품에 대해 어느 수준으로 평가할 것인가? 여기서, 어떤 제품 간의 관계가 같으냐 다르냐는 의미가 없음\n각각 2개의 가설검증이 있어야 한다.\n블록으로 처리해서 순수한 처치들에 대한 효과를 보겠다는 것 반복이 생길 땐 교오작용이 생김\nimport os import pandas as pd\n\n\n상대 경로 설정\nfile_path = os.path.join(‘data’, ‘Usage.csv’)\n\n\nCSV 파일 읽기\nUsage = pd.read_csv(file_path)\n\n\n데이터 확인\nUsage.head(5)\n제품 혹은 소비자에 따라서 선호도의 차이가 있는가? 를 볼려고 하는 것이다. 소비자도 처치로 놓을 수 있다. 실무에서는 소비자의 개인보다는 그룹일 가능성이 더 높으며 그게 더 맞다. 그리고 고객을 대상으로도 반복이 없는 분석 진행이 생각보다 많다.\n총변동은 변함이 없는데 이를 계산은 SSA, SSB, SSE로 나뉜다 시그마a 시그마b ( yij - y헷.. )^2 = 시그마a 시그마b (yij - y헷i. + y헷i. + y헷.j - y헷.j + y헷..)^2 를 기준으로 식이 출발한다. 이 식은 2가지로 묶에서 더한 식으로 진행하고 지워지는 것을 지우면 y헷.. 만이 남을 것이다. 이것은 각 A, B의 평균의 차이를 보이는 총변동을 보려하려는 것.\n평균제곱합은 3가지, F값은 2가지가 나와야 한다. 반복이 없을 때에는 교오작용이 들어가지 않는다. 왜?\nfrom bioinfokit.analys import stat res = stat() res.anova_stat(df = Usage, res_var = ‘Prefer’, anova_model = ‘Prefer~C(Customer)+Product’, ss_typ=3) res.anova_summary\n소비자에 대한 효과는 없고, 제품에 대한 효과만 있다.\nC = 5 - 1 R = 전체 샘플 사이즈 - 모든 자유도 = 20 - 6 = 12 3 나누기 126 = 38\n수준에 따라 차이가 있다면 어느 제품의 수준이 가장 좋은가?\n따라서 기초통계량으로 어느 곳에 차이가 나는지를 보았다.\nUsage.groupby(“Product”).agg({“Prefer”:[“mean”, “std”, “min”, “max”]})\n선호 점수의 차이가 있다. A1을 가장 선호한다. 여기서 끝내지 않고, 어디에서 차이가 나는지를 확인하기 위해 다중비교를 한다.\nres.tukey_hsd(df=Usage, res_var = ‘Prefer’, xfac_var=‘Product’, anova_model=‘Prefer~C(Customer)+Product’) res.tukey_summary 0번과 2번은 차이가 있다. 나머지 차이가 없다는 건 사실상 같은 그룹으로 묶어도 된다는 의미이다.\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nres = stat() df = pd.DataFrame(Usage) res.tukey_summary = pairwise_tukeyhsd(endog = df[‘Prefer’], groups = df[‘Product’], alpha = 0.05) print(res.tukey_summary)\n여기까지가 반복이 없는 경우이다.\n01 반복이 있는 경우 2p 실험설계\n2 . 반복이 있는 경우 y11r: 1번만 한 것이 아니라 r번씩 반복한 것. 분산분석표에서 상호작용(SSAB)가 존재한다. 3개의 요인을 분석만 해도, 모델은 기하급수적으로 복잡해진다.\n그러나 사실 변수의 개수가 3라고 해도 별로 특별한 이유가 아니다. 그러니 다른 분석 기법에 비해 해석 방법이 매우 복잡한 것이 그 이유이다.\n여기선 교오작용에 대한 효과도 고려한다. 식에도 포함된다. F통계량도, 귀무가설도 3개로 늘어난다.\nA, B, C 디자인에 대한 대, 중, 소 도시 지역에 따라 어떤 디자인이 먹힐 것인가?\n판매한 결과를 표로 작성하였다. 데이터를 먼저 설계하는 것이 가장 중요하다.\nimport os import pandas as pd\n\n\n상대 경로 설정\nfile_path = os.path.join(‘data’, ‘Market.csv’)\n\n\nCSV 파일 읽기\nMarket = pd.read_csv(file_path)\n\n\n데이터 확인\nMarket.head(5)\n12개의 관측치, 디자인과 도시, 판매량 여기서 반복인지 아닌지 알수 있어야 한다.\n기초 통계량\nMarket.groupby(“City”).agg({“Sales”:[“mean”, “std”, “min”, “max”]})\n도시에서는 차이가 나는 지 잘 모르겠다.\n기초 통계량\nMarket.groupby(“Design”).agg({“Sales”:[“mean”, “std”, “min”, “max”]})\n교오작용 확인하기.\nMarket.groupby([“City”, “Design”]).agg({“Sales”:[“mean”, “std”, “min”, “max”]})\n순수하게 보았을 때에는 차이가 보이지 않았으나, 둘이서 같이 보니 소도시에서 A와 C에서 차이가 나는 것을 확인할 수 있다. 교오작용을 존재할 수도 있겠다고 차이가 난다고 추측할 수 있다. 이런 식으로 근거를 찾는 탐색적 분석이 필요하다.\n모든 분석은 전체적인 스케치를 한 다음에 모델을 적용하는 것이다. 아래는 개별 도시와 그것에 합친 교오작용 그래프\nMarket.boxplot(“Sales”, by = “City”) Market.boxplot(“Sales”, by = “Design”) Market.boxplot(“Sales”, by = [“City”, “Design”])\n중위수가 박스의 윗쪽에 있으므로,\nB디자인을 더 선호한다.\n시티를 먼저 썼으므로, 대중소가 차례로 나온다. 가장 많은 것은 중도시의 B이다.\n대도시와 소도시는 서로 다른 패턴을 보인다.\nimport matplotlib.pyplot as plt import seaborn as sns sns.pointplot(x=“City”, y=“Sales”, color=‘red’, data = Market) sns.pointplot(x=“Design”, y=“Sales”, color=‘green’, data = Market)\nB가 가장 높은 것을 알 수 있다.\n분산분석 실시\nfrom bioinfokit.analys import stats res = stat() res.anova_stat(df = Market, res_var = ‘Sales’, # City:Design: 교오작용 anova_model = ‘Sales ~ City + Design + City:Design’, ss_typ = 3) res.anova_summary\n교오작용은 콜론(:)으로 쓴다. 여기도 책에 문제가 있음, 확실한 차이가 존재함을 알 수 있음 디자인에 차이가 있고, 디자인과 도시간 교오작용에서도 차이가 있다.\nsns.pointplot(x=“City”, y=“Sales”, hue = “Design”, ci = 0, data = Market, markers = [“o”, “*“,”s”], linestyles = [“-”, “–”, “:”])\n겹치는 부분이 존재하므로, 교오작용이 존재한다. 여러가지 일은 하는 것보다 한가지만을 특화해서 하는 것도 방법이다.; 그렇지 않고 모두 떨어지거나 모두 올라가면 교오 작용이 없는 것이다.\n01 3원 분류 잔차를 포함해서 2의 3승 개의 이 분산분석표의 교오작용이 있다면 이것으로도 만족되지만 tukey는 더 명확하게 보고 싶으면 하는 방법이다. 꼭 사용해야만 하는 방법은 아니고 추가 지표이다. 이런 건 스마트 팩토리, 바이오 분야에서 더 많이 쓰이고, 실제 실무에선 많이 쓰이진 않을 수 있다. 그리고 여기서도 변수의 수가 많다고 무조건 과적합이 발생된다고 단정하면 안된다. 실무에선 20-30개 변수를 사용하는 건 기본이지만서도 분산분석에서는 그렇게 많이도 하진 않는다. 비용대비 결과가 만족스럽게 나오진 않기에. 그래도 적어도 변수는 3-5개까지는 한다는 것이다. 그리고 교오 작용에서도 몇 번째의 각각 ABC이냐까지 고려 해야 한다.\n반복이 없는 경우\n반복이 있는 경우\n공분산 분석\n처리요인이 아니면서 반응변수에 영향을 주는 요인을 교락요인이란 한다. 교략요인은 범주형(그룹형, 블록) 형태와 연속형(공변량)인 경우가 있음 이럴 경우, 적용하는 것이 공분산 분석이다.\n이 공변량을 모형에 추가적인 설명변수로 삽입하여 그 효과를 제거했다면 그런 영향을 주는 공변량 변동을 미리 제거하여 순수한 처리효과를 파악하고자 하는 통계적 기법 실험 정도를 높이고, 실험의 편의를 높이기 위해 사용한다.\n참고로 범주형일 땐 공분산 분석을 할 수 없음.\n예제: 심리 점수 데이터 20명의 사람을 10명 씩 두 개의 집단으로 나눈 다음, 기존 방법과 신규 방법으로 공부를 한 다음 심리 점수를 측정하였다.\n사람의 나이 데이터는 연속형 데이터이다. 그러나 이것은 처치요인은 아니므로, 이러한 것을 교략요인이자 공변량이라고 부르는 것이며 이때, 공분산 분석을 하는 것이다.\n그래서 먼저 아무것도 하지 않는 비교 집단을 기존방법1 실험 집단을 신규방법2으로 한다. 이것을 아노바로 하는 것은 과하다. t 검정 (분산 분석)을 먼저 한다. 그래서 식을 세운다. 집단 1, 2의 평균을 구한 뒤 각 평균의 차이를 계산한다.\n이번엔 공분산 분석을 한다. 이때 여기선 베타x1j라는 나이 변수를 넣었다.\nimport os import pandas as pd\n\n\n상대 경로 설정\nfile_path = os.path.join(‘data’, ‘Ancova.csv’)\n\n\nCSV 파일 읽기\nAncova = pd.read_csv(file_path)\n\n\n데이터 확인\nAncova.head(5)\nimport statsmodels.api as sm import statsmodels.formula.api as smf AncovaFit = smf.ols(formula = ‘Score~Age+C(Group)’, data = Ancova).fit() AncovaFit.summary()\n이를 통해 공분산 분석을 진행하였다. 결과적으로 아노바 분석을 진행하였고, —- 타입 3이 이야기 해야 함 + 군집 분석 까지.\n변수를 한꺼번에 넣는 게 아니라 타입1은 순서대로 넣어서 테이블을 만듦 원way에서는 상관없고 투way부터 상호작용이 생기는 데 이를 존재하지 않다고 말하는 게 타입 2이다. 메인 효과만 고려. 상호작용까지 고려하는 게 타입3 이다. 그래서 타입3를 대부분 사용하는 것이다.\n언발라스드 디자인, 이란 셀의 크기가 다를 경우, 를 의미한다. 일부 셀에 데이터가 없는 경우에 타입에 따라서 결과값이 달라진다. 그럴땐 타입3를 사용하는 것이 적절하다. 또한 유닉크 효과 테스팅, 순수한 자기 효과만을 보여준다는 것이다.\nsm.stats.anova_lm(AncovaFit, typ = 3)\nAncova.groupby(“Group”).agg({“Score”:[“mean”, “std”, “min”, “max”]})\nControl = Ancova[Ancova[‘Group’] == 1] Treatment = Ancova[Ancova[‘Group’] == 2]\nfrom statsmodels.stats.weightstats import ttest_ind ttest_ind(Control[‘Score’], Treatment[‘Score’], alternative=‘smaller’, usevar=‘pooled’)\nimport statsmodels.formula.api as smf AncovaFit1 = smf.ols(formula = ‘Score~Age+C(Group)+Age:C(Group)’, data = Ancova).fit() AncovaFit.summary()\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA10.0.html",
    "href": "ADA/ADA10.0.html",
    "title": "제 10장 분산분석: 일원분류",
    "section": "",
    "text": "Reporting Date: November. 5, 2024\n일원분류 분산분석(ANOVA)의 전반적인 내용에 대해 다루고자 한다."
  },
  {
    "objectID": "ADA/ADA10.0.html#유래와-역사적-배경",
    "href": "ADA/ADA10.0.html#유래와-역사적-배경",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 유래와 역사적 배경",
    "text": "1 . 유래와 역사적 배경\n1935년, 통계학자 로널드 A. 피셔(Ronald A. Fisher)가 저서 The Design of Experiments에서 처음 체계적으로 제시하였다.\n그는 실험 설계법을 통해 통계적 추론의 근거를 마련하였으며, 이후 널리 알려진 붓꽃 데이터(Iris dataset)를 사용하여 그룹 간 평균 차이를 검정하는 절차를 구체적으로 설명하였다.\n이 연구는 실험의 무작위화(randomization), 통제(control), 반복(repetition)이라는 세 가지 원칙을 확립함으로써, 현대 통계학의 기초를 세우는 중요한 전환점이 되었다.\n피셔는 또한 귀무가설(null hypothesis)과 유의수준(significance level) 개념을 도입하여, 관찰된 차이가 단순한 우연인지 아닌지를 판단하는 통계적 검정의 표준 절차를 확립하였다."
  },
  {
    "objectID": "ADA/ADA10.0.html#홍차-실험",
    "href": "ADA/ADA10.0.html#홍차-실험",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . 홍차 실험",
    "text": "2 . 홍차 실험\nTea-tasting experiment\n피셔의 통계 사상을 상징적으로 보여주는 사례로, 임의화 실험(randomized experiment)의 효시로 평가된다.\n피셔의 비서는 홍차와 우유 중 어느 것을 먼저 넣었는지 맛만으로 구별할 수 있다고 주장했다.\n이를 검증하기 위해 피셔는 총 8잔의 홍차를 준비했으며, 이 중 4잔은 홍차를 먼저, 나머지 4잔은 우유를 먼저 넣은 뒤 무작위로 제시했다.\n비서가 모든 잔을 완벽히 맞출 필요는 없었고, 통계적으로 유의미한 비율로 구분할 수 있다면 그의 주장이 단순한 우연이 아님을 입증할 수 있었다.\n이 사례는 오늘날 우리가 사용하는 “귀무가설을 설정하고, 특정 유의수준에서 이를 기각하는” 가설 검정 절차의 시초로 간주된다."
  },
  {
    "objectID": "ADA/ADA10.0.html#문화적-일화-조지-오웰의-홍차론",
    "href": "ADA/ADA10.0.html#문화적-일화-조지-오웰의-홍차론",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 문화적 일화: 조지 오웰의 홍차론",
    "text": "3 . 문화적 일화: 조지 오웰의 홍차론\n흥미롭게도, 홍차에 대한 관점은 통계학적 접근 외에도 문화적으로 다양하게 전개되었다.\n영국의 작가 조지 오웰(George Orwell)은 1946년 에세이 A Nice Cup of Tea에서 “뜨거운 홍차를 먼저 따르고, 그 후 차가운 우유를 넣는 것이 가장 이상적이다”라고 주장했다.\n그는 이 방식이 홍차의 향과 우유의 부드러움을 동시에 살려준다고 설명했다.\n피셔가 실험을 통해 ’객관적 검증’을 중시했다면, 오웰은 경험과 감각을 바탕으로 ’주관적 완성’을 추구한 셈이다. 이 대비는 과학적 탐구와 문화적 감수성의 공존을 상징적으로 보여준다."
  },
  {
    "objectID": "ADA/ADA10.0.html#반응변수",
    "href": "ADA/ADA10.0.html#반응변수",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반응변수",
    "text": "1 . 반응변수\nResponse Variable\n실험의 결과로 측정되는 변수로, 요인의 변화에 따라 값이 달라지는 종속변수이다.\n본 실험에서는 수확량(Yield)이 반응변수로 설정된다. 이는 각 실험 조건(비료와 온도 조합)에 따라 달라질 수 있으며, 연구의 주된 분석 대상이 된다."
  },
  {
    "objectID": "ADA/ADA10.0.html#요인",
    "href": "ADA/ADA10.0.html#요인",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . 요인",
    "text": "2 . 요인\nFactor\n실험에서 반응변수에 영향을 줄 수 있는 독립변수를 의미한다. 연구자는 요인을 조작하여 각 수준별로 반응변수의 변화를 관찰한다.\n본 실험에서는 비료의 종류와 정화수의 온도가 두 개의 요인이다. 각 요인의 효과를 분리하여 비교함으로써, 어떤 요인이 수확량에 더 큰 영향을 미치는지 파악할 수 있다."
  },
  {
    "objectID": "ADA/ADA10.0.html#수준",
    "href": "ADA/ADA10.0.html#수준",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 수준",
    "text": "3 . 수준\nLevel\n각 요인이 취할 수 있는 구체적인 값 또는 조건을 의미한다. 하나의 요인은 여러 수준을 가질 수 있으며, 수준의 조합을 통해 다양한 실험 처리가 구성된다.\n비료 요인: A, B, C → 3개 수준 정화수 온도 요인: 10 °C, 15 °C, 20 °C → 3개 수준 이처럼 요인의 수준 수가 많아질수록 가능한 처리 조합의 수도 기하급수적으로 증가한다."
  },
  {
    "objectID": "ADA/ADA10.0.html#처리-또는-처치",
    "href": "ADA/ADA10.0.html#처리-또는-처치",
    "title": "제 10장 분산분석: 일원분류",
    "section": "4 . 처리 또는 처치",
    "text": "4 . 처리 또는 처치\nTreatment\n요인들의 수준 조합에 의해 형성되는 구체적인 실험 조건을 의미한다. 즉, 각 처리는 서로 다른 조건하에서 실험이 수행되는 개별 실험 단위를 나타낸다.\n(비료 A, 온도 10 °C) (비료 B, 온도 15 °C) (비료 C, 온도 20 °C) 각 조합은 하나의 독립적인 처리이며, 처리별 수확량을 비교함으로써 요인의 효과를 통계적으로 분석할 수 있다.\n개념 예시 반응변수 실험의 결과로 관찰되는 값 수확량 요인 반응변수에 영향을 주는 독립변수 비료 종류, 정화수 온도 수준 요인의 구체적 조건 비료 A/B/C, 온도 10 / 15 / 20 °C 처리 요인 수준의 조합 (비료 A, 10 °C), (비료 B, 15 °C) 등\n실험 연구 과정\n03 실험 설계의 기본 원리 Principles of Experimental Design\n실험 설계의 목적은 요인의 효과를 정확하게 추정하고, 불필요한 오차를 최소화하는 것에 있다.\n이를 달성하기 위해 통계학자 로널드 A. 피셔(Ronald A. Fisher)는 세 가지 기본 원리 ― 반복화(Replication), 확률화(Randomization), 블록화(Blocking) ― 를 제시하였다.\n이 세 원리는 현대 모든 실험 설계의 기초로 사용되며, 분산 분석의 이론적 기반을 이룬다."
  },
  {
    "objectID": "ADA/ADA10.0.html#반복화",
    "href": "ADA/ADA10.0.html#반복화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반복화",
    "text": "1 . 반복화\nReplication\n동일한 실험 처리를 여러 번 반복하여 수행하는 절차를 말한다.\n실험 단위는 완전히 동일하지 않기 때문에, 실험 오차(experimental error)가 항상 존재한다.\n따라서 동일한 조건하에서 실험을 반복함으로써 오차의 크기를 추정하고, 결과의 신뢰도를 높일 수 있다.\n각 반응값(response value)은 확률변수로 간주되므로, 반복 측정을 통해 변이(variation)를 관찰할 수 있다.\n이를 통해 평균 반응의 안정성과 분산의 크기를 평가할 수 있으며, 이는 F-검정의 분모에 해당하는 오차항(Mean Square Error) 추정에 직접적으로 기여한다.\n비료 A와 온도 15 °C의 조합을 세 번 반복 측정할 경우, 수확량의 평균뿐 아니라 변동성까지 고려한 신뢰성 있는 결론을 얻을 수 있다."
  },
  {
    "objectID": "ADA/ADA10.0.html#확률화",
    "href": "ADA/ADA10.0.html#확률화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . 확률화",
    "text": "2 . 확률화\nRandomization\n실험 단위에 각 처리(조건)를 무작위로 배정(random assignment) 하는 절차이다.\n확률화를 통해 연구자의 의도나 외부 환경 요인에 의한 편향(bias)이 최소화된다. 즉, 각 실험 단위가 특정 처리에 배정될 확률이 동일하게 유지되므로, 관찰된 차이는 요인 자체의 효과로 해석할 수 있다.\n확률화는 통계적 추론의 전제인 독립성(independence)을 확보하고, 실험 결과에 대한 통계 검정의 정당성을 보장한다.\n농지의 위치, 토양의 질, 일조량 등 통제하기 어려운 요인이 존재할 때, 처리 배정을 무작위로 수행하면 이러한 외부 요인의 체계적 영향이 결과에 편향을 일으키는 것을 방지할 수 있다."
  },
  {
    "objectID": "ADA/ADA10.0.html#블록화",
    "href": "ADA/ADA10.0.html#블록화",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 블록화",
    "text": "3 . 블록화\nBlocking\n비슷한 특성을 지닌 실험 단위를 묶어 동질적인 블록(block)을 형성한 후, 각 블록 내에서 실험 처리를 배정하는 방법이다.\n블록화의 목적은 요인 이외의 변동 요인을 통제하여 실험의 정밀도(precision)를 향상시키는 것이다.\n즉, 블록 내에서는 실험 단위 간 차이가 최소화되므로, 관찰된 차이는 요인 효과로 더 정확히 설명될 수 있다.\n블록은 하나의 보조 요인(nuisance factor)으로 간주되어, 분산 분석 시 오차 제곱합(Error Sum of Squares)을 줄이고 요인 효과의 추정 오차를 감소시킨다.\n농지의 일조량이 다를 경우, 일조량 수준이 유사한 구역을 하나의 블록으로 묶은 뒤, 각 블록 내에서 비료 종류를 무작위로 배정하면 요인 효과를 보다 정확히 평가할 수 있다.\n세 가지 원리는 서로 독립적이지만 상호보완적으로 작용한다.\n원리 목표 주요 효과 반복화 (Replication) 실험 오차 추정 및 결과의 신뢰성 확보 변이 분석 가능, 평균 안정화 확률화 (Randomization) 편향 제거 및 독립성 확보 결과의 타당성 향상 블록화 (Blocking) 외부 요인 통제 및 정밀도 향상 오차 감소, 요인 효과의 명확화"
  },
  {
    "objectID": "ADA/ADA10.0.html#anova의-필요성",
    "href": "ADA/ADA10.0.html#anova의-필요성",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . ANOVA의 필요성",
    "text": "2 . ANOVA의 필요성\n두 집단 간의 평균 비교는 t-검정으로 충분하다. 그러나 세 집단 이상을 비교할 때 t-검정을 반복 적용하면 누적된 유의수준(α) 문제가 발생한다.\n예를 들어, 유의수준 α = 0.05로 세 집단을 비교하는 경우, 서로 다른 세 쌍의 평균 비교가 필요하다.\n\\[\n1 - (1 - 0.05)^3 \\approx 0.14\n\\]\n즉, 전체적으로 약 14% 확률로 잘못된 기각(제1종 오류)이 발생할 수 있다. 이는 실제로 차이가 없음에도 불구하고 유의미한 차이가 있다고 잘못 판단할 위험을 높인다."
  },
  {
    "objectID": "ADA/ADA10.0.html#분산분석의-원리",
    "href": "ADA/ADA10.0.html#분산분석의-원리",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 분산분석의 원리",
    "text": "3 . 분산분석의 원리\nANOVA는 단순히 평균 차이를 직접 비교하지 않고, 총 변동(Total Variation)을 다음 두 가지로 분해하여 비교한다.\n\\[\n\\text{총제곱합} = \\text{집단 간 제곱합 (SSB)} + \\text{집단 내 제곱합 (SSW)}\n\\]\n집단 간 제곱합 (Between-group variance): 각 집단 평균 간의 차이에서 기인한 변동, 즉 요인의 효과를 나타낸다.\n집단 내 제곱합 (Within-group variance): 같은 집단 내에서의 개별 오차로 인한 변동을 의미한다.\n이 두 변동의 비율을 이용해 계산되는 통계량이 F-통계량이다.\n\\[\nF = \\frac{\\text{집단 간 평균제곱 (MSB)}}{\\text{집단 내 평균제곱 (MSW)}}\n\\]\n이 값이 F분포 상에서 임계값보다 크면, 집단 간 평균 차이가 통계적으로 유의하다고 판단한다."
  },
  {
    "objectID": "ADA/ADA10.0.html#해석의-의미",
    "href": "ADA/ADA10.0.html#해석의-의미",
    "title": "제 10장 분산분석: 일원분류",
    "section": "4 . 해석의 의미",
    "text": "4 . 해석의 의미\nANOVA의 결과, 귀무가설이 기각된다면 ” 적어도 하나의 집단 평균이 다르다 ” 는 결론을 내릴 수 있다. 다만, 어떤 집단 간의 차이가 존재하는지는 알 수 없으므로, 그 이후에는 사후검정(Tukey HSD, Duncan, Scheffé 검정 등) 을 통해 집단 간 세부 비교를 수행해야 한다."
  },
  {
    "objectID": "ADA/ADA10.0.html#반응값-구조와-표본평균",
    "href": "ADA/ADA10.0.html#반응값-구조와-표본평균",
    "title": "제 10장 분산분석: 일원분류",
    "section": "1 . 반응값 구조와 표본평균",
    "text": "1 . 반응값 구조와 표본평균\n반응값 ( y )는 각 실험 단위에서 관측된 수확량을 의미한다. 비료 ( i )에 대해 ( n )번 반복 측정했다면, ( y_{i1}, y_{i2}, , y_{in} ) 이 수집된다.\n예: - ( y_{11} ): 1번 비료의 첫 번째 실험에서 얻은 수확량 각 비료에 대해 반복 측정된 반응값을 평균 내면 표본평균 ({y}_i)이 계산된다."
  },
  {
    "objectID": "ADA/ADA10.0.html#anova의-귀무가설",
    "href": "ADA/ADA10.0.html#anova의-귀무가설",
    "title": "제 10장 분산분석: 일원분류",
    "section": "2 . ANOVA의 귀무가설",
    "text": "2 . ANOVA의 귀무가설\n일원분류 ANOVA의 귀무가설은 다음과 같다.\n\\[\nH_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k\n\\]\n즉, 비료 종류가 달라도 모평균 수확량은 동일하다고 가정한다. 이는 모든 비료 효과가 0이라는 의미에 해당한다. 반면, 어느 하나라도 다른 경우 대립가설 (H_1)이 성립한다."
  },
  {
    "objectID": "ADA/ADA10.0.html#제곱합의-의미",
    "href": "ADA/ADA10.0.html#제곱합의-의미",
    "title": "제 10장 분산분석: 일원분류",
    "section": "3 . 제곱합의 의미",
    "text": "3 . 제곱합의 의미\nSum of Squares\n일원분류 분산분석(ANOVA)은 전체 변동을 처리 간 변동과 처리 내 변동으로 분해한다. 이는 요인(예: 비료 종류)이 반응변수(수확량)에 미치는 영향을 확인하기 위한 핵심 구조이다.\n\n총제곱합(TSS, Total Sum of Squares)\n\n\\[\nTSS = \\sum_{i=1}^{k}\\sum_{j=1}^{r} (y_{ij}-\\bar{y})^2\n\\]\n모든 관측값이 전체 평균에서 얼마나 벗어나는지 보여준다. 실험에서 관측된 전체 변동(total variability)을 의미하며, 이후 처리 효과와 오차 변동으로 분해된다. (2) 처리제곱합(SST, Treatment Sum of Squares)\n\\[\nSST = r\\sum_{i=1}^{k}(\\bar{y}_i - \\bar{y})^2\n\\]\n각 처리 평균이 전체 평균에서 얼마나 떨어져 있는지를 측정한다. 반복수 ( r )을 곱함으로써 각 처리 평균이 전체 변동에 기여하는 정도를 반영한다. 비료별 평균 차이가 클수록 SST가 커지며, 이는 처리 효과가 클 가능성을 의미한다. 해석:\nSST가 크면 → 처리 간(비료 간) 평균 수확량 차이가 크다 → 대립가설 지지 가능성 증가 SST가 작으면 → 비료 간 차이가 거의 없다 (3) 오차제곱합(SSE, Error Sum of Squares)\n\\[\nSSE = \\sum_{i=1}^{k}\\sum_{j=1}^{r}(y_{ij}-\\bar{y}_i)^2\n\\]\n동일한 처리 내에서 개별 관측값들이 처리 평균에서 벗어나는 정도를 나타낸다. 내부 변동(within-group variability), 즉 실험 통제로 해결되지 않는 자연적·비체계적 변동을 의미한다. SSE가 작을수록 처리군 내부의 일관성이 높아지고, 처리 효과 검정의 민감도가 상승한다. (4) 제곱합의 관계\n\\[\nTSS = SST + SSE\n\\]\n전체 변동이 처리 간 차이(SST) 처리 내 오차(SSE) 로 구성됨을 의미한다. 이는 일원분류 ANOVA 전체 구조의 토대이다.\n4 . 처리제곱합의 해석 관점 처리제곱합의 해석은 다음 원칙에 기반한다.\nSST가 작다면, 모든 처리 평균이 서로 유사하여 실험을 통해 구분할 만한 차이가 존재하지 않음을 의미한다.\nSST가 상대적으로 크고 SSE가 작다면, 처리 간 차이가 오차 변동보다 충분히 크다는 뜻이며, ANOVA에서 유의한 결과가 나올 가능성이 높아진다.\n최종 판단은 다음을 충족할 때 이루어진다. SST가 크고 SSE가 작으며 계산된 F 통계량이 유의수준에서 임계값을 초과할 때 이 경우 처리 간 평균 차이가 통계적으로 유의하다고 결론 내릴 수 있다.\n5 . 오차의 특성 ANOVA의 F 검정이 타당하려면 다음 오차 구조를 충족해야 한다.\n등분산성(homoscedasticity): 각 처리 집단의 오차 분산이 동일 정규성(normality): 오차항이 정규분포를 따름 독립성(independence): 관측값 간 종속 구조가 없음 이 조건이 충족될 때, 계산된 F-통계량은 이론적 F-분포와 부합한다.\n6 . F-검정의 구조 ANOVA의 목적은 다음 귀무가설을 검정하는 것이다.\n\\[H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_k = 0\\]\n즉, 모든 처리 평균이 동일하다는 가설이다.\n평균제곱(MS)을 기반으로 F 통계량을 계산한다.\n\\[F = \\frac{MS_{treatment}}{MS_{error}}\\]\n처리 평균제곱: (MS_{treat} = ) 오차 평균제곱: (MS_{error} = ) F-값이 클수록 처리평균이 동일하다는 귀무가설을 기각하고 처리 효과가 존재한다는 결론을 얻는다.\n7 . 추가 설명 반복수 ( r )이 동일하지 않아도(불균형 설계), 제곱합과 자유도 계산만 조정되며 분석의 논리는 변하지 않는다. F-분포는 오른쪽이 긴 비대칭 분포이며, 제곱항 기반이므로 음수 값이 존재하지 않는다.\n06 모형 설정의 목적과 구조\n일원분류 ANOVA를 기준으로 한다.\n1 . 모델을 세우는 이유 비료 종류가 수확량에 어떤 영향을 주는지를 정량적으로 파악하기 위해서는 관측된 수확량을 처리 효과(비료의 영향)와 오차(통제 불가능한 변동)로 명확히 분리할 필요가 있다.\n모형화(Modeling)는 다음 두 가지 목적을 가진다.\n처리 효과와 오차를 구분하여 변동의 원인을 구조적으로 이해한다. 비료 간 차이가 통계적으로 유의한지 검정할 수 있는 기반을 제공한다. 즉, 모델을 세움으로써 각 비료가 수확량에 미치는 효과를 체계적으로 설명하고 추론할 수 있다.\n2 . 모형의 기본 구조 일원분류 ANOVA에서는 개별 수확량 ( y_{ij} )를 다음과 같이 분해한다.\n\\[y_{ij} = \\mu_i + \\epsilon_{ij}\\]\n여기서\n( i = 1, 2, , k ): 비료 종류(처리 수준) ( j = 1, 2, , n ): 반복 실험 번호 ( i ): i번째 비료의 모평균 (각 처리에 대한 모평) ( {ij} ): 개별 수확량의 오차항(Error term)\n3 . 처리 평균의 분해: 전체 평균과 처리 효과 처리별 모평균 ( _i )다시 다음과 같이 표현된다.\n\\[\\mu_i = \\mu + \\tau_i\\]\n( ) : 전체 모평균(overall mean) (모든 비료에 대한 평균) ( _i ) : i번째 처리(비료)의 효과(treatment effect) 그리스 알파벳 타우(τ)로 표기 (19번째 글자) 전체 평균에서 벗어난 정도를 의미 즉, 각 처리의 평균은 전체 평균 위에 얼마나 효과가 더해졌는가를 나타낸다.\n4 . 최종 모형의 형태 위 식을 결합하면 개별 수확량은 다음과 같이 분해된다.\n\\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\]\n이 모형은 다음의 세 구성요소로 수확량을 설명한다.\n전체 모평균(μ) 비료의 처리 효과(τᵢ) 설명되지 않는 변동(오차, εᵢⱼ)\n5 . 오차항의 가정 오차항 ( _{ij} )는 ANOVA의 핵심 가정에 따라 다음을 만족한다.\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma^2)\n\\]\n평균 0 분산 ( ^2 ) 정규분포를 따르며 서로 독립 이러한 가정이 충족되어야 F-검정이 타당하게 동작한다.\n6 . 모델 설정의 의의 이 모형을 사용하면 다음이 가능해진다.\n각 비료의 효과(τᵢ)가 0인지 검정하여 비료 종류가 수확량에 유의한 차이를 만드는지 평가 처리 효과와 오차가 분리되므로 변동의 원인을 구조적으로 해석 분산분석표(ANOVA Table)에서 제곱합 → 평균제곱 → F값 으로 이어지는 분석 구조 확립 결과적으로, 이 모형은 비료 간 평균 차이가 단순한 우연인지, 실제 효과인지 판단할 수 있는 통계적 기반을 형성한다.\n필요하면 이어서 모형의 제약조건(∑τᵢ = 0), 모수추정(OLS 관점), F 검정 유도 과정도 전문적으로 정리해드립니다. (흠…)\n64 = 72 + (-8) = 전체 모평균 + 오\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA09.0.html",
    "href": "ADA/ADA09.0.html",
    "title": "제 9장 회귀분석",
    "section": "",
    "text": "Reporting Date: October. 15, 2024\n한 변수가 다른 변수에 의해 설명 및 예측 과정을 분석하는 회귀분석에 대해 논의하고자 한다."
  },
  {
    "objectID": "ADA/ADA09.0.html#기계-학습",
    "href": "ADA/ADA09.0.html#기계-학습",
    "title": "제 9장 회귀분석",
    "section": "01 기계 학습",
    "text": "01 기계 학습\nMachine Learning\n데이터 기반으로 패턴과 규칙을 학습하고, 그 학습된 모델을 활용해 미래 예측, 분류, 의사결정 등을 수행하는 알고리즘 및 방법론을 연구하는 인공지능(AI)의 하위 분야이다.\n기계학습은 3 가지의 유형으로 구분된다.\n\n1 . 지도 학습\nSupervised Learning\n입력 변수 X 와 정답에 해당하는 타깃 변수 Y 가 함께 주어졌을 때, X 로부터 Y 를 예측하는 모델을 학습하는 방법이다.\n학습 과정에서는 데이터에 포함된 정답을 활용하여 입력과 출력 간의 규칙을 찾고, 이를 기반으로 새로운 데이터에 대한 예측을 수행할 수 있다.\n크게 연속적인 값을 예측하는 회귀 문제와, 특정 범주를 예측하는 분류 문제로 나눌 수 있다.\n대표적인 알고리즘으로는 선형 회귀, 로지스틱 회귀, 결정 트리, K-최근접 이웃(KNN), 서포트 벡터 머신(SVM), 신경망 등이 있다.\n\n\n2 . 비지도 학습\nUnsupervised Learning\n정답(타깃 변수) 없이 입력 데이터 X 만을 가지고 데이터의 패턴, 구조, 분포를 학습하는 방법이다.\n주로 유사한 데이터들을 묶는 군집화, 데이터의 차원을 줄여 본질적인 특성을 추출하는 차원 축소기법이 사용된다.\n대표적인 예로는 장바구니 분석(연관 규칙 학습), 다차원 척도법(MDS), 주성분 분석(PCA), 요인 분석(FA) 등이 있다.\n\n\n3 . 강화 학습\nReinforcement Learning\n에이전트가 환경(Environment)과 상호작용하며, 각 상태(State)에서 취할 수 있는 행동(Action)을 선택한다.\n그 결과로 보상(Reward)을 받으면서, 장기적으로 누적 보상을 최대화할 수 있는 최적의 행동 방식을 학습하는 방법이다.\n정답 데이터가 주어지는 것이 아닌 시행착오를 통해 스스로 전략을 개선하는 것이 특징입니다.\n이러한 특성 덕분에 강화 학습은 로봇 제어, 게임 인공지능, 자율주행, 추천 시스템 등 순차적 의사결정 문제에 널리 활용된다.\n이 중 지도 학습에 속하는 회귀 모델을 중심으로 살펴보고자 한다.\n“회귀” 용어의 유래 19세기 후반, 영국의 통계학자 프랜시스 골턴은 유전학 연구에서 부모와 자녀의 키 상관관계를 조사하면서 처음으로 “회귀” 라는 용어를 사용하였다.\n그는 부모의 키가 평균보다 크거나 작은 경우, 자녀의 키가 평균값으로 되돌아가는 경향을 발견하였고, 이를 회귀라고 설명하였다. 즉, 자녀들의 키가 부모의 극단적인 키보다 평균에 가까워지는 현상을 의미한 것이다.\n이후 골턴의 연구를 바탕으로 칼 피어슨은 회귀 분석을 체계화하고 수학적으로 확장하였으며, 선형 회귀의 공식과 상관계수 개념을 발전시켜 오늘날 회귀는 통계학에서 중요한 분석 도구로 자리잡게 되었다."
  },
  {
    "objectID": "ADA/ADA09.0.html#회귀-모델의-정의",
    "href": "ADA/ADA09.0.html#회귀-모델의-정의",
    "title": "제 9장 회귀분석",
    "section": "02 회귀 모델의 정의",
    "text": "02 회귀 모델의 정의\n회귀 모델은 타겟 변수인 Y 를 예측하기 위해 입력 변수 X 간의 관계를 학습하는 지도 학습의 한 유형이다.\n회귀 모델은 목적과 특성에 따라 5 가지 유형으로 구분된다.\n\n1 . 종속 변수의 개수에 따른 분류\n일변량 회귀 Univariate Regression\n하나의 종속 변수 Y 를 예측하기 위해 입력 변수 X 와 Y 간의 관계를 모델링하는 방법이다.\n다변량 회귀 Multivariate Regression\n여러 개의 종속 변수 Y1, Y2, …, Ym 를 동시에 예측하며, 종속 변수 간의 상관관계와 입력 변수와의 관계를 함께 고려하여 고차원적 관계를 모델링하는 것을 목표로 한다.\n\n\n2 . 두 변수 간의 관계에 따른 분류\n선형 회귀 Linear Regression\n독립 변수와 종속 변수 사이의 관계가 선형적일 때 사용하는 모델로, 종속 변수를 독립 변수들의 선형 결합으로 표현하며 회귀선을 직선으로 모델링한다.\n비선형 회귀 Nonlinear Regression\n두 변수 간의 관계가 비선형일 때 사용되며, 다항 회귀나 곡선 형태의 수학적 함수를 통해 관계를 모델링한다.\n\n\n3 . 독립 변수의 개수에 따른 분류\n단순 회귀 Simple Regression\n하나의 독립 변수 X를 사용하여 종속 변수 Y 를 예측하는 방법이다.\n다중 회귀 Multiple Regression\n여러 독립 변수 X1, X2, …, Xp ​를 활용해 종속 변수 Y 를 예측하는 방법이다.\n현실적인 데이터 분석에서는 종속 변수에 영향을 미치는 요인이 여러 개 존재하는 경우가 많기 떄문에 다중 회귀가 널리 활용된다.\n\n\n4 . 분산과 공분산\n분산 Variance\n단일 변수의 산포 정도, 즉 값들이 평균으로부터 얼마나 흩어져 있는지를 나타낸다.\n회귀 분석에서 종속 변수 Y 의 분산은 데이터가 얼마나 퍼져 있는지를 보여주며, 분산이 작을수록 모델이 더 정밀한 예측을 할 가능성이 높다.\n공분산 Covariance\n두 변수 간의 선형적 관계를 나타내는 지표로, 두 변수가 함께 어떻게 변하는지를 측정한다.\n공분산이 양수이면 두 변수가 같은 방향으로 변하는 경향이 있고, 음수이면 반대 방향으로 변하는 경향이 있다.\n다만 공분산 값 자체는 단위의 영향을 받아 크기 비교가 어려워, 이를 표준화한 상관계수(Correlation Coefficient)가 자주 활용된다.\n\n\n5 . 로지스틱 회귀\nLogistic Regression\n종속 변수가 이진형(Binary)일 때 사용하는 회귀 기법이다. 이진형 변수의 예로는 성공/실패, 생존/사망, Yes/No 등이 있다.\n선형 회귀처럼 값을 직접 예측하는 것이 아니라, 특정 사건이 일어날 확률을 예측한다.\n예측된 확률은 0 과 1 사이의 값으로 표현되며, 이를 기준으로 특정 클래스로 분류할 수 있다.\n이를 위해 선형 회귀 모형을 기반으로 한 결과를 로그 오즈로 변환하고 로지스틱 함수를 적용하여 확률로 해석 가능하게 만든다."
  },
  {
    "objectID": "ADA/ADA09.0.html#단순-회귀분석",
    "href": "ADA/ADA09.0.html#단순-회귀분석",
    "title": "제 9장 회귀분석",
    "section": "03 단순 회귀분석",
    "text": "03 단순 회귀분석\nSimple Linear Regression\n하나의 독립 변수 X 와 하나의 종속 변수 Y 간의 선형적 관계를 분석하는 기법이다.\n주로 두 변수 사이의 직선적 관계를 파악하고, 이를 바탕으로 종속 변수를 예측하는 데 활용된다.\n예를 들어, 예약 건수와 판매량 간의 관계를 분석하거나, 공부 시간과 시험 성적 간의 관계를 분석할 때 적용할 수 있다.\n\n1 . 절편\n통계적으로 유의하지 않더라도 직선의 위치를 결정하는 수학적 요소이므로 회귀모형에는 일반적으로 포함하며, 실질적인 해석의 초점은 독립변수의 효과를 나타내는 기울기 계수에 맞추어진다.\n\n\n2 . 오차\n통계 분석에서 회귀모형은 항상 일정 수준의 오차를 전제로 한다. 만약 오차가 전혀 없다면 모든 데이터가 회귀직선 위에 위치하게 되지만, 실제 데이터에서는 관측값이 직선에서 벗어나기 때문에 오차가 발생한다.\n실무에서는 다중선형회귀와 로지스틱 회귀가 가장 많이 활용되며, 단순선형회귀는 주로 기초 개념 학습이나 기본 분석에 사용된다.\n단순선형회귀에서 중요한 점은 회귀계수를 추정하는 것으로, 추정된 값은 보통 헷(^) 기호를 붙여 나타낸다. 헷이 없는 기호는 모집단의 실제 모수를 의미한다.\n\n\n3 . 최소제곱법\nOrdinary Least Squares, OLS\n회귀분석에서 가장 널리 사용되는 회귀계수 추정 방법으로, 잔차(실제값과 예측값의 차이)의 제곱 합을 최소화하여 가장 적합한 회귀 직선을 구하는 기법이다.\n이를 통해 회귀 직선의 기울기(β₁)와 절편(β₀)을 추정할 수 있으며, 단순 선형 회귀의 경우 공식은 다음과 같다.\n기울기 공식\n절편의 공식\n\n\n4 . 회귀계수(기울기) 검정\n회귀 분석에서 중요한 요소는 기울기이다. 이는 독립 변수 X 가 종속 변수 Y 에 미치는 평균적 영향을 나타낸다. 이 계수가 통계적으로 유의미한지 검증하기 위해 가설 검정을 수행한다.\n가설 설정 Hypothesis Formulation\n귀무가설(H₀) : β = 0 (X 가 Y 에 영향을 주지 않는다) 대립가설(H₁) : β ≠ 0 (X 가 Y 에 유의한 영향을 준다)\n단순 선형 회귀에서는 회귀계수가 2 개(절편, 기울기)이므로 자유도는 n − 2 이다. 여기서 n 은 표본의 크기(데이터 포인트 수)이다.\n검정 통계량 Test Statistic\n회귀분석에서 검정 통계량(t - 통계량)은 회귀계수가 0 인지 여부를 검정하는 데 사용된다. 즉 독립변수와 종속변수 간의 관계가 유의미한지 검정할 수 있다.\n회귀분석에서 계수 추정치에 대한 검정은 주로 t-통계량을 활용하며, 이는 표본의 크기가 충분히 클 경우 정규분포에 근사하지만 원칙적으로는 자유도를 고려한 t - 분포를 따른다.\n이러한 검정을 위해 오차항은 평균이 0이고 분산이 일정하며, 서로 독립적으로 정규분포를 따른다는 가정이 필요하다.\n이 가정을 바탕으로 회귀계수 β 에 대한 가설검정과 신뢰구간이 설정되며, 연구자는 보통 귀무가설(계수=0)을 기각하고 대립가설(계수≠0)을 채택하기를 기대한다.\n기울기의 표준 오차 Standard Error of the Slope\n회귀계수(기울기) 추정치의 변동성 또는 신뢰도를 나타내는 값이다.\n즉, 동일한 데이터 표본에서 회귀분석을 반복할 경우, 추정된 기울기가 얼마나 변동할 수 있는지를 보여준다.\n표준 오차가 작을수록 기울기 추정치가 보다 정확하며, 회귀계수 검정과 신뢰구간 계산에 활용된다.\n잔차의 표준편차 Standard Error of the Estimate, σₑ\n회귀모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표로, 단순 선형 회귀에서는 추정할 회귀계수가 2 개(절편과 기울기)이므로 자유도는 n − 2 가 된다.\n이 값을 이용하여 회귀계수의 t − 통계량을 계산하고, 이를 기반으로 p − 값을 산출하여 귀무가설 기각 여부를 결정한다.\n일반적으로 p − 값이 0.05 보다 작으면 귀무가설을 기각하고, 독립변수가 종속변수에 유의미한 영향을 준다고 판단한다.\n단순 회귀분석을 통해 독립 변수와 종속 변수 간의 관계를 파악할 수 있으며, 최소제곱법(OLS)을 사용하여 회귀 직선을 추정한다.\n이후 회귀계수의 유의성을 검증하는 가설 검정과 자유도 계산을 통해, 모델이 통계적으로 신뢰할 만한지 판단하고 분석 결과를 해석할 수 있다.\n단순 회귀분석 사례 12개의 기업에 대해 1년 광고비와 매출액을 조사하여 얻은 것이다.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Sales.csv\"\nSales = pd.read_csv(url)\nSales.head()\n\n\n\n\n\n\n\n\nCompany\nAdver\nSales\n\n\n\n\n0\n1\n11\n23\n\n\n1\n2\n19\n32\n\n\n2\n3\n23\n36\n\n\n3\n4\n26\n46\n\n\n4\n5\n56\n93\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.jointplot(x = 'Adver', y = 'Sales',\n             data = Sales, kind = 'reg')\n\nimport statsmodels.formula.api as smf\nSalesFit = smf.ols(formula = 'Sales ~ Adver',\n                   data = Sales).fit()\nSalesFit.summary()\n\nC:\\Users\\sinji\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=12 observations were given.\n  return hypotest_fun_in(*args, **kwds)\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.979\n\n\nModel:\nOLS\nAdj. R-squared:\n0.976\n\n\nMethod:\nLeast Squares\nF-statistic:\n455.5\n\n\nDate:\nThu, 04 Dec 2025\nProb (F-statistic):\n1.14e-09\n\n\nTime:\n17:45:39\nLog-Likelihood:\n-32.059\n\n\nNo. Observations:\n12\nAIC:\n68.12\n\n\nDf Residuals:\n10\nBIC:\n69.09\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.2848\n2.889\n1.137\n0.282\n-3.153\n9.723\n\n\nAdver\n1.5972\n0.075\n21.343\n0.000\n1.430\n1.764\n\n\n\n\n\n\n\n\nOmnibus:\n0.879\nDurbin-Watson:\n2.470\n\n\nProb(Omnibus):\n0.644\nJarque-Bera (JB):\n0.379\n\n\nSkew:\n0.419\nProb(JB):\n0.828\n\n\nKurtosis:\n2.768\nCond. No.\n101.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n한편, 결정계수(R^2)는 회귀모형이 종속변수의 변동을 얼마나 설명하는지를 나타내지만, 값이 높다고 해서 항상 좋은 모델을 의미하는 것은 아니며 과적합 여부도 고려해야 한다.\n&lt;그림1&gt; 산점도와 회귀직선 데이터를 직관적으로 이해하기 위해 선형 관계를 가정하고 직선 형태의 모델을 사용하는 경우가 많지만, 실제 데이터가 비선형 관계를 보이면 비선형 모델을 적용하기도 한다. 데이터가 선형인지 비선형인지는 산점도를 통해 시각적으로 확인할 수 있으며, 이때 산점도는 독립변수와 종속변수가 모두 연속형일 때 의미가 있다. 한편, 범주형 변수는 산점도로 바로 표현할 수 없지만, 회귀모델에서는 더미 변수로 변환하여 포함시킬 수 있으므로, 범주형 변수 역시 분석에 반영할 수 있다.\n\nsns.lmplot(x = 'Adver', y = 'Sales', data = Sales)\nsns.regplot(x = 'Adver', y = 'Sales', data = Sales, lowess = True)\n\n\n\n\n\n\n\n\n반응변수에 대한 예측\n\npredictions = SalesFit.get_prediction()\npredictions.summary_frame(alpha = 0.05).round(3)\n\nSalesNew = pd.DataFrame({'Adver':[20, 30, 40]})\npredictions = SalesFit.get_prediction(SalesNew)\npredictions.summary_frame(alpha = 0.05).round(3)\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n35.228\n1.612\n31.636\n38.820\n25.961\n44.495\n\n\n1\n51.199\n1.185\n48.559\n53.840\n42.258\n60.140\n\n\n2\n67.171\n1.153\n64.601\n69.741\n58.251\n76.091"
  },
  {
    "objectID": "ADA/ADA09.0.html#잔차-분석",
    "href": "ADA/ADA09.0.html#잔차-분석",
    "title": "제 9장 회귀분석",
    "section": "04 잔차 분석",
    "text": "04 잔차 분석\nResidual\n실제 관측값과 회귀 모형에 의해 추정된 예측값의 차이.\n이는 데이터 점과 회귀직선 사이의 수직 거리를 의미한다. 회귀 모델이 데이터를 얼마나 잘 설명하는지를 나타내는 중요한 지표이며, 잔차가 작을수록 모델의 적합도가 높음을 의미한다.\n따라서 잔차 분석을 통해 회귀 직선이 데이터를 얼마나 잘 표현하는지, 그리고 모델 가정이 적절히 충족되는지를 평가할 수 있다.\n표준화잔차 standardized residuals\n표준화는 변수의 값을 평균 0 , 분산 1 로 변환하여 서로 다른 척도를 가진 변수를 비교 가능하게 만드는 과정이다.\n이를 통해 특정 변수가 값의 범위가 크다는 이유만으로 회귀분석에서 과도하게 영향을 미치는 문제를 완화할 수 있다.\n또한 잔차 분석 단계에서는 표준화 잔차나 학생화 잔차를 사용하여 이상치를 탐지 및 모형의 적합성을 진단하기도 한다.\n\nFitted = SalesFit.predict()\nResidual = SalesFit.resid # 순수한 자기 잔차\nRStandard = SalesFit.resid_pearson # 표준화잔차\npd.DataFrame({'Fitted':Fitted, \n              'Residual':Residual,\n              'RStandard':RStandard})\n\nfig, ax = plt.subplots(figsize = (10, 8))\nsns.scatterplot(x = Fitted, y = RStandard)\nax.axhline(y = 0)\n\n\n\n\n\n\n\n\n그러나 표준화는 잔차 간의 독립성을 보장하거나 분산 불균형 문제를 근본적으로 해결하지는 못한다.\n독립성 문제는 주로 데이터의 자기상관에서 기인하며, 이분산 문제는 가중회귀나 분산 안정화 변환과 같은 방법을 통해 보완해야 한다.\n따라서 표준화는 변수의 스케일을 맞추고 분석의 안정성을 높이는 데 중요한 역할을 하지만, 모든 회귀 진단 문제를 해결하는 방법은 아니라는 점을 유념해야 한다."
  },
  {
    "objectID": "ADA/ADA09.0.html#회귀-모델의-기본-가정",
    "href": "ADA/ADA09.0.html#회귀-모델의-기본-가정",
    "title": "제 9장 회귀분석",
    "section": "05 회귀 모델의 기본 가정",
    "text": "05 회귀 모델의 기본 가정\n회귀분석을 적용하기 위해서는 4 가지 기본 가정이 충족되어야 한다.\n\n1 . 선형성\nLinearity\n독립 변수와 종속 변수 간의 관계가 선형이어야 한다. 이는 산점도 등의 그래프를 통해 확인할 수 있다.\n\n\n2 . 정규성\nNormality\n오차의 분포가 정규분포를 따라야 한다. 이는 대부분의 데이터가 회귀선이라는 평균값 주변에 집중되어 있어야 함을 의미한다.\n\nimport numpy as np\nsns.distplot(RStandard, bins = 10)\n\nfrom scipy.stats import probplot\nprobplot(RStandard, plot = plt)\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_2200\\646685327.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(RStandard, bins = 10)\n\n\n((array([-1.58815464, -1.09814975, -0.78255927, -0.53069113, -0.30892353,\n         -0.101534  ,  0.101534  ,  0.30892353,  0.53069113,  0.78255927,\n          1.09814975,  1.58815464]),\n  array([-1.50086884, -1.04842015, -0.86297515, -0.42536736, -0.31286804,\n         -0.15710484,  0.07160403,  0.26692088,  0.31018475,  0.55989614,\n          1.15452596,  1.94447261])),\n (np.float64(1.014930164527946),\n  np.float64(3.64578479126661e-15),\n  np.float64(0.9869321023876501)))\n\n\n\n\n\n\n\n\n\n\n\n3 . 등분산성\nHomoscedasticity\n독립 변수의 값에 관계없이 오차의 분산이 일정해야 한다. 이 가정들이 만족되지 않으면,모델의 예측 성능이 저하되거나 편향된 결과를 초래할 수 있다. 따라서 회귀 분석을 수행할 때는 현실 문제에서 이러한 가정들이 만족되는지 먼저 확인해야 한다. 가정이 충족되면, 독립 변수와 종속 변수 간의 관계에 따라 적합한 회귀 모델 유형을 선택하여 분석을 진행하면 된다.\n\n\n4 . 독립성\nIndependence\n관측값들 간에는 독립성이 유지되어야 한다. 다중공선성일 경우, 각 독립 변수가 종속 변수에 미치는 영향을 개별적으로 평가하기가 어렵기 때문이다.\n\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(RStandard) # 기준: 표준화잔차\n\nimport statsmodels.api as sm\nfig = plt.figure(figsize = (12, 8))\nfig = sm.graphics.plot_regress_exog(SalesFit, 'Adver', fig = fig)\n\n\n\n\n\n\n\n\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA08.1.html",
    "href": "ADA/ADA08.1.html",
    "title": "제 8장-연관성의 측도",
    "section": "",
    "text": "Reporting Date: October. 5, 2025"
  },
  {
    "objectID": "ADA/ADA08.1.html#연관성의-측도",
    "href": "ADA/ADA08.1.html#연관성의-측도",
    "title": "제 8장-연관성의 측도",
    "section": "01 연관성의 측도",
    "text": "01 연관성의 측도\nAssociation Measures\n두 변수 간 관계의 정도와 강도를 파악할 때 사용되는 통계적 지표를 총칭한다.\n특히 명목형(범주형) 변수의 경우, 단순히 유의성 검정(p-value)만으로는 관계의 강도를 알 수 없으므로, 파이계수(ϕ), 분할계수(C), 크라머의 V 등 다양한 연관성 측도를 활용한다.\n연관성 척도 중 일부 지표는 순서형 변수에 특히 적합하다(예: 스피어만 상관계수).\n즉, 단순히 “관련이 있는가? 를 넘어”관련이 있다면, 그 강도는 어느 정도인가?“를 정량적으로 평가하는 것이 목적이다.\n앞서 코호트 및 사례 대조 연구에서는 오즈비(OR)를 통해 사건 발생과 노출 간의 연관성을 평가하였다.\n이번 장에서는 이를 확장하여, 범주형 변수 전반에서의 연관성 강도를 정량적으로 측정하는 대표적 지표들을 살펴본다.\n1 . 파이계수 Phi Coefficient\n두 명목형 변수 간의 관계의 강도를 측정한다. 주로 2×2 교차표에 사용되며, 상관계수(Pearson’s r)와 유사한 해석이 가능하다.\n값의 범위는 (-1 ≤ ϕ ≤ +1)이다.\n0 : 독립적 관계 +1: 완전한 양의 관계 −1: 완전한 음의 관계 \\[\\phi = \\frac{ad - bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\\]\n여기서, a, b, c, d는 교차표의 각 셀 값이다.\n2 . 분할계수 Contingency Coefficient, C\n2×2 이상 교차표에서 변수 간 연관성을 평가하는 지표로, 파이계수의 확장으로 이해 가능할 수 있다.\n값의 범위는 (0 ≤ C &lt; 1)이다.\n0: 독립적 관계 1에 가까울수록 강한 의존 관계(강한 연관성) 단, 교차표의 크기가 커질수록 C의 최대값이 1 보다 작아지므로 지표 간 비교 시 주의가 필요하다.\n3 . 크라머의 V Cramér’s V\nn×m 형태 교차표에서 변수 간 연관성을 표준화된 형태로 측정한다.\n파이계수의 일반화된 형태이며, 교차표의 크기 차이를 보정하여 표준화된 연관성 강도를 제공한다.\n값의 범위는 (0 ≤ V ≤ 1)이다.\n0: 독립적 관계 1: 완전한 연관 관계 \\[V = \\sqrt{\\frac{\\chi^2}{n \\cdot \\min(k-1, r-1)}}\\]\nχ²: 카이제곱 통계량 n: 총 샘플 수 k: 행 개수, r: 열 개수\n[요약 비교표] 지표 적용 교차표 형태 값의 범위 관계 방향 표준화 여부 비고 파이계수 (ϕ) 2×2 −1 ~ +1 가능 X 상관계수와 유사 분할계수 (C) 2×2 이상 0 ~ 1 미만 불가능 X 표 크기 커지면 1 미만 크라머의 V n×m 0 ~ 1 불가능 O 표준화된 강도 비교 가능\n02 명목형 변수들의 연관성 앞 장에서는 파이계수(ϕ), 분할계수(C), 크라머의 V 등을 통해 범주형 변수 간의 관계 강도를 정량적으로 측정하였다.\n이번 장에서는 그중에서도 명목형 변수 간의 예측력 기반 연관성을 나타내는 지표인 람다(Lambda)를 살펴본다.\n1 . 개념 개요 람다는 두 명목형 변수 간의 관계를, 한 변수를 이용해 다른 변수를 얼마나 정확히 예측할 수 있는가로 표현한다.\n즉, “A 변수를 알고 있을 때 B의 예측 오류가 얼마나 감소하는가” 를 통해 변수 간의 연관성을 측정한다.\n람다는 두 가지 형태로 구분된다.\n비대칭 람다 (Asymmetric Lambda): 한 변수를 기준으로 다른 변수를 예측할 때 사용 대칭 람다 (Symmetric Lambda): 두 변수의 관계를 방향성 없이 측정할 때 사용\n2 . 비대칭 람다 Asymmetric Lambda\n한 변수를 기준으로 다른 변수를 예측할 때의 오류 감소 비율을 계산한다. 즉, 기준 변수(행 또는 열)의 정보가 주어졌을 때 예측 오류가 얼마나 줄어드는가를 측정한다.\n\\[\\lambda = \\frac{E_1 - E_2}{E_1}\\]​​\n(E_1:)​ 기준 변수의 정보를 사용하지 않았을 때의 예측 오류 (E_2:)​ 기준 변수의 정보를 사용했을 때의 예측 오류 λ가 0이면 두 변수 간 연관성이 없으며, λ가 1에 가까울수록 기준 변수를 통해 다른 변수를 더 정확히 예측 가능하다는 의미다.\n3 . 대칭 람다 Symmetric Lambda\n두 변수 간의 관계를 방향성 없이, 즉 서로 동등하게 평가하는 방법이다. 비대칭 람다를 각각 계산한 후 평균을 취하여 구한다.\n\\[\\lambda_s = \\frac{(\\lambda_{A|B} + \\lambda_{B|A})}{2}\\]\n이 값이 클수록 두 변수는 상호 예측력이 높아 강한 연관성을 가짐을 의미한다.\n03 순서형 변수의 연관성 측정 MH 카이제곱을 사용하여 연관성을 분석할 수 있다. 두 순서형 변수 간의 관계를 측정하는 데 유용한 방법입니다.\n감마 계수\nGamma Coefficient\n순서형 변수에서 감마 계수를 사용하여 두 변수 간의 단조 관계를 분석할 수 있다. 감마 계수는 일치쌍과 비일치쌍의 수를 통해 계산됩니다.\n일치쌍: 지위가 높을 때 만족하는 경우 (내가 세운 가설) 비일치쌍: 지위가 높을 때 불만족하는 경우 (가설과 반대되는 것.)\n감마 계수는 -1에서 +1 사이의 값을 가진다. +1은 완전한 양의 일치, -1은 완전한 음의 일치를 의미합니다. 0은 관련성이 없음을 의미한다.\n코드: 학력의 따른 비율 상관계수 서로의 영향이 높지 않음, 약간의 영향 두 순서형 변수의 선형적 연관성 검정 수치적으로는 났지만 연관성은 있다. 감마계수 계산은 파이썬에서 제공을 하지 않는다.\n심프슨의 역설\nSimpson’s Paradox\n제3의 변수가 두 변수 간의 관계에 영향을 미칠 수 있다는 개념이다. 즉, 제3의 변수가 없을 경우에는 관계가 보였던 두 변수 간의 연관성이, 제3의 변수를 포함하면 역으로 나타날 수 있습니다.\n예를 들어, 흡연과 폐암 간의 관계를 분석할 때, 연령대라는 변수에 따라 두 변수 간의 관계가 다르게 해석될 수 있습니다.\n심프슨의 역설을 방지하기 위해서는 분석 시 제3의 변수를 항상 고려하는 것이 중요합니다.\n남여의 대학원 합격율이 전체에서 10% 차이가 남 이에 대해 버클리 대학원 관계자들은 합격율을 전공 별로 변수를 봄. 오히려 합격율이 남성이 더 높았음, 이는 지원자의 비가 8:1로 차이가 났기 때문.\nIQ와 알코올의 관계에서 저연령과 고령령 변수를 넣으니 고지능자가 덜 술을 마신다는 것.\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA07.0.html",
    "href": "ADA/ADA07.0.html",
    "title": "제 6장-두 모집단에 대한 비교",
    "section": "",
    "text": "Reporting Date: Septemger. 28, 2024\n\n두 연속형 변수들 간의 연관성을 측정하는 데 사용되는 상관계수에 대해 다루고자 한다. (4장 두 변수 자료의 요약과 이어지는 내용이다.)\n두 변수의 공분산 구하는 과정\n\n각 데이터에서 평균을 빼서, 두 변수의 편차를 각각 구한다.\n\n각각의 편차 계산 과정\n\n두 변수 각각의 편차를 곱한 후 합산하는 방식이다.\n\n표준 공분산 계산 과정 더한 값에서 데이터 개수 n – 1 로 나눈 값이 표준 공분산이 된다.\n\n공분산의 값은 – ∞ ~ + ∞ 사이에 존재한다.\n\n공분산은 두 변수 간의 선형 관계를 나타내기 때문에, 그 값은 음의 무한대에서 양의 무한대까지의 범위를 가질 수 있다.\n공분산이 양수이면 두 변수는 같은 방향으로 움직이고, 음수이면 반대 방향으로 움직인다. 두 값의 편차를 구하고, 이를 공분산으로 계산하는 과정은 Z–분포와 일부 유사할 수 있다. Z–분포는 표준화를 기반으로 하고, 상관계수도 공분산을 표준화하는 과정이 있기 때문에 연결이 가능하다. 다만, Z–분포는 정규 분포의 표준화 개념을 나타내고, 상관계수는 두 변수 간의 관계를 표준화하여 설명하는 것이므로 서로 다른 개념이라는 점을 주의해야 한다. 공분산은 단위가 포함된 값이다.\n서로 다른 단위를 가진 변수들 간의 공분산은 크기나 해석에서 어려움이 있을 수 있다.\n예를 들어, 키와 몸무게의 공분산은 그 자체로 의미를 이해하기 어렵다. 이를 보다 쉽게 해석하기 위해, 공분산을 표준화하여 단위를 제거한 새로운 값인 상관계수를 사용한다.\n단위를 제거하기 위해 공분산을 각각의 표준편차로 나눈다.\n이 과정에서 각 변수의 단위가 없어지고, –1 ~ +1 사이의 범위를 갖는다. 결과적으로 이것을 피어슨 적률상관계수라고 한다.\n피어슨의 적률상관계수 (Pearson correlation coefficient)\n둘 다 피어슨 상관계수를 나타내지만, 일반적으로 상관계수는 피어슨의 적률상관계수를 뜻한다. 왼쪽은 표본을 통해 추정한 상관계수이고,오른쪽은 모집단의 상관계수를 의미한다. 표본상관계수 r을 이용하여,모상관계수 ρ에 대한 가설검증을 할 수 있다.\n귀무가설에 대한 검정통계량은 아래와 같은 통계량으로서 이는 귀무가설 하에서 자유도 n – 2인 T–분포를 따른다. 더 일반적인 상관계수에 대한 귀무가설 검정을 수행하려면, 피셔의 Z–변환을 이용할 수 있다.\n스피어만의 순위상관계수 (Spearman’s Rank Correlation Coefficient)\n두 변수 간의 비선형 관계를 측정하는 방법으로, 각 데이터의 순위를 사용하여 상관관계를 계산한다.\n이는 데이터가 서열형(순위) 혹은 비모수적일 때 유용하며, 피어슨 상관계수와 달리 데이터의 분포에 대해 가정하지 않는다.\n이 공식은 순위 차이의 크기가 클수록 상관관계가 약해진다는 점을 반영한다.\n두 순위 간 차이를 나타내므로 양수, 음수, 0 모두 가질 수 있다.\n사례: 소득과 지출 사이에는 상관관계가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Student.csv\"\nStudent = pd.read_csv(url)\nStudent.head()\n\n\n\n\n\n\n\n\nID\nAge\nIncome\nExpense\n\n\n\n\n0\n1\n25\n170\n67\n\n\n1\n2\n28\n177\n62\n\n\n2\n3\n20\n165\n53\n\n\n3\n4\n16\n150\n48\n\n\n4\n5\n19\n160\n58\n\n\n\n\n\n\n\n4개의 변수 중 Age(나이), Income(소득), Expense(지출) 변수를 사용하기로 한다.\n먼저, 산점도를 통해 데이터 분포의 전체적인 형태를 시각화한다.\n\nimport matplotlib.pyplot as plt\nplt.plot('Income', 'Expense', # 소득, 지출\n         'o', # 점의 형태: 원형\n         color = 'black', data = Student)\nplt.xlabel('Income')\nplt.ylabel('Expense')\n\nText(0, 0.5, 'Expense')\n\n\n\n\n\n\n\n\n\nplot 버전\n\nplt.scatter('Income', 'Expense', \n            data = Student)\nplt.xlabel('Income')\nplt.ylabel('Expense')\n\nText(0, 0.5, 'Expense')\n\n\n\n\n\n\n\n\n\nscatter 버전 위 산점도는 전반적으로 상승하는 경향을 보이며, 타원형 분포를 통해 양의 상관관계를 나타낸다. 신뢰구간, 회귀직선, 히스토그램이 추가된 산점도를 그린다\n\nimport seaborn as sns\nsns.jointplot(x = 'Income', y = 'Expense', \n              data = Student, kind = \"reg\")\n\n\n\n\n\n\n\n\n하늘색 영역은 신뢰구간이다.\n산점도 행렬 출력하기\n\nsns.pairplot(Student.iloc[:,1:4])\ng = sns.PairGrid(Student.iloc[:,1:4])\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n정방행렬의 형태로 피어슨의 상관계수 출력\nStudent.iloc[:,1:4].corr(method=‘pearson’)\n같은 행렬에서 상관계수가 1로 나오는 이유: 완벽한 양의 선형관계가 존재하기 때문이다. 전반적으로, 모든 변수들 간의 상관계수가 0.5 이상으로 비교적 높다.\n두 변수 수입과 지출 간의 모상관계수가 0인지를 검정한다.\n\nfrom scipy.stats import pearsonr\npearsonr(Student.Income, Student.Expense)\n\nPearsonRResult(statistic=np.float64(0.6812956535794542), pvalue=np.float64(0.0026006496946941993))\n\n\n사회과학 분야에서는 상관계수가 0.4 정도만 되어도 높은 값으로 간주된다. 표본 상관계수는 0.681로 비교적 높게 나타났으며, p–값은 0.002(0.2%)로 매우 낮아 귀무가설을 기각할 수 있다.\n따라서, 모상관계수가 0이 아니라고 결론지을 수 있다.\n상관계수와 p–값을 함께 출력한다.\n\n# !pip install pingouin\n\nimport pingouin as pg\nStudent.iloc[:,1:4].pairwise_corr(method='pearson').round(3)\n\n\n\n\n\n\n\n\nX\nY\nmethod\nalternative\nn\nr\nCI95%\np-unc\nBF10\npower\n\n\n\n\n0\nAge\nIncome\npearson\ntwo-sided\n17\n0.547\n[0.09, 0.81]\n0.023\n3.255\n0.653\n\n\n1\nAge\nExpense\npearson\ntwo-sided\n17\n0.530\n[0.07, 0.81]\n0.029\n2.735\n0.619\n\n\n2\nIncome\nExpense\npearson\ntwo-sided\n17\n0.681\n[0.3, 0.88]\n0.003\n19.505\n0.889\n\n\n\n\n\n\n\n상관계수행렬을 그래프로 그린다\n\ncorrMatrix = Student.iloc[:,1:4].corr(\n    method = 'pearson')\n\nimport seaborn as sns\nsns.heatmap(corrMatrix, \n            annot = True) # 그림 위 수치 표시\n\n\n\n\n\n\n\n\n편상관계수 (Partial Correlation Coefficient)\n두 변수 간의 상관관계를 분석할 때,특정 다른 변수의 영향을 통제(고정)한 후의 상관관계를 나타내는 지표. 이를 통해 변수들 간의 순수한 연관성을 파악할 수 있다.\n사례: 변수인 나이를 통제했을 때, 기능과 디자인에 대한 만족도 간에 상관관계가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satis.csv\"\nSatis = pd.read_csv(url)\nSatis.head()\n\n\n\n\n\n\n\n\nID\nAge\nSatis1\nSatis2\n\n\n\n\n0\n1\n28\n0\n70\n\n\n1\n2\n23\n0\n55\n\n\n2\n3\n26\n5\n65\n\n\n3\n4\n27\n5\n65\n\n\n4\n5\n25\n10\n60\n\n\n\n\n\n\n\n4개의 변수 중 ID를 제외한 나머지 변수를 사용하기로 한다.\n1 ~ 3번째 열까지의 상관계수를 구하고, 그 값을 소수점 셋째 자리로 반올림하여 출력\n\nSatis.iloc[:,1:4].corr().round(3)\n\n\n\n\n\n\n\n\nAge\nSatis1\nSatis2\n\n\n\n\nAge\n1.000\n0.698\n0.993\n\n\nSatis1\n0.698\n1.000\n0.703\n\n\nSatis2\n0.993\n0.703\n1.000\n\n\n\n\n\n\n\n모든 상관계수들이 0.6보다 큰 값을 가지고 있다.\n두 변수 간의 피어슨 상관계수와 p–값 출력\n\nfrom scipy.stats import pearsonr\npearsonr(Satis.Satis1, Satis.Satis2)\n\nPearsonRResult(statistic=np.float64(0.7030501468749641), pvalue=np.float64(0.0005449862495058279))\n\n\np–값이 0.0005(0.05%)이므로, 귀무가설을 기각할 수 있다.\n1 ~ 3열까지의 데이터에 대해 편상관계수 출력\n\nimport pingouin as pg\nSatis.iloc[:,1:4].pcorr().round(3)\n\n\n\n\n\n\n\n\nAge\nSatis1\nSatis2\n\n\n\n\nAge\n1.000\n-0.003\n0.987\n\n\nSatis1\n-0.003\n1.000\n0.115\n\n\nSatis2\n0.987\n0.115\n1.000\n\n\n\n\n\n\n\nAge – Satis2 (0.9): 매우 강한 양의 상관관계가 있으며, Age – Satis1 (-0.0): 0에 가까우므로, 거의 상관관계가 없다.\nSatis1 – Satis2 (0.1): 약한 양의 상관관계를 가지고 있다.\nAge를 공변량(covariate)으로 통제한 뒤, Satis1과 Satis2의 순수한 상관관계를 측정한다.\n\nSatis.partial_corr(x='Satis1', y='Satis2', covar='Age').round(3)\n\n\n\n\n\n\n\n\nn\nr\nCI95%\np-val\n\n\n\n\npearson\n20\n0.115\n[-0.36, 0.54]\n0.639\n\n\n\n\n\n\n\nCI95%는 상관계수 r에 대한 95% 신뢰구간이다. 즉, 실제 상관계수가 -0.36 ~ 0.54 사이에 있을 가능성이 95%라는 의미이다.\n상관계수가 음수와 양수일 가능성을 모두 포함하는 넓은 범위는 상관관계가 약하다는 것을 의미한다.\np–값은 0.6(60%)이므로, 두 변수 사이의 상관관계는 유의하지 않다고 결론지을 수 있다. 결론적으로, 나이가 많을수록 제품의 기능과 디자인에 대한 만족도가 모두 증가하는 경향이 있다. 앞서 나이가 중요한 요인으로 확인되었으므로, 이번에는 나이를 30세 이하와 30세 초과로 나누어 산점도를 그려보았다.\n\nSatis.loc[Satis.Age &lt; 30,\"AgeGroup\"] = \"Under30\"\nSatis.loc[Satis.Age &gt;= 30,\"AgeGroup\"] = \"Over30\"\nsns.jointplot(x = 'Satis1', y = 'Satis2', \n              data = Satis, hue = \"AgeGroup\")\n              # hue: 집단 변수(색을 구분하는 변수)\n\n\n\n\n\n\n\n\n연령대별로 구분할 경우, 두 변수의 상관성이 상당히 낮아짐을 볼 수 있다. 두 변수로 구분된 산점도가 전혀 겹치지 않고, 비선형적인 분포를 보이므로, 이들은 서로 연관성이 없다고 판단할 수 있다.\n연령대별에 대한 상관계수를 출력해본다.\n\nSatisUnder30 = Satis.loc[Satis.AgeGroup == \"Under30\",]\nSatisOver30 = Satis.loc[Satis.AgeGroup ==\"Over30\",]\n\nfrom scipy.stats import pearsonr\nprint(pearsonr(SatisUnder30.Satis1, SatisUnder30.Satis2))\nprint(pearsonr(SatisOver30.Satis1, SatisOver30.Satis2))\n\nPearsonRResult(statistic=np.float64(0.38797014489949266), pvalue=np.float64(0.38979066100659016))\nPearsonRResult(statistic=np.float64(0.17182016983164788), pvalue=np.float64(0.5746020827503785))\n\n\n상관계수의 값은 모두 낮고, 유의수준보다 p–값이 더 크므로, 두 변수 사이의 상관관계는 유의하지 않다고 결론지을 수 있다.\n신뢰도 분석 측정 도구가 일관되게 측정하고 있는지를 평가하는 과정.\nt와 e는 서로 독립(두 변수가 서로 대응되지 않는 것)이다.\n분산을 기반으로 두 변수 간의 관계 강도를 나타낸다.\n이 식은 피어슨 상관계수의 정의에서 유도된 것이며, 상관계수의 제곱을 나타내는 공식이다.\n식은 다음과 같이 분리된 형태로 표현된다.\n오차 e와 진정한 값 t가 독립적이라고 가정하면 Cov( e, t ) = 0이다.\n위 식을 대입한 뒤, 정리한다.\n따라서, 신뢰도는 참점수 t와 관측점수 X의 분산비(ratio of variance)로 해석할 수 있다.\n크론바흐의 알파 (Cronbach’s alpha)\n내적일치도(internal consistency)을 측정하기 위한 통계 지표. 여러 문항이 동일한 개념을 얼마나 잘 측정하는지를 나타낸다.\n위 계수는 0 ~ 1 사이의 값을 가지며, 값이 클수록 측정 도구의 신뢰도가 높다는 것을 의미한다.\n사례: 알파 계수 사용하여, 기업 구성원의 의식을 알아보기\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Ability.csv\"\nAbility = pd.read_csv(url)\nAbility.head()\n\n\n\n\n\n\n\n\nID\nQ01\nQ02\nQ03\nQ04\nQ05\nQ06\nQ07\nQ08\nQ09\nQ10\n\n\n\n\n0\n1\n4.0\n4.0\n4.0\n4.0\n4.0\n4\n3\n3.0\n3.0\n2.0\n\n\n1\n2\n4.0\n4.0\n3.0\n3.0\n4.0\n4\n2\n2.0\n4.0\n2.0\n\n\n2\n3\n5.0\n4.0\n3.0\n4.0\n4.0\n5\n2\n3.0\n2.0\n4.0\n\n\n3\n4\n3.0\n4.0\n2.0\n4.0\n4.0\n4\n2\n4.0\n2.0\n3.0\n\n\n4\n5\n4.0\n3.0\n4.0\n3.0\n2.0\n3\n2\n2.0\n3.0\n2.0\n\n\n\n\n\n\n\n위 설문조사는 10개의 문항이 존재하며, 대상자는 각 문항에 대해 1 ~ 5점까지의 값을 매길 수 있다.\n상사의 업무수행능력에 대한 신뢰도를 분석한다.\n\nAbility[[\"Q01\", \"Q02\", \"Q03\"]].corr()\n\nimport pingouin as pg\npg.cronbach_alpha(data = Ability[[\"Q01\", \"Q02\", \"Q03\"]])\n\n(np.float64(0.7351921832148215), array([0.697, 0.769]))\n\n\n알파 계수의 값이 0.735로서 내적일관성 신뢰도가 높다는 것을 알 수 있다. 상사와의 공적/사적 긴밀함에 대한 신뢰도 분석하기.\n\nAbility[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]].corr()\n\n\n\n\n\n\n\n\nQ04\nQ05\nQ06\nQ07\n\n\n\n\nQ04\n1.000000\n0.100787\n0.307348\n-0.351547\n\n\nQ05\n0.100787\n1.000000\n0.241432\n-0.273088\n\n\nQ06\n0.307348\n0.241432\n1.000000\n-0.350307\n\n\nQ07\n-0.351547\n-0.273088\n-0.350307\n1.000000\n\n\n\n\n\n\n\n4개의 문항에 대해 알파 계수를 계산하였다.\n\npg.cronbach_alpha(data = Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]])\n\n(np.float64(-0.2224494957901865), array([-0.386, -0.073]))\n\n\n알파 계수의 값이 –0.222로 계산되었으며, 이 값은 신뢰도가 음수라는 것을 나타낸다. 일반적으로 알파가 음수값을 가질 경우, 문항들이 서로 일관성이 없음을 의미한다. 이는 측정 도구가 잘못 설계되었거나, 각 문항이 서로 다른 개념을 측정하고 있음을 나타낼 수 있다.\n특히 문항 Q07이 나머지 문항들의 점수와 음의 상관을 가진다는 것을 알 수 있다.\n이 신뢰구간은 크론바흐의 알파 값의 신뢰성을 보여준다. 신뢰구간의 하한이 –0.386이고 상한이 –0.073인 것으로, 신뢰구간 내에 음수가 포함되어 있다.\n이는 크론바흐의 알파가 신뢰할 수 없는 상태임을 더욱 확증한다. 이 결과를 바탕으로 측정 도구의 문항을 재검토하거나 수정하는 것이 필요할 것으로 보인다.\nQ07 문항의 값을 6에서 빼는 방식으로 역전환을 시도한다.\n\nAbility[\"Q07_R\"] = 6 - Ability.Q07\nAbility[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]].corr()\n\n\n\n\n\n\n\n\nQ04\nQ05\nQ06\nQ07\n\n\n\n\nQ04\n1.000000\n0.100787\n0.307348\n-0.351547\n\n\nQ05\n0.100787\n1.000000\n0.241432\n-0.273088\n\n\nQ06\n0.307348\n0.241432\n1.000000\n-0.350307\n\n\nQ07\n-0.351547\n-0.273088\n-0.350307\n1.000000\n\n\n\n\n\n\n\n긍정적인 답변을 가진 경우, 다른 문항들과 반대되는 경향을 보여주기 때문이다.\n\npg.cronbach_alpha(data = Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07_R\"]])\n\n(np.float64(0.5876937990663734), array([0.532, 0.638]))\n\n\n알파 계수의 값이 0.587로서 비교적 만족스럽고, 각 문항들과 다음 문항들의 점수와의 상관계수도 모두 양의 부호를 가진다는 것을 알 수 있다.\n업무추진의 독자성에 대한 신뢰도 분석하기.\n\nAbility[[\"Q08\", \"Q09\", \"Q10\"]].corr()\n\n\n\n\n\n\n\n\nQ08\nQ09\nQ10\n\n\n\n\nQ08\n1.000000\n0.22047\n0.067734\n\n\nQ09\n0.220470\n1.00000\n-0.001290\n\n\nQ10\n0.067734\n-0.00129\n1.000000\n\n\n\n\n\n\n\n3개의 문항에 대해 알파계수를 계산한 것이다.\n\npg.cronbach_alpha(data = Ability[[\"Q08\", \"Q09\", \"Q10\"]])\n\n(np.float64(0.25112573908813685), array([0.144, 0.347]))\n\n\n알파 계수의 값이 0.251로 매우 낮게 나타나고 있다.\n따라서, 이들 문항이 하나의 동질적인 개념을 측정한다는 것을 신뢰할 수 없으며 세 문항의 합점수를 사용하는 것은 문제가 된다는 것을 알 수 있다. 사용자 정의 함수를 이용하여, 표준화 크론바흐 알파의 계산식을 구현하였다.\n\nimport numpy as np\ndef CronbachAlpha(df):\n    df_corr = df.corr()\n    N = df.shape[1]\n    rs = np.array([])\n    for i,col in enumerate(df_corr.columns):\n        sum_ = df_corr[col][i+1:].values\n        rs = np.append(sum_, rs)\n    mean_r = np.mean(rs)\n    cronbach_alpha = (N * mean_r) / (1 + (N-1) * mean_r)\n    return cronbach_alpha\nCronbachAlpha(Ability[[\"Q01\", \"Q02\", \"Q03\"]])\n\nnp.float64(0.7340559223240841)\n\n\n일반적으로 크론바흐의 알파가 0.7 이상이면 문항들 간의 일관성이 적절하다고 평가된다.\n\nCronbachAlpha(Ability[[\"Q04\", \"Q05\", \"Q06\", \"Q07\"]])\n\nnp.float64(-0.25906347398614543)\n\n\n음수가 나온 경우, 문항들 간에 부정적인 상관관계가 있을 가능성이 높으며, 문항들이 제대로 된 일관성을 가지지 못하고 있음을 나타낸다.\n\nCronbachAlpha(Ability[[\"Q08\", \"Q09\", \"Q10\"]])\n\nnp.float64(0.24084556669287197)\n\n\n이 값은 상대적으로 낮은 문항들 간의 상관성을 나타낼 수 있으며, 이는 문항들이 일관성이 부족하다는 것을 의미한다.\n결과적으로 의미있는 신뢰도를 가진 문항은 Q01 ~ Q03 사이이다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA06.0.html",
    "href": "ADA/ADA06.0.html",
    "title": "제 6장-두 모집단에 대한 비교",
    "section": "",
    "text": "Reporting Date: Setember. 18, 2024 두 모집단의 모평균, 모비율, 모분산의 차이에 대한 가설검증 문제를 다루고자 한다. (12장: 두 모집단의 비교와 이어지는 내용이다.)\n\n표본 평균을 추정하려면, 표본의 크기와 모분산을 고려해야 한다.\n[ 1 ] 두 모분산 σ12, σ 22 이 모두 알려져 있는 경우,\n두 모평균 차에 대한 “추정량” ⇨ “두 표본평균의 차” 통계적 추론을 위한 “준비물” ⇨ “추정량의 분포”\n이 분포는 다음과 같은 평균과 분산을 가진 정규분포를 따른다:\n표준화된 확률변수 Z는 표준정규분포 N(0, 1)를 따른다.\n[ 2 ] 두 모분산 σ12, σ 22 을 모두 모르는 경우, 표본의 크기를 고려하게 된다.\n표본의 크기가 충분히 큰 경우 ( 25 이상 )\n중심극한정리에 의해 모집단의 분포에 관계없이 x̄ 와 ȳ 가 근사적으로 정규분포를 따른다.\n두 모분산의 추정치인 표본분산 s₁², s₂² 를 고려한 통계량을 사용하여 검정을 수행한다.\n[ 3 ] 두 모집단이 알려져 있지는 않지만, 모분산이 동일한 것으로 가정할 수 있는 경우,\n다음과 같은 평균과 분산을 가지는 정규분포를 따른다:\n[1] 과 동일하다.\n공통분산 σ ² 의 합동추정량 (Pooled Variance)\n자유도 n ₁ + n ₂ – 2인 t-분포를 따른다.\n[ 4 ] 두 모분산이 서로 다른 경우, [3]번 식은 t-분포를 따르지 않는다.\n단, 아래와 같이 자유도를 수정할 경우, 근사적으로 t-분포를 따르게 된다.\n근사적으로 t-분포를 따르게 된다.\n수정된 자유도(df).\n독립표본에 의한 두 모평균의 비교:\n독 립 표 본 t – 검 정\n두 개의 서로 독립적인 집단의 평균을 비교하여 그 차이가 통계적으로 유의한지 판단하는 방법이다.\n사례 새로운 강의방식이 초등학생 독해력 향상에 도움이 되는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Reading.csv\"\nReading = pd.read_csv(url)\nReading.head()\n\n\n\n\n\n\n\n\nID\nGroup\nScore\n\n\n\n\n0\n1\nNew\n75\n\n\n1\n2\nNew\n80\n\n\n2\n3\nNew\n72\n\n\n3\n4\nNew\n77\n\n\n4\n5\nNew\n69\n\n\n\n\n\n\n\n가설검증을 결정하기 전에 데이터를 시각화한다.\n\nimport seaborn as sns  # 박스 플롯\nsns.boxplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n중위수와 같은 요인을 비교한 결과, 차이가 나타나므로 이를 근거로 검증을 진행할 수 있다.\n\n# 바이올린 플롯\nsns.violinplot(x = 'Group', y = 'Score', data = Reading)\n\n\n\n\n\n\n\n\n\n# 기술통계량\nReading.groupby('Group').Score.describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nGroup\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n8.0\n75.375\n4.373214\n69.0\n71.75\n76.0\n78.50\n81.0\n\n\nOld\n8.0\n69.125\n4.086126\n63.0\n67.25\n69.0\n71.25\n76.0\n\n\n\n\n\n\n\n양측검정 적용.\n\n# 그룹 나누기\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n# 양측검증:\n# 두 강의 방식에 차이가 있다. vs 차이가 없다.\n\nfrom scipy.stats import ttest_ind  # 독립 t-검정\nttest_ind(New.Score, Old.Score, equal_var = True)\n\n\n# T통계량: 그룹 간 평균 차이가 실제로 존재하는지를 나타내는 통계량.\n# 통계량이 클수록 차이가 있을 가능성이 높다.\n\n# [3]번 통계량: statistic=2.9536127902039953\n\n\n# 두 꼬리 검정에서의 p-값: pvalue=0.010470744188033123\n\n# 통상적으로 p-값이 0.05보다 작으면 귀무가설을 기각할 수 있다. \n# 즉, 두 강의 방식에 차이가 있다고 결론 내릴 수 있다.\n\nTtestResult(statistic=np.float64(2.9536127902039953), pvalue=np.float64(0.010470744188033123), df=np.float64(14.0))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\n# 단측검정:\n# 새로운 학습법이 더 효과적이다. vs 효과적이지 않다.\n\nstat, pval = ttest_ind(New.Score, Old.Score, equal_var = True)\nprint(\"P\", pval/2)\n\n# p-값이 0.0052로 유의수준 0.05보다 작으므로, 대립가설을 채택할 수 있다.\n\nP 0.005235372094016561\n\n\n단측검정과 등분산 가정 적용.\n\n# 단측검정\nfrom statsmodels.stats.weightstats import ttest_ind\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'pooled') # 등분산 가정 적용:\n          # 두 그룹 간의 분산이 동일하다고 가정\n\n(np.float64(2.9536127902039953),\n np.float64(0.005235372094016561),\n np.float64(14.0))\n\n\n단측검정과 이분산 가정 적용.\n\n# 단측검정\nttest_ind(New.Score, Old.Score, alternative = 'larger', \n          usevar = 'unequal') # 이분산 가정 적용:\n          # 두 그룹의 분산이 서로 다르다는 가정\n\n# [4]번 통계량: usevar= 'pooled' ⇨ 'unequal'\n# 14 ⇨ 13.935945095796395 (자유도가 실수로 바뀜)\n\n(np.float64(2.9536127902039953),\n np.float64(0.005256688626975243),\n np.float64(13.935945095796395))\n\n\n결론적으로, 새로운 강의방식이 초등학생 독해력 향상에 도움이 된다고 할 수 있다.\n대응표본에 의한 두 모평균의 비교:\n대응표본 t – 검정\n어떤 신발의 마모율을 비교할 때, 독립 표본 검정에 경우, 한 그룹의 사람이 왼쪽 신발을 신고, 다른 그룹의 사람이 오른쪽 신발을 신더라도 상관이 없다.\n하지만 대응 표본 검정은 동일한 사람이 왼쪽 신발과 오른쪽 신발을 모두 신어야 만 한다.\n각 쌍이 서로 연관되어 있으므로 두 신발을 신는 사람이 동일해야 하며, 표본의 수도 일치해야 한다.\n이는 마모율에 영향을 줄 수 있는 교락 요인(confounding factor), 즉 신발을 신는 사람의 특성 등을 배제하기 때문이다.\n그러므로, 대응 표본 검정은 같은 대상에 대한 실험 전후의 결과를 비교할 때 주로 사용된다.\n사례 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Paired.csv\"\nPaired = pd.read_csv(url)\nPaired.head()\n\n\n\n\n\n\n\n\nID\nPretest\nPosttest\n\n\n\n\n0\n1\n80\n82\n\n\n1\n2\n73\n71\n\n\n2\n3\n70\n95\n\n\n3\n4\n60\n69\n\n\n4\n5\n88\n100\n\n\n\n\n\n\n\n박스플롯 시각화 및 기술 통계량 출력.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Pretest와 Posttest에 대한 박스플롯 시각화\nsns.boxplot(data = Paired.iloc[:, [1, 2]], \n            orient = 'h') # 수평 방향\n\n# Pretest와 Posttest의 차이 계산 및 새로운 열(Diff) 추가\nPaired[\"Diff\"] = Paired.Pretest - Paired.Posttest \n                # = 교육 전 성적 - 교육 후 성적\n                # 교육이 효과가 있다면 교육 후 성적이 더 높을 것이므로\n                # 결과적으로는 변수 Diff의 값이 음수로 나와야 한다.\n\n\n\n\n\n\n\n\n두 변수에 대한 상자 그림\n\nPaired.iloc[:,1:4].describe()\n\n# 변수 Diff 평균(mean)이 -7.93이며\n# 실제로 그래프 상에서도 대부분의 개체에서 \n# 변수 Diff의 값이 0보다 작음을 볼 수 있다.\n\n\n# 표준편차(std)는 데이터의 산포도(변동성)를 측정하는 지표로, \n# 데이터가 평균으로부터 얼마나 떨어져 있는지를 나타낸다. \n\n# 표준편차는 항상 0 이상의 값을 가지며, 음수가 될 수 없다. \n# 이는 표준편차가 데이터 값의 차이를 제곱하여 계산하기 때문이다.\n\n\n\n\n\n\n\n\nPretest\nPosttest\nDiff\n\n\n\n\ncount\n15.000000\n15.000000\n15.000000\n\n\nmean\n70.266667\n78.200000\n-7.933333\n\n\nstd\n18.041487\n14.313829\n9.931671\n\n\nmin\n37.000000\n60.000000\n-25.000000\n\n\n25%\n59.500000\n67.000000\n-12.500000\n\n\n50%\n73.000000\n75.000000\n-7.000000\n\n\n75%\n82.000000\n90.500000\n-2.500000\n\n\nmax\n98.000000\n100.000000\n13.000000\n\n\n\n\n\n\n\n히스토그램 및 커널 밀도 추정(KDE) 시각화\n\nsns.distplot(Paired.Diff)\n\n# Seaborn의 최신 버전에서는 더 이상 지원되지 않으므로,\n# sns.histplot 또는 sns.kdeplot을 사용하는 것이 권장된다.\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_19760\\4004193321.py:1: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(Paired.Diff)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 히스토그램 그리기\nsns.histplot(Paired.Diff, \n             stat = 'density')  # y축을 밀도로 변경\n\n# KDE만 수정하기 위해 따로 그리기\nsns.kdeplot(Paired.Diff, \n            fill = True) # 음영 처리\n\nplt.xlim(-40, 30)     # x축 범위 설정\n\n\n\n\n\n\n\n\n양측검정 적용\n\n# ttest_rel에서 rel은 paired 또는 related를 의미한다.\n\n# 이 함수는 대응표본 t-검정을 수행하는 것으로, \n# 두 관련된 표본에 대한 평균의 차이를 비교하는 데 사용된다.\n\n\nfrom scipy.stats import ttest_rel\nttest_rel(Paired.Pretest, Paired.Posttest)\n\n# p-값이 0.0079(0.79%)로 0.05(5%)보다 작기 때문에 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다. \n# 이는 두 표본 간에 유의미한 차이가 있음을 의미한다.\n\nTtestResult(statistic=np.float64(-3.093705670004429), pvalue=np.float64(0.007930923229026533), df=np.int64(14))\n\n\n양측검증을 수행한 뒤, p-value를 2로 나누어 단측 검정을 수행한 것과 동일한 결과를 얻고 있다.\n\nstat, pval = ttest_rel(Paired.Pretest, Paired.Posttest)\nprint(\"one-sided p-value =\", pval/2)\n\n# 이 경우에도, p-값이 0.05보다 작으므로 \n# 귀무가설을 기각하고 대립가설을 채택할 수 있다.\n\none-sided p-value = 0.003965461614513267\n\n\n결론적으로 컴퓨터 교육을 실시하기 전과 후의 성적에 차이가 있으며, 사후 테스트의 결과가 더 좋다고 할 수 있다.\n독립표본에 의한 두 모비율의 비교:\n피셔의 정확검정 Fisher’s Exact Test\n두 모비율에 대한 검정을 수행하기 위해 사용할 수 있는 대표적인 검정법은 두 독립된 이항분포의 비율에 대한 z-검정이다.\n사례: 현 정부에 대한 지지율이 성인 남녀별로 차이가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Support.csv\"\nSupport = pd.read_csv(url)\nSupport.head()\n\n\n\n\n\n\n\n\nID\nGender\nYesNo\n\n\n\n\n0\n1\nMale\nNo\n\n\n1\n2\nFemale\nYes\n\n\n2\n3\nFemale\nNo\n\n\n3\n4\nFemale\nNo\n\n\n4\n5\nFemale\nNo\n\n\n\n\n\n\n\n이 데이터에 대한 2차원 분할표(빈도표) 작성하기.\n\nimport pandas as pd\nSupportTable = pd.crosstab(index = Support[\"Gender\"],\n                           columns = Support[\"YesNo\"])\n\nSupportTable\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n96\n104\n\n\nMale\n140\n110\n\n\n\n\n\n\n\n행 백분율 계산하기.\n\npd.crosstab(index=Support[\"Gender\"], columns=Support[\"YesNo\"],\n           normalize = \"index\") # 각 행의 합을 기준으로 비율을 계산\n\n\n\n\n\n\n\nYesNo\nNo\nYes\n\n\nGender\n\n\n\n\n\n\nFemale\n0.48\n0.52\n\n\nMale\n0.56\n0.44\n\n\n\n\n\n\n\n다음과 같은 교차 테이블(Cross Table)을 만들 수 있다.\n양측검증 적용.\n\nfrom scipy.stats import fisher_exact\nfisher_exact(SupportTable, \n             alternative = 'two-sided')\n             \n# 이 결과는 검정 통계량이 0.725이고 \n# p-값이 0.106(10.6%)이다.\n\n# 이는 일반적으로 사용되는 유의 수준 0.05(5%)에서 \n# 통계적으로 유의하지 않다는 것을 의미한다. \n\n# 결론적으로, 두 그룹(또는 변수) 간에 유의한 차이 또는 \n# 연관성을 찾지 못했다는 것을 나타낸다.\n\nSignificanceResult(statistic=np.float64(0.7252747252747253), pvalue=np.float64(0.10634531219761142))\n\n\n정규 근사 검정\n이항분포의 표본 크기 n이 충분히 크면, 이항분포는 정규분포로 근사할 수 있으며, 이를 정규 근사라고 한다. 일반적으로 n×p와 n×(1 − p)가 모두 5 이상이면, 정규분포로 근사할 수 있다고 간주한다.\n여기서 p 는 성공 확률이다.\n이러한 정규화된 변수를 제곱하면, 이는 카이제곱 분포를 따르게 된다.\n자유도가 1인 카이제곱 분포를 따른다.\n카이제곱검정(Chi-Square Test) 적용.\n\nfrom scipy.stats import chi2_contingency\nchi2_contingency(SupportTable)\n\n# 카이제곱 통계량: 2.54\n# 유의 수준이 일반적으로 0.05(5%)인 경우, \n# p-값이 0.111(11.1%)이므로 귀무가설을 기각할 수 없다.\n\n# 따라서 이 결과는 두 변수 간에 통계적으로 \n# 유의한 연관성이 없다고 결론지을 수 있다. \n\n# 즉, 이 교차표에 따르면 두 변수는 독립적이다.\n\nChi2ContingencyResult(statistic=np.float64(2.5395141968952935), pvalue=np.float64(0.1110289428837834), dof=1, expected_freq=array([[104.88888889,  95.11111111],\n       [131.11111111, 118.88888889]]))\n\n\n결론적으로, 현 정부에 대한 지지율이 성인 남녀별로 차이가 없다고 할 수 있다.\n대응표본에 의한 두 모비율의 비교:\n맥니머 검정\n맥니머 검정은 피셔의 정확검정이나 카이제곱 검정과 달리 대응 표본에 적용할 수 있는 검정이다. 이 검정은 대응 표본 t-검정과 유사하게 교락 효과를 제거하는 것이 중요하다.\n독립 표본의 경우, 한 사람이 A, B 제품 모두를 사용하지 않아도 무방하다. 그러나 대응 표본에서는 한 사람이 반드시 두 제품 모두를 사용해야 한다.\n사례: 정부에서 정책 발표 후 지지율에 변화가 있는가?\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Prepost.csv\"\nPrepost = pd.read_csv(url)\nPrepost.head()\n\n\n\n\n\n\n\n\nID\nPre\nPost\n\n\n\n\n0\n1\nYes\nYes\n\n\n1\n2\nNo\nNo\n\n\n2\n3\nYes\nNo\n\n\n3\n4\nNo\nNo\n\n\n4\n5\nNo\nNo\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nPrepostTable = pd.crosstab(index = Prepost[\"Pre\"], \n                           columns = Prepost[\"Post\"], \n                           margins = True, # 각 행과 열의 합계 추가\n                           margins_name = \"합계\")\nPrepostTable\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n18\n27\n45\n\n\nYes\n8\n67\n75\n\n\n합계\n26\n94\n120\n\n\n\n\n\n\n\n\npd.crosstab(index=Prepost[\"Pre\"], columns=Prepost[\"Post\"], \n            margins=True, margins_name=\"합계\", \n            normalize=\"all\") # 전체 데이터에 대한 비율 변환\n            \n# 정책 발표 이전 지지율(pre): 62.5%\n# 정책 발표 이후 지지율(post): 78.3%\n# 결과적으로 15.8%p가 상승하였음을 볼 수 있다.\n\n\n\n\n\n\n\nPost\nNo\nYes\n합계\n\n\nPre\n\n\n\n\n\n\n\nNo\n0.150000\n0.225000\n0.375\n\n\nYes\n0.066667\n0.558333\n0.625\n\n\n합계\n0.216667\n0.783333\n1.000\n\n\n\n\n\n\n\n\n# pip install statsmodels\n\nfrom statsmodels.stats.contingency_tables import mcnemar\nprint(mcnemar(PrepostTable, \n              exact = True)) # 이항분포 기반의 정확 검정 방법\n              \n# 0.001(0.1%) &lt; 0.05(5%)\n\nprint(mcnemar(PrepostTable, \n              exact=False)) # 카이제곱분포를 사용한 근사 검정 방법\n              \n# 0.002(0.2%) &lt; 0.05(5%)\n\npvalue      0.0018782254774123432\nstatistic   8.0\npvalue      0.0023457869795667934\nstatistic   9.257142857142858\n\n\n결론적으로, 정부에서 정책 발표 전후 지지율에 변화가 있으며, 정책 발표 후에 지지율이 상승한 것으로 볼 수 있다.\n모분산의 동일성에 대한 검정:\nF – 검정 (F–test)\n가장 일반적인 검정 방법으로, 두 집단의 모분산이 동일한지 평가한다. 두 집단의 분산 비율을 계산하고, 이를 기반으로 F–분포를 사용하여 p–값을 구한다.\nReading 데이터의 모분산이 다른가?\n이전에 다루었던 Reading 데이터에 대해 분산의 동일성 검정을 위한 사용자 정의 함수를 작성하고, 가설검정을 수행하였다.\n\nimport pandas as pd\n\n# file_path = os.path.join('data', 'Reading.csv')\n# Reading = pd.read_csv(file_path)\n\nNew = Reading[Reading.Group == 'New']\nOld = Reading[Reading.Group == 'Old']\n\n\nimport numpy as np\nfrom scipy import stats\n\ndef F_test(x, y):\n    f = np.var(x, ddof = 1)/np.var(y, ddof = 1)\n    df1 = x.size -1 \n    df2 = y.size -1 \n    p = 2*(1-stats.f.cdf(f, df1, df2))\n    return f, p\n\nF_test(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\n(1.1454545454545453, np.float64(0.8624138071371459))\n\n\nBartlett’s Test\n\nfrom scipy import stats\nstats.bartlett(New.Score, Old.Score)\n\n# 0.8(80%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nBartlettResult(statistic=np.float64(1.3291852026213666), pvalue=np.float64(0.24895022280539136))\n\n\nLevene’s Test\n\nstats.levene(New.Score, Old.Score)\n\n# 0.6(60%) &gt; 0.05(5%) \n# 귀무가설을 기각할 수 없다.\n\nLeveneResult(statistic=np.float64(0.1978798586572438), pvalue=np.float64(0.6632376240724351))\n\n\n결론적으로, 두 집단의 모분산이 다르다고 말할 수 없다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA08.0.html",
    "href": "ADA/ADA08.0.html",
    "title": "제 8장-카이제곱 통계",
    "section": "",
    "text": "Reporting Date: October. 5, 2024\n질적변수의 분석에 널리 이용되는 다항분포, 카이제곱검정을 소개하고 교차표에 대한 기초적인 분석에 대해 다루고자 한다."
  },
  {
    "objectID": "ADA/ADA08.0.html#데이터의-분류",
    "href": "ADA/ADA08.0.html#데이터의-분류",
    "title": "제 8장-카이제곱 통계",
    "section": "01 데이터의 분류",
    "text": "01 데이터의 분류\n\n1. 양적 데이터\nQuantitative Data\n관찰 대상의 속성을 수치로 측정할 수 있는 데이터로, 덧셈과 뺄셈 등의 산술 연산이 가능하다.\n척도의 성격에 따라 구간 척도와 비율 척도로 구분된다.\n① 구간 척도 (Interval Scale)\n값들 간의 간격이 일정하다는 특성을 지니며, 절대적 영점이 존재하지 않는다.\n따라서 0은 속성의 부재(absence)를 의미하지 않으며, 덧셈과 뺄셈은 가능하지만 곱셈이나 나눗셈을 통한 비율 비교는 무의미하다.\n예: 섭씨(°C)와 화씨(°F) 온도. (0°C는 ’온도가 없음’을 의미하지 않음)\n② 비율 척도 (Ratio Scale)\n구간 척도의 모든 특성을 지니면서, 절대적 영점이 존재한다. 즉, 0이 속성의 완전한 부재를 의미하므로 사칙연산이 모두 가능하며 ‘두 배’, ’절반’과 같은 비율 비교가 가능하다.\n예: 연령, 키, 무게, 길이, 소득, 켈빈(K) 온도.\n\n\n2. 질적 데이터\nQualitative Data\n수치로 양을 측정하기보다는, 관찰 대상의 속성이나 범주를 나타내는 데이터이다.\n이러한 데이터는 주로 구분(classification)이나 서열(ordering)에 초점이 맞추어지며, 척도의 수준에 따라 명목척도와 순서척도로 구분된다.\n① 명목 척도 (Nominal Scale)\n범주 간에 순서가 존재하지 않으며, 단순한 구분만 가능하다. 각 범주는 상호 배타적이며, 수치적 크기나 간격 비교는 불가능하다.\n예: 성별, 출생지, 국적, 혈액형 등.\n명목척도 변수는 주로 카이제곱 검정(χ² test)이나 교차분석을 통해 변수 간의 독립성 또는 연관성을 분석한다.\n② 순서 척도 (Ordinal Scale)\n범주 간에 자연스러운 순서(rank)가 존재하지만, 인접한 범주 간 간격이 동일하다고 볼 수 없다.\n따라서 서열 비교는 가능하나, 차이의 크기를 정량적으로 해석하기는 어렵다.\n예: 만족도, 학점, 성적 등급 등.\n순서척도 변수 간의 연관성은 주로 순위상관계수(Spearman’s ρ, Kendall’s τ)를 이용해 평가할 수 있다."
  },
  {
    "objectID": "ADA/ADA08.0.html#카이제곱-분포",
    "href": "ADA/ADA08.0.html#카이제곱-분포",
    "title": "제 8장-카이제곱 통계",
    "section": "02 카이제곱 분포",
    "text": "02 카이제곱 분포\nChi-Square Distribution, χ²\n서로 독립인 표준 정규분포를 따르는 확률변수를 제곱한 뒤 그 합으로 정의되며, 항상 0 이상의 값을 갖는 확률분포이다. 분포 형태는 일반적으로 오른쪽 꼬리가 긴 비대칭을 보인다.\n\n1 . 자유도(df)\n작을수록 비대칭성이 크며, 평균, 중앙값, 최빈값의 차이가 뚜렷하게 나타난다. 커질수록 분포는 점점 대칭에 가까워지고 정규분포에 근사한다. 분포의 평균은 (df)와 같으며, 분산은 (2 × df)이다.\n\n\n2 . 통계적 활용\n카이제곱 분포는 주로 카이제곱 검정의 기반이 된다.\n예: 적합도 검정(goodness-of-fit test)과 독립성 검정(test of independence)에서, 귀무가설이 참일 경우 검정 통계량이 χ² 분포를 따른다고 가정한다.\n검정은 주로 분포의 오른쪽 꼬리 영역을 기준으로 수행되며, 계산된 통계량이 해당 영역에 속하면 귀무가설을 기각한다.\n따라서 χ² 분포는 데이터가 귀무가설로부터 얼마나 크게 벗어났는지를 평가하는 핵심 도구로 널리 활용된다."
  },
  {
    "objectID": "ADA/ADA08.0.html#카이제곱-적합도-검정",
    "href": "ADA/ADA08.0.html#카이제곱-적합도-검정",
    "title": "제 8장-카이제곱 통계",
    "section": "03 카이제곱 적합도 검정",
    "text": "03 카이제곱 적합도 검정\nChi-Square Goodness of Fit Test 피어슨의 카이제곱 통계량\n단일 범주형 변수에 대해, 실제로 관측된 빈도가 이론적으로 기대되는 빈도와 얼마나 일치하는지를 평가하기 위한 통계적 방법이다.\n즉, 주어진 표본이 특정한 이론적 분포인 균등분포, 정규분포, 이항분포 등을 얼마나 잘 따르는지를 검정하는 데 사용된다.\n이 검정은 동질성 검정이나 독립성 검정과 달리, 두 변수 간의 관계를 비교하는 것이 아니라\n한 변수의 분포 형태 자체가 이론적 가정과 일치하는지를 확인하는 데 초점을 둔다.\n로그 우도비 검정통계량 Log Likelihood Ratio Test Statistic\n우도비(Likelihood Ratio) 는 두 개의 통계 모형의 적합도를 비교하기 위한 지표이다.\n하나는 귀무가설(H₀)​하의 제한된 모형(restricted model), 다른 하나는 대립가설(H₁)하의 포괄적 모형(full model)이다.\n우도비 검정의 핵심은 두 모형의 최대우도(maximum likelihood)를 비교하여 대립가설이 데이터를 더 잘 설명하는지를 평가하는 것이며, 정의는 다음과 같다:\n(L(_0):) 귀무가설 H₀ 하에서의 최대우도 (L():) 대립가설 H₁ 하에서의 최대우도\n로그 우도의 개념 표본이 ({x_1, x_2, , x_n})이고, 모수 () 에 따른 확률밀도(또는 확률질량) 함수가 (f(x|))일 때, 우도 함수는 다음과 같이 정의된다.\n\\[L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta)\\]\n하지만 이 곱은 표본 크기가 커질수록 수치적으로 매우 작아지므로, 계산의 안정성을 위해 로그를 취한다.\n\\[\nl(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n\\]\n이를 로그우도(log-likelihood) 라고 하며, 개별 확률들의 로그값을 모두 더한 형태이다.\n로그 우도비 검정통계량 두 모형의 로그우도를 비교하여 검정통계량 (D)를 정의한다.\n\\[\nD = -2 \\log \\left( \\frac{L(\\theta_0)}{L(\\hat{\\theta})} \\right)\n\\]\n(D)의 값이 클수록 대립가설 하의 모형이 데이터를 더 잘 설명한다는 의미이며, 결과적으로 귀무가설을 기각할 가능성이 높아진다.\n분포 및 해석 표본의 크기가 충분히 크면, Wilks의 정리(Wilks’ Theorem) 에 따라 검정통계량 (D)는 df가 “두 모형의 추정 파라미터 개수 차이”인 카이제곱 분포(χ²) 를 따른다.\n따라서 유의수준 ()를 설정하고, 카이제곱 분포표에서 기준치(critical value)를 찾는다. 만약 계산된 (D)가 기준치보다 크면, 귀무가설(H₀)을 기각한다.\n\n1 . 멘델의 법칙\n멘델의 유전 법칙에 따르면, 특정 형질이 자손에게 전달될 때 일정한 분리비율(segregation ratio) 이 나타난다.\n단일 형질의 경우: 우성 대 열성 = 3 : 1, 두 형질이 동시에 관찰될 경우: 9 : 3 : 3 : 1의 기대비율(expected ratio) 이 예측된다. 그러나 실제 실험에서는 표본의 우연한 변동(sampling variation) 으로 인해 관찰된 비율이 이론적으로 기대되는 비율과 정확히 일치하지 않을 수 있다.\n이때, 관찰된 빈도와 기대빈도의 차이가 단순한 우연에 의한 것인지, 혹은 통계적으로 유의한 차이인지를 판단하는 방법이 바로 카이제곱(χ²) 적합도 검정이다.\n카이제곱 적합도 검정은 관찰빈도와 기대빈도의 차이를 정량적으로 평가하여, 그 차이가 유전법칙에서 제시된 이론적 비율과 통계적으로 일치하는지를 검정한다.\n따라서 이 검정은 멘델의 유전 법칙이 실제 실험 데이터와 부합하는지를 검증하는 핵심 통계적 도구로 사용된다.\n\n\n2 . 적합도 검정의 단계\n카이제곱 적합도 검정은 다음과 같은 절차로 수행된다.\n① 가설 설정 귀무가설(H₀): 관측된 비율은 이론적으로 기대되는 비율과 같다. 대립가설(H₁): 관측된 비율은 기대되는 비율과 다르다. 예: 완두콩의 우성:열성 비율이 3 : 1 이라는 멘델의 가설을 검정한다고 하자.\n② 기대도수 계산 총 표본 수에 각 범주의 기대 비율을 곱하여 기대도수를 구한다.\n예: 400개의 완두콩 샘플에서 3 : 1 비율을 예상한다면\n우성: (400 = 300) 열성: (400 = 100) ③ 카이제곱 통계량 계산 관측도수와 기대도수의 차이를 바탕으로 다음 공식을 사용한다.\n\\[\n\\chi^2 = \\sum \\frac{(관측도수 - 기대도수)^2}{기대도수}\n\\]\n④ 자유도(DF) 계산 자유도는 범주의 수 - 1 로 계산한다.\n예: 두 범주(우성, 열성)가 있을 때\n\\[\nDF = 2 - 1 = 1\n\\]\n⑤ 결과 해석 계산된 χ² 값을 자유도에 따른 임계값(critical value) 과 비교한다. 만약 (χ²_{} &gt; χ²_{})이면, 귀무가설을 기각한다. 반대로, 계산값이 임계값보다 작으면 귀무가설을 기각할 수 없으며, 이는 관측된 비율이 멘델의 법칙(3 : 1 비율)에 부합함을 의미한다.\n04 교차분석 이제 질적 변수 간의 연관성을 시각적·수치적으로 확인할 수 있는 도구인 교차표를 살펴본다.\n\n\n1 . 교차표\nContingency Table\n두 개 이상의 범주형 변수를 기준으로, 각 범주의 조합이 몇 번 나타나는지를 빈도로 정리한 표.\n예: 암의 종류와 성별을 교차하여 각 조합이 몇 번 발생하는지를 나타낼 수 있다.\n일반적으로 행(row)과 열(column)로 구성되며, 각 셀(cell)에는 해당 행과 열의 범주 조합에 해당하는 빈도수가 표시된다.\n차원은 행과 열의 범주 수에 따라 결정되며, (r)개의 범주를 가진 행과 (c)개의 범주를 가진 열이 있을 경우, 표의 크기는 (r×c) 가 된다.\n이러한 교차표는 변수 간 관계를 시각적으로 파악하고, 카이제곱 검정(χ² test)과 같은 통계적 방법을 통해 변수 간 독립성 여부를 평가하는 데 활용된다.\n교차표를 만드는 이유\n교차표는 두 개 이상의 범주형 변수 간 관계를 시각적으로 확인하는 도구이다. 이를 통해 각 범주 조합의 빈도를 정리하고, 변수 간 연관성을 탐색할 수 있다.\n변수 간 관계를 수치적으로 평가하기 위해 카이제곱 검정을 수행할 수 있다.\n카이제곱 검정은 관측 빈도(observed frequency)와 기대 빈도(expected frequency) 간의 차이를 기반으로, 변수 간 독립성 여부를 평가하는 통계적 방법이다.\n따라서 교차표는 단순히 빈도를 정리하는 역할을 넘어, 카이제곱 검정을 통해 변수 간 연관성을 정량적으로 검증하는 중요한 기초 자료로 활용된다.\n카이제곱 검정의 종류\n\n\n1 . 동질성 검정\nTest of Homogeneity\n서로 다른 그룹의 범주형 데이터 분포가 동일한지 여부를 평가하는 통계적 방법이다.\n예: 암 종류별로 성별 분포가 동일한지를 확인할 때 활용할 수 있다.\n가설 설정 귀무가설(H₀): 각 그룹의 분포는 서로 같다. 대립가설(H₁): 각 그룹의 분포는 서로 다르다.\n\n\n2 . 독립성 검정\nTest of Independence\n두 범주형 변수 간에 독립적인 관계가 성립하는지를 평가하는 통계적 방법이다.\n예: 암의 종류와 성별이 서로 독립적인지, 즉 암의 종류가 성별과 관련이 없는지를 확인할 때 활용할 수 있다.\n가설 설정 귀무가설(H₀): 두 변수는 서로 독립적이다. 대립가설(H₁): 두 변수는 서로 독립적이지 않다.\n\n\n3 . 검정 절차\n검정은 카이제곱 통계량을 기반으로 수행되며, 관측 빈도 (O_{ij}) 와 기대 빈도 (E_{ij}) 간의 차이를 비교하여 그룹 간 분포의 동일성을 평가한다.\n기대 빈도는 각 관측치가 행과 열의 비율대로 분포한다고 가정하여 계산된다.\n① 서로 다른 그룹의 범주형 데이터를 수집한다. ② 교차표를 작성한 뒤, 각 셀의 기대 빈도를 계산한다.\n③ 카이제곱 통계량을 다음과 같이 계산한다:\n\\[\n\\chi^2 = \\sum_{i=1}^{r}\\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\n(df = (r-1)(c-1)) ④ 설정한 유의수준 (α) 과 비교하여, 통계량이 임계값보다 크면 귀무가설(H₀)을 기각하고, 두 변수는 독립적이지 않다고 판단한다.\n그렇지 않으면 귀무가설을 채택하여, 두 변수가 독립적이라고 결론짓는다.\n동질성 검정은 독립성 검정과 유사하지만, 독립성 검정은 단일 표본에서 변수 간 관계를 평가하는 반면, 동질성 검정은 서로 다른 집단 간 분포의 동일성을 평가한다는 점에서 차이가 있다.\n관찰도수 Observed Frequency, O\n실제 데이터에서 측정된 빈도를 의미하며, 연구자가 수집한 자료를 기반으로 각 범주에 속하는 사례 수를 나타낸다.\n기대도수 Expected Frequency, E\n두 범주형 변수가 독립적일 경우, 각 셀에 기대되는 빈도를 의미한다.\n이는 관측된 데이터가 아닌 이론적 계산 값으로, 두 변수 간 관계가 없다고 가정할 때 각 범주에 속할 것으로 예상되는 사례 수를 나타낸다.\n기대도수는 다음 공식으로 계산할 수 있다:\n\\[\nE_{ij} = \\frac{(\\text{행 합계}_i) \\times (\\text{열 합계}_j)}{\\text{전체 합계}}\n\\]\n예: 음주 여부와 암 발생 관계 음주 여부와 암 발생 간의 관계를 조사할 때, 교차표의 각 셀은 다음과 같이 구성된다.\n음주자 중 암 발생자 수 음주자 중 암 미발생자 수 비음주자 중 암 발생자 수 비음주자 중 암 미발생자 수 관찰도수와 기대도수는 교차표 작성과 카이제곱 검정 수행에 활용되며, 변수 간 관계를 평가하는 핵심 자료가 된다.\n\n\n4 . 검정의 목적\n검정 통계량인 카이제곱 값(χ²)이 클수록 관측 빈도와 기대 빈도의 차이가 크다는 의미이며, 이는 두 변수 간 관계가 존재할 가능성을 시사한다.\n따라서 χ² 값이 유의미하게 크면 귀무가설(H₀)을 기각하고, 두 변수 간 관계가 있다고 판단한다.\n카이제곱 검정은 독립성 검정과 동질성 검정 모두에서 활용되며, 범주형 데이터의 관계를 분석하는 데 중요한 도구로 사용된다.\n\n\n5 . 충분조건\n카이제곱 검정을 적용하기 위해서는 다음과 같은 조건이 충족되어야 한다:\n① 기대도수\n각 셀의 기대도수는 5 이상이어야 한다. 기대도수가 5 미만인 셀이 전체 셀의 20%를 초과하지 않아야 한다. 이를 충족하지 못하면 검정 결과의 신뢰성이 저하될 수 있다.\n예: 연령대별 정치 선호도를 10점 척도로 분석할 경우, 일부 연령대에서 기대도수가 낮아 기준을 만족하기 어려울 수 있으며, 이 경우 연령을 상·중·하 등으로 그룹화해야 한다.\n② 표본 크기\n표본 크기가 지나치게 작으면 검정력(power)이 낮아져 제2종 오류(Type II error)의 발생 위험이 증가한다.\n따라서 실질적으로 의미 있는 차이를 높은 확률로 탐지하려면 충분히 큰 표본을 확보하는 것이 필요하다.\n③ 셀 간 균형\n범주 간 사례 수가 크게 불균형하면 일부 셀의 기대도수가 낮아지고, 검정 결과가 불안정해질 수 있다.\n조사 설계 단계에서 범주별 사례수를 균등하게 확보하는 것이 권장된다.\n④ 기대도수 부족 시 대처 방법\n범주 통합: 빈도가 낮은 범주를 합쳐 각 셀의 기대도수를 증가시킨다. 범주 재설정: 데이터를 적절히 재범주화하여 기대도수를 확보한다. 기타 범주 생성: 매우 작은 빈도를 가진 범주를 ’기타’와 같이 하나로 묶어 분석한다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA08.2.html",
    "href": "ADA/ADA08.2.html",
    "title": "제 8장-오즈비",
    "section": "",
    "text": "Reporting Date: October. 5, 2024\n오즈비와 연구 설계에 대해 다루고자 한다."
  },
  {
    "objectID": "ADA/ADA08.2.html#오즈비",
    "href": "ADA/ADA08.2.html#오즈비",
    "title": "제 8장-오즈비",
    "section": "01 오즈비",
    "text": "01 오즈비\nOdds Ratio, OR\n앞서 다룬 카이제곱 검정과 로그 우도비 검정은 범주형 변수 간의 통계적 유의성을 판단하는 데 초점을 두었다.\n본 장에서는 그 관계의 크기(효과 크기) 를 수치화하는 지표인 오즈비(OR) 를 다룬다.\n두 집단 간 사건 발생 가능성의 상대적 비교를 제공하며, 특히 2×2 분할표 분석과 로지스틱 회귀분석에서 널리 사용된다. 위험비(Risk Ratio)와 유사하지만, 사건이 발생할 확률 대비 발생하지 않을 확률(odds)의 비율을 사용한다.\n\n1 . 정의 및 기본 공식\n두 집단(노출 vs 비노출)에 대해 사건(또는 성공)의 발생 여부를 2×2 분할표로 나타낼 수 있다.\n사건 발생 (Yes) 사건 미발생 (No) 노출 그룹 (Exposed) a b 비노출 그룹 (Non-exposed) c d 각 집단의 오즈(Odds)는 사건 발생 확률을 사건 미발생 확률로 나눈 값이다.\n노출 집단의 오즈: ()​ 비노출 집단의 오즈: ()​ 따라서, 오즈비는 두 오즈의 비율로 정의된다.\n\\[\n\\mathrm{OR} = \\frac{a/b}{c/d} = \\frac{a \\cdot d}{b \\cdot c}\n\\]\n해석 규칙 ( &gt; 1:) 노출이 사건 발생 확률을 증가시킴(위험 증가) ((OR&lt;1:) 노출이 사건 발생 확률을 감소시킴(보호 효과) ( = 1:) 노출과 사건 발생 간 차이 없음\n\n\n2 . 로지스틱 회귀에서의 의미\n로지스틱 회귀모형에서 종속변수가 이진일 때, 회귀계수 ()는 로그 오즈에 대한 영향력을 나타낸다.\n\\[\n\\log\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots\n\\]\n따라서 변수 (X_j)가 1 단위 증가할 때 오즈는 지수함수적으로 변하며, 그 비율이 바로 오즈비이다.\n\\[\n\\mathrm{OR} = e^{\\beta_j}\n\\]\n(e^{_j})​가 1 보다 크면 해당 변수의 증가가 사건의 오즈를 e^{_j}배로 증가시킨다.\n예: (_j = 0.69)이면 (e^{0.69})으로, 오즈가 2배로 증가함을 의미.\n주의: 로지스틱 회귀에서 보고되는 오즈비는 조건부(다른 공변량을 고정한 상태) 효과이며, 단변량 오즈비와 다를 수 있다.\n\n\n3 . 비교 연구에서의 활용 사례\n비교연구(예: 케이스-컨트롤, 코호트 연구)에서는 오즈비가 두 집단 간 상대적 위험을 평가하는 주요 지표로 사용된다.\n예: 흡연(노출)과 폐암(사건)의 관계를 조사할 때\n각 그룹의 사건·비사건 수를 집계하여 오즈를 계산한다. 오즈비로 비교하여 흡연과 폐암의 연관성 크기를 정량화한다. ( &gt; 1)이면 흡연자가 비흡연자보다 폐암 발생 오즈가 더 높음.\n코호트 연구에서는 비율(RR)과 오즈비가 다를 수 있음에 유의한다.\n사건 발생률이 충분히 낮을 때((&lt;10%)범위)에는 OR ≈ RR이지만, 발생률이 높을 경우 OR은 RR보다 과장된 효과를 나타낼 수 있다.\n\n\n4 . 수학적 범위와 해석상의 주의점\n오즈비의 값 범위: (0 &lt; &lt; )\n(=0:) 실무적으로는 사건이 전혀 발생하지 않는 경우(이론적 경계) ( :) 사건이 항상 발생하는 경우(이론적 경계) 중심값은 1이며, 1에서 멀어질수록 효과 크기가 큼을 의미한다.\n범주 순서에 따른 영향 분할표에서 기준 집단을 바꾸면 오즈비는 역수((1/))가 된다. 이는 관계의 방향성만 달라질 뿐, 강도 자체는 동일하다.\n노출 집단을 기준으로 계산한 OR이 3이라 가정 시, 비노출 집단을 기준으로 계산한 OR은 ()​이다. 따라서 결과 해석 시 기준 집단(레퍼런스)을 명확히 기술해야 한다.\n\n\n5 . 적용 범위의 일반화\n오즈비는 이진 결과 전반 (사건/비사건, 성공/실패, 긍정/부정)에 적용 가능하다.\n로지스틱 회귀뿐 아니라 교차표 분석, 환자군 비교, 약물효과 평가 등에서 표준화된 비교 지표로 활용되며,\n특정 집단이 다른 집단보다 성공할 오즈가 얼마나 큰가(혹은 작은가)가 주 관심사이다."
  },
  {
    "objectID": "ADA/ADA08.2.html#코호트-연구",
    "href": "ADA/ADA08.2.html#코호트-연구",
    "title": "제 8장-오즈비",
    "section": "02 코호트 연구",
    "text": "02 코호트 연구\nCohort Study; [전향적 연구]\n두 집단(예: 흡연자 vs 비흡연자)을 시간에 따라 추적하여 사건 발생(예: 질병) 여부를 비교하는 연구 설계이다.\n장점 신뢰성: 후향적 연구보다 자료의 신뢰성이 높다. 높은 정확도: 시간에 걸쳐 직접 데이터를 수집하므로 관찰 오차가 적다. 단점 긴 추적 시간: 연구 기간이 길고 참여자 추적이 어려울 수 있다. 선정 바이어스(Selection Bias): 특정 집단이 과대 또는 과소 대표될 수 있음 (예: 비흡연자가 모집 과정에서 과소 대표되는 경우, 결과 해석 시 주의 필요.)\n\n1 . 비율 차이 및 비교 지표\n코호트 연구에서 두 집단 간 사건 발생을 비교할 때 대표 지표는 다음과 같다.\n① 상대 위험비(RR, Relative Risk)\n두 집단의 사건 발생 확률을 직접 비교한 비율 사건 발생 확률을 직접 사용하므로 직관적 해석이 용이하다.\n\\[\n\\mathrm{RR} = \\frac{P(\\text{사건 | 노출})}{P(\\text{사건 | 비노출})}\n\\]\n예: 흡연자의 폐암 발생 확률 ÷ 비흡연자의 폐암 발생 확률\n② 오즈비(OR, Odds Ratio)\n사건 발생 오즈를 비교한 비율 상대 비율보다 큰 값이 나올 수 있으며, 사건 발생률이 높을수록 RR과 차이가 커진다.\n\\[\n\\mathrm{OR} = \\frac{a/b}{c/d}\n\\]​\n2×2 분할표 분석과 로지스틱 회귀에서 상대적 위험을 정량화할 때 유용하다.\n③ 상대 위험비와 오즈비의 비교\n지표 의미 특징 상대 비율(RR) 실제 사건 발생 비율 비교 사건 발생 확률을 직접 사용, 직관적 오즈비(OR) 사건 발생 오즈 비율 비교 상대 비율보다 큰 값이 나올 수 있음; 사건 발생 확률이 높을수록 RR과 차이 확대 코호트 연구에서는 RR과 OR을 함께 고려해야 연구 결과를 정확히 해석할 수 있다. 특히 로지스틱 회귀에서 계산되는 OR은 조정된(조건부) 상대 위험으로, 단순 RR과 다를 수 있음을 이해해야 한다.\n\n\n2 . 연구 활용 및 해석\n코호트 연구에서 흡연과 폐암 관계를 분석할 때, OR과 RR을 함께 고려하면 연구 결과를 보다 종합적으로 이해할 수 있다.\n연구 설계 및 분석 과정에서 오즈비를 적절히 해석하는 것이 중요하며, 상대 비율과 비율 차이와 함께 결과를 해석해야 정확한 결론을 도출할 수 있다."
  },
  {
    "objectID": "ADA/ADA08.2.html#사례-대조-연구",
    "href": "ADA/ADA08.2.html#사례-대조-연구",
    "title": "제 8장-오즈비",
    "section": "03 사례 대조 연구",
    "text": "03 사례 대조 연구\nCase-Control Study; [후향적 연구]\n이미 질병이 발생한 집단(사례군)과 그렇지 않은 집단(대조군)을 비교하여, 과거 노출 상태와 질병 발생 간의 관계를 분석한다.\n\n1 . 집단 정의\n구분 설명 예시 사례군 (Case Group) 연구에서 질병이 발생한 사람들 폐암 환자 대조군 (Control Group) 질병이 없는 사람들 폐암에 걸리지 않은 일반인 연구에서는 이들 집단의 과거 노출 상태(예: 흡연 여부)를 조사하여, 질병과 노출 간의 연관성을 평가한다.\n장점 과거 노출 상태를 조사하므로 전향적 연구보다 연구 기간이 짧음 사례군을 충분히 확보할 수 있어 효율적(희귀 질병 연구에 적합) 단점 후향적 설계 특성상 상대 위험도(RR) 산출 불가 선택 편의(Selection Bias): 사례군과 대조군 선정 방식에 따라 결과가 달라질 수 있음 회상 오차(Recall Bias): 참여자가 과거 행동을 정확히 기억하지 못해 정보가 왜곡될 수 있음\n\n\n2 . 오즈비 활용\n사례 대조 연구는 사건 발생률이 불명확하므로, 상대 위험비(RR) 대신 오즈비(OR)를 사용하여 두 집단 간 노출 상태를 비교하고 상대적 위험을 추정한다.\n2×2 분할표 예시 흡연 비흡연 사례군 (폐암 환자) a b 대조군 (일반인) c d\n\n\n3 . 연구 활용 및 해석\n사례 대조 연구는 질병 발생 원인과 노출 요인 간 관계 분석에 적합하며, 데이터 수집 시 선택 편의와 회상 오차를 최소화하는 절차 필요하다.\n오즈비를 통한 상대적 위험 추정이 핵심이며, 해석 시 코호트 연구의 RR과 달리 사건 발생률을 직접 반영하지 않음을 명확히 해야 한다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA09.1.html",
    "href": "ADA/ADA09.1.html",
    "title": "제 9장-다중 회귀",
    "section": "",
    "text": "Reporting Date: September. 07, 2025 다중 회귀분석에 대해 다루고자 한다."
  },
  {
    "objectID": "ADA/ADA09.1.html#다중-회귀분석",
    "href": "ADA/ADA09.1.html#다중-회귀분석",
    "title": "제 9장-다중 회귀",
    "section": "01 다중 회귀분석",
    "text": "01 다중 회귀분석\n현실의 사회적 또는 자연적 현상을 설명할 때, 반응 변수 (y) 의 변화를 단일 설명 변수만으로 충분히 설명할 수 있는 경우는 거의 없다.\n따라서 여러 개의 설명 변수를 적절히 선택하고, 이들의 함수로 반응 변수를 설명하면 보다 정확한 예측과 분석이 가능하다.\n이러한 상황을 다루기 위해 사용하는 방법이 다중 선형 회귀 모형이며, 일반적인 수식으로는 다음과 같이 표현된다:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\n\\]\n여기서 추정해야 할 모수(parameters)는 p + 1개의 회귀계수 _0, _1, , _p​이며, _i​는 오차항을 나타낸다.\n\n1 . 오차항 가정\n회귀분석의 타당성을 위해 오차항 ε 은 다음과 같은 가정을 만족해야 한다:\n\\[\n{E}[\\varepsilon] = 0, \\quad \\mathrm{Var}(\\varepsilon) = \\sigma^2\n\\]\n기댓값은 0이며, 분산은 일정(등분산성) 서로 다른 관측치의 오차항은 서로 독립적 반응 변수 y의 전체 변동은 회귀모형과 오차항으로 분해 가능 이러한 변동 분해를 제곱합의 분할이라고 하며, 식으로 표현하면 다음과 같다:\n\\[\n\\text{SST} = \\text{SSR} + \\text{SSE}\n\\]\n\n\n1 . 전체제곱합\nTSS, SST, Total Sum of Squares\n반응변수 y 의 총 변동을 나타낸다.\n이는 각 관측값이 전체 평균으로부터 얼마나 떨어져 있는지를 제곱하여 모두 합한 값이다. 따라서 y 의 전체 변동량을 측정하는 지표라고 할 수 있다. 전체제곱합의 자유도는 n − 1 이다.\n\n\n2 . 회귀제곱합\nSSR, Sum of Squares due to Regression SSM, Sum of Squares due to Model\n회귀모형에 포함된 설명변수들이 반응변수의 변동을 얼마나 설명하는지를 나타낸다. 즉, 평균으로부터의 변동 중에서 회귀모형이 설명할 수 있는 부분이다. 회귀제곱합의 자유도는 설명변수의 개수 p 이다.\n\n\n3 . 잔차제곱합\nSSE, Sum of Squares due to Error\n회귀모형이 설명하지 못하는 변동으로, 실제값과 회귀모형이 예측한 값 간의 차이를 제곱하여 합한 값이다.\n즉, 오차항에 의한 변동을 의미한다. 잔차제곱합의 자유도는 n − p − 1 이다.\n총제곱합(SST, Total Sum of Squares): 반응 변수 yyy의 전체 변동량을 나타내며, 각 관측값이 평균으로부터 얼마나 떨어져 있는지를 제곱하여 합한 값입니다. 자유도는 n−1n-1n−1입니다. 회귀제곱합(SSR, Sum of Squares due to Regression / SSM, Sum of Squares due to Model): 회귀모형이 반응 변수 변동 중 얼마를 설명할 수 있는지를 나타냅니다. 자유도는 설명 변수의 개수 p이다. 잔차제곱합(SSE, Sum of Squares due to Error): 회귀모형으로 설명되지 않는 변동으로, 실제값과 예측값 간의 차이를 제곱하여 합한 값입니다. 자유도는 n−p−1n - p - 1n−p−1입니다. 즉, 반응 변수 y의 총 편차는 회귀모형에 의해 설명된 편차와 오차항에 의한 편차로 나뉘게 되며, 이는 다음과 같이 해석할 수 있다:\n\\[\n\\underbrace{\\text{총 편차}}_{\\text{SST}} = \\underbrace{\\text{회귀로 설명된 편차}}_{\\text{SSR}} + \\underbrace{\\text{잔차}}_{\\text{SSE}}\n\\]\n또한 각 제곱합을 자유도로 나눈 값을 평균제곱합(Mean Squares, MS)이라 하고, 이 값들을 요약한 표를 분산분석표(ANOVA table)라고 한다.\nANOVA 테이블 변동 요인(Source) 제곱합(SS) 자유도(df) 평균제곱합(MS) F-값 회귀(Regression, 모형) SSR p = k-1 MSR = ​ F = ​ 오차(Error, 잔차) SSE n − p − 1 MSE = ​\n전체(Total) TSS n − 1\nF 통계량은 회귀모형이나 분산분석에서 모형이 설명하는 변동(설명된 변동)과 잔차 변동(설명되지 않은 변동)을 비교하여, 모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표이다. 값이 클수록 모형의 설명력이 상대적으로 높다고 해석할 수 있다.\n이는 전체 변동 중 모형이 설명하는 비율인 결정계수와도 연결된다. 일반적으로 1에 가까울수록 모형이 데이터를 잘 설명한다고 본다.\n다만, 단순히 F 통계량이나 R2 값의 크기만으로는 모형의 유의성을 단정할 수 없다. 자유도와 표본 크기, 변동의 구조에 따라 “얼마나 크면 충분히 큰가”라는 절대적 기준이 달라지기 때문이다.\n따라서 F 통계량과 함께 p-값을 확인하는 것이 필수적이다. p-값이 유의수준(예: 0.05)보다 작으면, “모든 집단의 평균이 같다” 혹은 “회귀계수가 모두 0이다” 라는 귀무가설을 기각할 수 있으며, 모형이 통계적으로 유의함을 의미한다.\n반대로 p-값이 크면 귀무가설을 기각할 증거가 부족하다는 뜻이다. 이 경우 반드시 모형을 “처음부터 다시” 만들어야 한다는 것은 아니며, 변수 선택, 데이터 수집, 가정 검토 등의 추가 작업이 필요할 수 있다.\n다중회귀분석에서는 p 개의 설명변수가 반응변수 y 를 설명하는 데 유의하게 기여하는지를 확인할 필요가 있다.\n이를 위해 귀무가설 H₀ : β₁ = β₂ = ⋯ = βp = 0 를 세우고, 다음과 같은 검정통계량을 사용한다:\n이 검정통계량은 귀무가설이 참일 때 자유도 (p, n − p − 1)를 갖는 F − 분포를 따른다.\n일원 분산분석 One-way ANOVA table\n다중 회귀분석의 ANOVA 표와 일원 분산분석(One-way ANOVA) 표는 기본적으로 동일한 통계적 원리 위에서 구성된다.\n두 분석 모두 전체 변동(총제곱합, SST)을 설명 가능한 부분과 설명 불가능한 부분으로 분해하고, 이를 토대로 F-검정을 수행하여 모형 또는 집단 간 차이가 유의한지 판단한다는 점에서 연결된다.\n다중 회귀에서는 총제곱합을 회귀제곱합(SSR)과 잔차제곱합(SSE)으로 분해하며, 이는 설명변수가 종속변수를 얼마나 잘 설명하는지 평가하는 과정이다.\n일원 분산분석에서는 총제곱합을 집단 간 제곱합(SSB)과 집단 내 제곱합(SSE)으로 분해하는데,\n이는 집단 평균 간 차이가 통계적으로 유의한지 검정하는 과정이다. 이러한 연결점은 일원 분산분석이 다중 회귀분석의 특수한 형태라는 점에서 명확해진다.\n즉, 범주형 독립변수(집단)를 더미 변수로 변환하여 회귀 모형에 포함하면, 일원 분산분석은 다중 회귀분석으로 표현될 수 있다.\n따라서 두 분석을 나란히 이해하는 것은 회귀분석과 분산분석의 수학적·논리적 일관성을 보여주며, 분석자가 다양한 데이터 구조를 동일한 틀에서 해석할 수 있도록 돕는다.\n그럼에도 불구하고 두 분석은 활용 목적과 해석에서 차이를 가진다.\n다중 회귀분석은 여러 개의 독립변수가 종속변수에 미치는 개별적·동시적 효과를 추정하고, 각 회귀계수에 대한 유의성 검정을 통해 변수가 기여하는 정도를 해석한다.\n반면, 일원 분산분석은 주로 집단 평균 차이라는 단일한 질문에 초점을 두며, 개별 집단 간 차이를 사후검정(Post-hoc test)을 통해 추가적으로 분석한다.\n결과적으로 회귀분석의 ANOVA 표는 모형 전체의 설명력을 평가하는 과정에서 필요하고, 일원 ANOVA 표는 집단 간 평균 차이가 존재하는지를 평가하는 과정에서 필요하다.\n즉, 두 방법론은 수학적 구조를 공유하지만, 회귀는 예측과 변수 효과 추정에 강점이 있고, ANOVA는 집단 비교에 특화되어 있다는 점에서 차별화된다.\n따라서 연구자가 어떤 질문을 던지는지에 따라 적절한 분석 방법을 선택해야 하는 것이 중요하다.\n변동 요인(Source) 제곱합(SS) 자유도(df) 평균제곱합(MS) F-값 Between Groups (SSB, 집단 간) SSB k − 1 MSB = SSB / (k − 1) F = MSB / MSE Within Groups (SSE, 집단 내) SSE N − k MSE = SSE / (N − k) Total (SST) SST N − 1\n다중 회귀분석 사례 700명의 고객을 대상으로 어떤 제품에 대해 조사하려 얻은 것이다.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satisfaction.csv\"\nSatisfaction = pd.read_csv(url)\nSatisfaction.head()\n\n\n\n\n\n\n\n\nID\nY\nX1\nX2\nX3\nX4\nGender\nAge\nGender1\nAge1\nAge2\nAge3\nAge4\n\n\n\n\n0\n1\n5\n5\n5\n6\n5\n1\n3\n1\n0\n0\n1\n0\n\n\n1\n2\n5\n5\n5\n5\n5\n1\n5\n1\n0\n0\n0\n0\n\n\n2\n3\n5\n5\n6\n5\n5\n2\n5\n0\n0\n0\n0\n0\n\n\n3\n4\n5\n6\n6\n5\n6\n1\n2\n1\n0\n1\n0\n0\n\n\n4\n5\n5\n5\n6\n5\n5\n1\n5\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nX1: 다자인 만족도 X2: 사용 편리성 만족도 X3: 성능 만족도 X4: 고장 및 견고성 만족도 Y: 구입 의향\n설문 데이터 분석에서의 접근 설문을 통해 얻은 만족도 및 구입 의향 데이터는 보통 순위형(ordinal) 데이터로 수집된다.\n예를 들어 ” 매우 만족 ~ 매우 불만족 ” 과 같은 리커트 척도는 순서가 존재하지만, 각 항목 간 간격이 동일하다고 보장되지 않는 특성이 있다.\n가전제품과 같은 대기업의 고객 만족 조사에서는 위과 같은 변수를 설정할 수 있다.\n이때 연구의 초점은 Y(구입 의향)을 종속변수로 두고, 네 가지 만족도 요인이 이에 얼마나 영향을 미치는지를 분석하는 것이다.\n분석 절차는 보통 다음과 같이 진행된다.\n① 상관계수 확인 각 독립변수(X1 ~ X4)와 종속변수(Y) 간의 관계를 파악한다. 데이터가 순위형일 경우 피어슨 상관계수보다는 스피어만 상관계수 같은 방법이 더 적절할 수 있다.\n\n# 피어슨의 상관계수\nSatisfaction.iloc[:, 1:6].corr()\n\nfrom scipy.stats import pearsonr\n\npearsonr(Satisfaction.Y, Satisfaction.X1)\npearsonr(Satisfaction.Y, Satisfaction.X2)\npearsonr(Satisfaction.Y, Satisfaction.X3)\npearsonr(Satisfaction.Y, Satisfaction.X4)\n\nPearsonRResult(statistic=np.float64(0.20687830951579425), pvalue=np.float64(3.321873797880896e-08))\n\n\n\n② 결정계수 확인\n단순 상관관계에서는 상관계수 r 의 제곱값 r² 이 결정계수로 해석될 수 있으며, 이는 종속변수의 변동 중 독립변수가 설명하는 비율을 의미한다.\n여러 독립변수가 있을 경우에는 다중회귀분석을 통해 전체 결정계수 R² 를 산출한다.\n\nimport statsmodels.formula.api as smf\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction).fit()\nprint(SatisfactionFit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.141\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     28.56\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           5.38e-22\nTime:                        17:45:44   Log-Likelihood:                -1174.4\nNo. Observations:                 700   AIC:                             2359.\nDf Residuals:                     695   BIC:                             2382.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.2918      0.347      3.719      0.000       0.610       1.974\nX1             0.2352      0.058      4.077      0.000       0.122       0.349\nX2             0.1666      0.066      2.543      0.011       0.038       0.295\nX3             0.2309      0.068      3.421      0.001       0.098       0.363\nX4             0.0585      0.050      1.168      0.243      -0.040       0.157\n==============================================================================\nOmnibus:                      147.199   Durbin-Watson:                   1.804\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              482.307\nSkew:                           0.987   Prob(JB):                    1.86e-105\nKurtosis:                       6.556   Cond. No.                         75.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n회귀분석 결과에서는 결정계수 R² = 0.141로, 전체 독립변수들이 구입 의향(Y)의 변동 중 약 14.1%만 설명하고 있다. 즉, 설명력이 높지는 않지만 전혀 의미 없는 수준도 아니다.\n전체 모형의 F 통계량 (F ≈ 28.56, p &lt; 0.001)은 “모형이 유의미하다”는 것을 보여준다.\n— 무작위로 요인들을 넣은 것보다, 적어도 일부 독립변수가 종속변수에 영향을 준다는 증거가 있음.\n개별 계수 검정에서는 X1, X2, X3이 유의하였고(X1, X3 특히 강함), X4는 유의하지 않음 (p ≈ 0.243)\n— 현재 모형 하에서는 고장·견고성 만족도가 구입 의향에 통계적으로 유의한 영향을 미친다고 보기 어렵다.\n결정계수가 낮은 원인으로는 가격 요소 등 중요한 변수가 빠져 있을 가능성, 또는 구입 의향과 관련된 여타 요인(브랜드 신뢰도, 마케팅, 사회적 영향 등)을 포함하지 않았을 가능성 등을 고려해야 한다.\n\n\n③ 표준화 회귀계수\n변수들이 서로 다른 단위를 가지고 있으므로, 회귀분석에서 비표준화 회귀계수만 보면 어떤 변수가 Y(구입 의향)에 더 큰 영향을 주는지 절대비교가 어렵다.\n따라서 표준화 회귀계수를 사용하면, 각 변수의 변화가 “자신의 표준편차 단위 하나 증가”했을 때 Y가 얼마나 표준편차 단위로 바뀌는지를 보여주므로, 변수들 간의 상대적 영향력 비교가 가능해진다.\n\n# 표준화 회귀계수\nfrom scipy import stats\nSatisfaction_z = Satisfaction.iloc[:,1:6].apply(stats.zscore)\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction_z).fit()\nSatisfactionFit.params.round(5)\n\nIntercept    0.00000\nX1           0.16435\nX2           0.11285\nX3           0.15935\nX4           0.04635\ndtype: float64\n\n\n예를 들어, 표준화 계수가 라면, X1과 X3이 구입 의향에 가장 큰 영향을 미치고, X4는 가장 영향이 작다는 해석이 가능하다.\n다만 표준화하면 절편은 일반적으로 의미가 0 근처가 되며, 변수의 원 단위 변화에 대한 실질적인 의미(예: 점수 1점의 변화)가 사라지므로, 연구 목적이 “영향력의 비교”인지 “실질적 예측”인지에 따라 비표준화 계수도 함께 살펴야 한다.\n편회귀계수(partial regression coefficient)\n다중회귀분석에서 각 독립변수가 종속변수에 미치는 순수한 영향력을 평가하기 위해 사용된다. 이는 다른 독립변수들의 영향을 고정한 상태에서 특정 독립변수의 기여도를 나타낸다.\n회귀계수는 모든 독립변수의 영향을 고려한 상태에서 특정 독립변수의 영향을 나타내며, 편회귀계수는 이러한 회귀계수의 일종으로 볼 수 있다.\n따라서 회귀계수와 유사하나 동일하지 않으며, 다중회귀분석에서 각 독립변수의 개별적인 영향을 평가할 때 중요한 지표로 활용된다."
  },
  {
    "objectID": "ADA/ADA09.1.html#변수-선택",
    "href": "ADA/ADA09.1.html#변수-선택",
    "title": "제 9장-다중 회귀",
    "section": "02 변수 선택",
    "text": "02 변수 선택\n풀모델(완전모델)은 모든 독립변수를 포함하여 종속변수에 영향을 줄 수 있는 요인을 전부 반영한 모델이다.\n그러나 실제로는 변수 과다로 인해 과적합이 발생할 수 있으므로, 변수를 일부만 선택하여 축소모델을 사용하는 것이 바람직하다. 이러한 과정이 바로 변수선택법이다.\n\nimport statsmodels.formula.api as smf\nSatisfactionFit1 = smf.ols(formula='Y~X1+X2+X3',\n                          data=Satisfaction).fit()\nprint(SatisfactionFit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.139\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     37.61\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           1.56e-22\nTime:                        17:45:44   Log-Likelihood:                -1175.1\nNo. Observations:                 700   AIC:                             2358.\nDf Residuals:                     696   BIC:                             2376.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.4020      0.334      4.193      0.000       0.746       2.059\nX1             0.2436      0.057      4.254      0.000       0.131       0.356\nX2             0.1763      0.065      2.712      0.007       0.049       0.304\nX3             0.2504      0.065      3.829      0.000       0.122       0.379\n==============================================================================\nOmnibus:                      140.914   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              452.399\nSkew:                           0.951   Prob(JB):                     5.79e-99\nKurtosis:                       6.449   Cond. No.                         62.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nX4를 제외하고 회귀분석을 다시 수행한 결과, 결정계수(R²)는 0.141에서 0.139로 약간 감소했다.\n이는 X4를 제거함으로써 모델의 전체 설명력이 조금 줄었음을 의미하며, X4가 종속변수에 미치는 직접적 영향은 미미하지만 다른 변수와의 상호작용 등을 통해 간접적 기여가 있을 수 있음을 시사한다.\nF-통계량은 37.61로 이전보다 증가했고, P-value는 매우 작아(1.56e-22) 모델 전체는 여전히 통계적으로 유의하다.\n개별 회귀계수 검정에서는 X1, X2, X3는 유의하지만, X4는 유의하지 않아 실질적인 영향력은 거의 없다고 볼 수 있다.\n변수선택법에는 크게 세 가지가 있다.\n① 전진선택법(Forward Selection) 가장 설명력이 높은 변수를 하나씩 추가하며, 선택된 변수는 이후 단계에서 제거되지 않는다.\n② 후진제거법(Backward Elimination) 모든 변수를 포함한 상태에서, 의미 없는 변수를 하나씩 제거하며, 제거 기준은 보통 p-value를 사용한다.\n③ 단계적 방법(Stepwise Selection) 전진선택법과 후진제거법을 혼합하여, 변수를 추가하면서 동시에 불필요한 변수가 없는지 검토하는 방식이다.\n모형 선택 시 결정계수(R²)만으로 평가해서는 안 되며, 조정된 결정계수(Adjusted R²), 평균제곱오차(MSE), AIC, BIC, 멜로우 C 등 여러 지표를 종합적으로 고려해야 한다.\n이러한 과정을 거친 후, 잔차 분석 등을 통해 모델 가정을 확인하는 것이 옳다.\n회귀분석에서 변수 선택 기능의 지원 정도 비교\nSAS PROC REG와 같은 절차를 통해 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다. 예를 들어, SELECTION=STEPWISE 옵션을 사용하여 단계적 선택법을 수행할 수 있다.\nR step() 함수를 사용하여 전진 선택법, 후진 제거법, 단계적 선택법을 수행할 수 있다. 이 함수는 AIC를 기준으로 변수 선택을 수행한다.\npython mlxtend: SequentialFeatureSelector를 제공하여 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.\nabess: 고속 최적 부분집합 선택을 위한 라이브러리로, 선형 회귀 및 분류 문제에 적용할 수 있다."
  },
  {
    "objectID": "ADA/ADA09.1.html#가변수",
    "href": "ADA/ADA09.1.html#가변수",
    "title": "제 9장-다중 회귀",
    "section": "04 가변수",
    "text": "04 가변수\nDummy Variable\n범주형 변수는 회귀 분석에 직접 사용할 수 없으므로, 이를 수치형 변수로 변환해야 한다.\n일반적으로 k 개의 범주를 가진 변수는 k - 1 개의 더미 변수로 변환하여 사용한다.\n이때, k - 1 개의 더미 변수 중 하나는 기준(reference) 범주로 설정되며, 나머지 범주들은 이 기준 범주와의 차이를 나타내는 변수로 해석된다.\n나이별 사는 지역 변수 처리\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Dummy.csv\"\nDummy = pd.read_csv(url)\nDummy.head()\n\n\n\n\n\n\n\n\nID\nY\nAge\nRegion\n\n\n\n\n0\n1\n46\n21\n1\n\n\n1\n2\n39\n21\n3\n\n\n2\n3\n62\n21\n3\n\n\n3\n4\n38\n21\n2\n\n\n4\n5\n39\n21\n3\n\n\n\n\n\n\n\n주어진 데이터에서 ‘Region’ 변수는 3개의 범주를 가집니다:\nRegion 1 Region 2 Region 3 이때, Region 3이 기준 범주(reference category)가 되며, 회귀식은 다음과 같이 표현된다:\n여기서:\nβ₁: Region 1에 해당하는 경우, Region 3과 비교하여 Y값의 차이 β₂: Region 2에 해당하는 경우, Region 3과 비교하여 Y값의 차이 β₀: Region 3에 해당하는 경우의 Y값 (기준 범주) 따라서, 회귀 계수 β₁과 β₂는 각각 Region 1와 Region 2이 Region 3에 비해 Y에 미치는 영향을 나타낸다.\nimport pandas as pd Dummy[‘D1’] = 0 Dummy.loc[Dummy[‘Region’]==1, ‘D1’]=1 Dummy[‘D2’] = 0 Dummy.loc[Dummy[‘Region’]==2, ‘D2’]=1 Dummy.head()\nimport statsmodels.formula.api as smf DummyFit = smf.ols(formula=‘Y~Age+D1+D2’, data=Dummy).fit() print(DummyFit.summary())\nimport statsmodels.formula.api as smf DummyFit1 = smf.ols(formula=‘Y~Age+C(Region)’, data=Dummy).fit() print(DummyFit1.summary())\nimport statsmodels.api as sm sm.stats.anova_lm(DummyFit1, typ=3)\n05 변수변환과 비선형 회귀분석 선형되지 않을 때 선형으로 바꿔주는 것. 예를 들면 로지스틱 회귀.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA10.1.html",
    "href": "ADA/ADA10.1.html",
    "title": "제 10장 분산분석: 일원분류 실습",
    "section": "",
    "text": "Reporting Date: August. 5, 2024 일원분류 분산분석에 대해 다루고자 한다.\n\n01 제목1 [ID 사용하기]\n\n\n1 . 제목2\n본문2\n4종류의 비료에 대해 생산량에 차이가 있는가?\nimport os import pandas as pd\n\n\n상대 경로 설정\nfile_path = os.path.join(‘data’, ‘Harvest.csv’)\n\n\nCSV 파일 읽기\nHarvest = pd.read_csv(file_path)\n\n\n데이터 확인\nHarvest.head(5)\n4종류의 비료가 농작물의 평균 수확량에 차이를 주는지 확인하기 위해, 총 16개의 경작지에서 실험을 진행하였다. 실험은 실험 설계의 기본 원리 3가지를 지켜 진행되었다. 무작위로 배정되었으며, 수확량 데이터는 비료 종류별로 (예: F1) 정리되었다. 분석을 위해서는 각 변수별로 정리된 데이터가 필요하며, 분석에 사용될 데이터는 표 형식으로 정리하지 않아도 분석이 가능하다.\nF1과 F2만 존재했다면 이것은 t테스트로 하는 게 맞다. 그룹 변수에 따라서 수확량의 차이가 난다. 그룹 간의 차이가 날 것이다 라고 추측하고 들어갈 수 있다. 항상 기초 통계량을 보고서 근거를 들 수 있다.\ny는 범주형 변수로 놓아서 하는 게 로지스틱 회귀이니 여기도 적용할 수 있을까? 그것은 아니다. F1~F4가 타겟이 아니기 때문이다. F1~F4에 의해서 수확량이 달라지는 것이기 때문이다.\nHarvest.groupby(“Fertil”).agg({“Yield”: [“mean”, “std”, “min”, “max”]})\n기초 통계량 분석에서는 각 그룹별 평균을 먼저 확인해야 한다. 각 그룹의 표준편차가 유사하다면, 오차도 비슷할 것으로 예상할 수 있다. 또한 최대값과 최소값을 통해 특이값의 존재 여부를 파악할 수 있다.\n이 분석에서는 퀄리티는 제외하고, 특정 농작물의 생산량에만 초점을 맞췄다. 각 그룹별 평균 수확량 차이를 살펴본 결과, 그룹 간 차이가 큰 것으로 나타났다.\n단순히 F4 그룹이 가장 높은 수확량을 기록했다고 해서 그 비료를 무조건 선택하는 것은 옳지 않다.\n이는 분석가로서 올바른 접근이 아니며, 신중한 추가 분석이 필요하다.\nimport statsmodels.api as sm import statsmodels.formula.api as smf\nHarvestFit = smf.ols(formula = ‘Yield ~ Fertil’, data = Harvest).fit() # 모형 적합 sm.stats.anova_lm(HarvestFit, typ = 3) # 분산분석표 출력\nOLS의 약자의 의미는 최소제곱법이 포함된 통계방법이라고 말할 수 있다. SST = Intercept SS + Fertil SS + Residual SS, SSR = SST – SSE, SSE = Residual sum_sq\nANOVA 표에서 MS(Fertil), MS(Residual) 은 각각 F 값 계산에 사용되는 Mean Square(평균제곱) 항목이다.\n구체적으로 표에서 다음 위치에 대응된다:\nMS(Fertil)\n계산: 209,803.533333 / 3 = 69,934.511111 이는 Fertil 행의 Mean Square 이며, 표에서는 따로 MS 열이 없지만, 다음 F값 계산식에 들어간다:\n\\[F = \\frac{MS(Fertil)}{MS(Residual)}\\]\nMS(Residual)\n계산: 27,158.216667 / 12 = 2,263.184722 이는 Residual(오차) 행의 Mean Square 이며, 모든 F값의 분모로 사용된다.\nF(Fertil) = 30.900929 계산 확인\n\\[F = \\frac{MS(Fertil)}{MS(Residual)}\\]\n\\[F = \\frac{69,934.511111}{2,263.184722} = 30.9009 \\quad (\\text{표의 F 값과 동일})\\]\n즉:\nMS(Fertil) → F 계산의 분자 MS(Residual) → F 계산의 분모 이 두 값은 표에 직접 나타나지 않고, 계산된 뒤 F 열에 반영되어 나타난다.\ntyp = 1 : 순차적 검정 변수들을 순서대로 추가하며 각 변수의 분산을 분석하는 방법.\ntyp = 2 : 부분적 검정 모든 변수의 주효과를 고려하되, 상호작용 효과는 제거하는 방법.\ntyp = 3 : 완전 검정 모든 주효과와 상호작용 효과를 포함하여 검정하는 방법. 분석의 정확성을 위해 이 방법을 사용하는 것이 좋다.(대체로 이 방식을 사용함.)\nfrom bioinfokit.analys import stat res = stat()\n\n\n데이터프레임\nres.anova_stat(df = Harvest, res_var=‘Yield’, # 아래에 있는 게 모델 anova_model=‘Yield~Fertil’, ss_typ=3) res.anova_summary\n표본 크기 (n) = 16 총 제곱합 (SST)의 자유도: 4 − 1 = 3 잔차 제곱합 (Residual 또는 SSE)의 자유도: 16 − 4 = 12\nANOVA 표에서 F 값(30.900929)은 다음과 같이 계산된다.\nF-통계량\n처치 효과(여기서는 ‘Fertil’)의 평균 제곱(sum_sq)을 잔차의 평균 제곱(mean_sq)으로 나눈 값.\n이는 비료의 효과가 통계적으로 유의미한 차이를 만들어냈음을 나타내는 F-값이다.\n각각의 분자와 분모는 평균 제곱 (Mean Square) 값으로, 이는 분산분석에서 다음과 같은 방법으로 계산된다.\n1 . 분자: 처치 효과(Fertil)의 평균 제곱 (Mean Square for Fertil) 처치 효과의 제곱합 (Sum of Squares, sum_sq) = 209803.533333 처치 효과의 자유도는 3입니다 (4개의 비료 그룹 - 1). 평균 제곱은 처치 효과의 제곱합을 자유도로 나눈 값입니다.\n2 . 분모: 잔차의 평균 제곱 (Mean Square for Residual) 잔차의 제곱합 (Residual Sum of Squares, sum_sq) = 27158.216667 잔차의 자유도는 12입니다 (총 관측값 16 - 비료 그룹 4). 평균 제곱은 잔차의 제곱합을 자유도로 나눈 값입니다.\n주어진 분산분석(ANOVA) 표를 보면, 처치 효과(즉, 비료(Fertil) 효과)에 대한 p-값이 0.000006로 매우 낮다. 이 값은 일반적으로 사용하는 유의 수준(예: 0.05)보다 작기 때문에, 처치 효과가 없다는 귀무 가설을 기각할 수 있다. 따라서 비료 종류에 따라 농작물의 평균 수확량에 유의미한 차이가 있다고 결론지을 수 있다. 즉, 처치 효과가 없다고 결론지을 수 없다.\n박스플롯 (Box Plot) # 상자그림 작성과 분산의 동일성에 대한 검증 Harvest.boxplot(“Yield”, by=“Fertil”)\n박스플롯은 각 그룹의 분포와 이상값을 시각적으로 보여주어, 그룹 간 분산이 비슷한지 확인하는 데 도움을 준다. 이를 통해 비료별 수확량이 비교된다. matplotlib 또는 seaborn을 사용해 박스플롯을 그리면 각 그룹의 중앙값, 사분위수, 그리고 잠재적인 이상치를 확인할 수 있다. 극단값이 없고, F2만 제외하면 대부분 정규분포를 따른다.\n등분산 검정을 위한 2가지 방법 등분산 검정은 그룹 간의 분산이 통계적으로 동일한지 확인하는 절차입니다. 일반적으로 다음 두 가지 방법이 사용됩니다. 이전의 정규성을 더 잘 검증하기 위한 방법이다. 이것을 하는 건 너가 수집한 수확량이 균질하진 않은지? 특이치가 있는지? 너무 수확량이 잘 나온 것만 수집한 건 아닌지?를 평가하기 위함이다. 그렇지 않고 균질하다는 것을 증명하기 위함이며 기본이 되는 방법이다. 그래서 F1에 비해 F4가 유의미하게 차이가 난다는 것을 증명할 수 있다.\n\nBartlett’s Test (바틀렛 검정) 각 그룹의 평균과의 편차를 비교하여 등분산성을 평가하는 방법. 이것의 귀무가설은?\n\nres.bartlett(df = Harvest, res_var = ‘Yield’, xfac_var = ‘Fertil’) res.bartlett_summary\nscipy.stats.levene 함수를 사용하여 레빈 검정을 수행할 수 있다. 제공된 결과는 등분산성 검정의 결과로 보이며, 다음과 같이 해석할 수 있다\nTest statistics (T): 0.6953\n이 값은 등분산성 검정의 검정 통계량으로, 분산이 동일한지 여부를 평가하는 기준입니다.\nDegrees of freedom (Df): 3.0000\n자유도는 그룹 간 차이를 평가하는 데 필요한 독립적인 정보의 수입니다. 이 값은 검정에서 사용된 그룹의 수를 바탕으로 계산됩니다.\np-value: 0.8743\np-value는 귀무가설을 기각할 수 있는지 여부를 알려줍니다. 일반적으로 p-value가 0.05보다 작으면 귀무가설을 기각하고, 두 집단의 분산이 다르다고 결론을 내립니다.\n이 경우 p-value가 0.8743으로 0.05보다 크므로 귀무가설을 기각할 수 없다. 즉, 각 그룹의 분산이 동일하다는 결론을 내릴 수 있다.\n\nLevene’s Test (레빈 검정) 정규성을 가정하는 등분산 검정.\n\nres.levene(df = Harvest, res_var = ‘Yield’, xfac_var = ‘Fertil’) res.levene_summary\n만약 데이터가 정규 분포를 따른다고 가정할 수 있다면, 바틀렛 검정을 사용할 수 있다. scipy.stats.bartlett 함수를 사용하여 바틀렛 검정을 수행할 수 있습니다.\n02 사후분석 - 다중비교 사후분석(다중비교)은 그룹 간 차이를 정확히 파악하기 위해 사용된다.\n주어진 예시에서는 비료 수준에 따라 농작물의 수확량에 유의미한 차이가 발생하는지 확인하기 위해 다중비교 방법을 사용한다.\n이를 통해 각 그룹 간 차이를 평가할 수 있다.\n1 . 최소유의차 LSD, Least Significant Difference\nLSD는 두 그룹 간 평균 차이가 유의미한 차이인지 판단하는 방법.\n단일비교에 사용되며, 유의미한 차이를 평가하기 위해 각각의 그룹을 순차적으로 비교한다. 다만, 다수의 비교를 할 경우 제1종 오류율이 증가할 수 있다는 단점이 있다.\n주로 t-test를 기반으로 하여 비교한다.\n2 . 다중범위검정 MRT, Multiple Range Test\n여러 그룹을 전체적으로 비교하는 방법. 각 그룹 간에 어떤 차이가 있는지를 여러 번 반복하여 평가합니다.\n여러 그룹 간에 차이를 평가할 때 다중검정에서 발생할 수 있는 오류를 수정하여 좀 더 보수적인 방식으로 유의미한 차이를 평가할 수 있다.\n3 . 정직유의차 HSD, Honestly Significant Difference\nTukey의 HSD 방법과 관련이 있으며, 여러 그룹 간의 평균 차이를 한 번에 비교할 수 있는 방법.\n그룹 간 차이가 얼마나 큰지 뿐만 아니라, 그 차이가 통계적으로 유의미한지 확인할 수 있게 해줍니다. 이 방법은 두 그룹을 비교할 때 발생할 수 있는 오류를 수정합니다.\n특히 비료 수준과 같이 3개 이상의 그룹을 비교할 때 적합한 방법입니다.\n\n\n실행되지 않을 수 있음!!!\nres.tukey_hsd(df = Harvest, res_var=‘Yield’, xfac_var=‘Fertil’, anova_model=‘Yield ~ Fertil’) res.tukey_summary 뮤는 항상 그대로 일정하게 있는 거고, F1과 F2가 각 계산을 통해 Diff값에 뮤가 포함된 신뢰구간을 구한 것이다. 그래서 Diff값이 기준이 되는 것이다. Diff값에도 평균이 있을 것이고, 이 평균과 뮤의 평균이 얼마나 차이가 나는지? 얼마나 뮤가 퍼져 있는지를 검증하는 코드인 것이다.\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nres = stat() df = pd.DataFrame(Harvest) res.tukey_summary = pairwise_tukeyhsd(endog = df[‘Yield’], groups = df[‘Fertil’], alpha = 0.05) print(res.tukey_summary)\n이 결과는 Tukey HSD (Honestly Significant Difference) 테스트를 사용하여 두 그룹 간의 평균 차이를 비교하였다.\ngroup1, group2: 비교되는 두 그룹입니다. 예를 들어, “F1”과 “F2”를 비교한 결과가 첫 번째 행에 나타납니다.\nmeandiff: 두 그룹 간의 평균 차이입니다. 예를 들어, F1과 F2의 평균 차이는 77.3333입니다.\np-adj: 보정된 p-value로, 다중 비교로 인한 오류를 보정한 값입니다. p-value가 0.05보다 작으면 해당 차이가 통계적으로 유의미하다고 판단할 수 있습니다.\nlower, upper: 평균 차이의 신뢰구간입니다. 예를 들어, F1과 F2의 평균 차이의 신뢰구간은 -30.54에서 185.2067까지입니다.\nreject: 통계적으로 유의미한 차이가 있는지 여부를 나타냅니다. True이면 두 그룹 간 평균 차이가 유의미하다는 의미입니다.\nF1과 F3, F1과 F4, F2와 F4, F3과 F4는 통계적으로 유의미한 차이가 있다. F1과 F2, F2와 F3는 유의미한 차이가 없다.\n\n\n실행되지 않을 수 있음\nimport matplotlib.pyplot as plt res.tukey_summary[“Group”] =\nres.tukey_summary[“group1”]+res.tukey_summary[“group2”] plt.hlines(“Group”, “Lower”, “Upper”, data = res.tukey_summary) import pandas as pd plt.hlines(tukey_summary_df[“Group”], tukey_summary_df[“lower”], tukey_summary_df[“upper”], color=‘grey’) plt.xlabel(“Mean Difference”)\n결론적으로 F4와 F3를 사용하는 것이 가장 적절하다고 판단할 수 있습니다. F4는 다른 비료들에 비해 가장 큰 평균 차이를 보였고, F3 또한 유의미한 차이를 나타냈기 때문입니다.\n분석 이후의 방향: 현업 적용: 데이터 분석 결과를 바탕으로 현업에서는 F4와 F3를 선택하는 것이 수확량을 극대화하는 데 유리할 것입니다. 그러나 현업에서는 여러 요소(비용, 공급망, 시장 반응 등)를 고려해야 하므로, 분석 결과를 참고하여 최종 결정을 내리는 것이 중요합니다. 마케팅 확장: 마케팅 분야에서는 F4와 F3의 효과를 강조하는 캠페인을 진행할 수 있습니다. 예를 들어, 특정 비료를 사용했을 때 얻을 수 있는 수확량의 증가를 강조하거나, F4와 F3가 효율적이라는 메시지를 타겟 소비자에게 전달하는 전략을 고려할 수 있습니다. 따라서, 분석을 바탕으로 선택된 비료를 현업과 마케팅에서 잘 활용하면 생산성과 판매 모두에 긍정적인 영향을 미칠 수 있을 것입니다.\nimport statsmodels.stats.multicomp as mc comp = mc.MultiComparison(Harvest[‘Yield’], Harvest[‘Fertil’]) post_hoc_res = comp.tukeyhsd() post_hoc_res.summary()\nfrom scipy import stats tbl, a1, a2 = comp.allpairtest(stats.ttest_ind, method = “bonf”) tbl\n이전의 res.tukey_summary과 값은 같다\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "ADA/ADA10.3.html",
    "href": "ADA/ADA10.3.html",
    "title": "제 10장 공분산",
    "section": "",
    "text": "Reporting Date: August. 5, 2024\n\n1 . 자 료 의 입 력 기존에 알려져 있는 방법과 신규 방법\n모델을 세운다.\n첫 번째 그룹 y1j (비교 집단) = 비교의 모평균 + 오차 = 뮤(비교와 실험의 모든 모평균) + 알파1(첫번째 처치 효과) + 오차 오차를 생략했을 때, 뮤1 - 뮤2 = bar{y}_1 - bar{y}_2라고 수식을 쓸 수 있다.\n그리고 공분산 수식으로 쓴다는 것은 여기에다 연령에 대한 효과를 반영하는 것이다.\n공분산 분석 모형의 가정 1. 공변량과 반응변수는 선형적인 관계를 가져야 한다. 아닐 경우, 블록이나, 비선형으로 간주해야 한다.\n\n공변량에 대한 두 집단에서의 기울기가 동일해야 한다. 아닐 경우, 다른 모형으로 분석을 시도해야 한다.\n\n공변량을 고려하지 않으면 바이어스가 발생할 수 있다.\n2가지 그룹에 대한 검정에서 t테스트를 하면 안 되는 이유\n최소제곱, 오차들의 제곱합 무를 자를 때, 어느 방향으로 잘라야 싱싱함을 알 수 있는가?\n회귀제곱이 아니여도 R-squared값은 나온다. 분산분석에서 관심있는 부분은 아랫부분이다. P값을 보았을 때, 처치효과가 기각할 수 없다고 나온다.\n17 = -98 + 0(첫번째 그룹은 0으로 본다) + 4.2x 실험집단에서 상담 점수가 더 높아지는 결과가 나온다.\n아노바 테이블을 만들어야 한다.\n\n교제: 파이썬을 활용한 데이터 분석과 응용"
  },
  {
    "objectID": "IDA/IDA02.html",
    "href": "IDA/IDA02.html",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "",
    "text": "Reporting Date: June. 9, 2024\n자료를 효과적으로 요약하고 이해하기 위해 표나 그림을 사용하는 방법에 대해 다루고자 한다. 자료 요약은 분석 대상인 자료의 형태와 특성에 따라 다양한 방법으로 이루어진다."
  },
  {
    "objectID": "IDA/IDA02.html#자-료-의-입-력",
    "href": "IDA/IDA02.html#자-료-의-입-력",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "1. 자 료 의 입 력",
    "text": "1. 자 료 의 입 력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제1: 사망자 목록 중 130명을 임의로 추출, 이들의 사망원인을 10가지로 분류하였다.(p.35)\n\nimport numpy as np\n\n# 변수 death에 NumPy 배열을 할당\ndeath = np.array([2, 1, 2, 4, 2, 5, 3, 3, 5, 6, 3, 8, 3, \n                  3, 6, 3, 6, 5, 3, 5, 2, 6, 2, 3, 4, 3, \n                  2, 9, 2, 2, 3, 2, 7, 3, 2, 10, 6, 2, 3, \n                  1, 2, 3, 3, 4, 3, 2, 6, 2, 2, 3, 2, 3, \n                  4, 3, 2, 3, 5, 2, 5, 5, 3, 4, 3, 6, 2, \n                  1, 2, 3, 2, 6, 3, 3, 6, 3, 2, 3, 6, 4, \n                  6, 5, 3, 5, 6, 2, 6, 3, 2, 3, 2, 6, 2, \n                  6, 3, 3, 2, 6, 9, 6, 3, 6, 6, 2, 3, 2, \n                  3, 5, 3, 5, 2, 3, 2, 3, 3, 1, 3, 3, 2, \n                  3, 3, 4, 3, 6, 6, 3, 3, 3, 2, 3, 3, 6])"
  },
  {
    "objectID": "IDA/IDA02.html#도수분포표",
    "href": "IDA/IDA02.html#도수분포표",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "2. 도수분포표",
    "text": "2. 도수분포표\nFrequency Distribution Table\n특정 데이터 집합의 값들이 속하는 범위와 속한 값들의 빈도수를 요약한 표.\n\nimport pandas as pd\n\n# death 배열의 빈도수를 계산하여 도수분포표를 생성\n# crosstab 함수: 범주형 변수의 빈도수를 교차표 형식으로 표현\ntable = pd.crosstab(index=death, colnames=[\"질병\"], columns=\"도수\") \n\n\n# 인덱스를 질병 이름으로 설정\n\n# 1: 감염성 질환, 2: 각종 암, 3: 순환기 질환, 4: 호흡기 질환, 5: 소화기 질환, \n# 6: 각종 사고사, 7:비뇨기 질환, 8: 정신병, 9: 노환, 10: 신경계 질환\n\ntable.index = [\"감염\" , \"각종암\", \"순환기\", \"호흡기\", \"소화기\", \"사고사\", \"비뇨기\", \"정신병\", \"노환\", \"신경계\"]\nprint(table)\n\n질병   도수\n감염    4\n각종암  33\n순환기  48\n호흡기   7\n소화기  11\n사고사  22\n비뇨기   1\n정신병   1\n노환    2\n신경계   1"
  },
  {
    "objectID": "IDA/IDA02.html#막대-그래프",
    "href": "IDA/IDA02.html#막대-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "3. 막대 그래프",
    "text": "3. 막대 그래프\nBar Chart\n데이터의 범주(또는 그룹)별 값을 막대(bar)의 길이로 나타낸 그래프.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 막대 그래프 그리기\nnum_bars = len(table.index)  # 막대의 개수\nbase_color = [0, 1, 1]  # 초기 색상\nbar_width = 0.8  # 막대의 폭\n\n# 데이터프레임의 각 행에 대한 반복 작업 정의\nfor i, (index, row) in enumerate(table.iterrows()):\n    \n    # 막대의 개수에 따라 색상을 연하게 표현\n    color = np.array(base_color) * (num_bars - i) / num_bars\n    \n    # 각 막대의 높이를 도수로 설정\n    plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정\n\n# 축 레이블 설정\nplt.xlabel('질병', fontsize=14)  # x축 레이블\nplt.ylabel('빈도수', fontsize=14)  # y축 레이블\n\n# 제목 설정\nplt.title('질병에 따른 막대그래프', fontsize=20)\n\n# x축 눈금 라벨 회전(가로로 표시)\nplt.xticks(rotation=0)\nplt.show()\n\nC:\\Users\\sinji\\AppData\\Local\\Temp\\ipykernel_7424\\2514237286.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  plt.bar(index, row[0], color=color, width=0.5)  # 막대의 폭 설정"
  },
  {
    "objectID": "IDA/IDA02.html#원형-그래프",
    "href": "IDA/IDA02.html#원형-그래프",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "4. 원형 그래프",
    "text": "4. 원형 그래프\nPie Chart\n전체에 대한 각 항목의 비율을 원 모양으로 표현한 그래프.\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 원형 그래프 크기 설정\nplt.figure(figsize=(10, 10))  # 가로, 세로 크기 조절\n\n# 블루 톤 색상 리스트(참고용 블로그 최하단에 표시)\ncolors = ['dodgerblue', 'royalblue', 'deepskyblue', 'lightskyblue', 'cornflowerblue', 'skyblue', 'deepskyblue']\n\n# 원형 그래프 그리기\n# 각 파이조각에 표시될 라벨, 비율 / 시작 각도 / 색상\npatches, texts, autotexts = plt.pie(table['도수'], labels=table.index, autopct='%1.1f%%', startangle=0, colors=colors)\n\n# 제목 설정\nplt.title(\"사망원인에 대한 원형 그래프\", fontsize=20)\nplt.show()"
  },
  {
    "objectID": "IDA/IDA02.html#파레토그림",
    "href": "IDA/IDA02.html#파레토그림",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "5. 파레토그림",
    "text": "5. 파레토그림\nPareto Chart\n막대그래프와 누적선그래프를 결합한 형태의 그래프.\n파레토 원칙(80/20 법칙): 전체 결과의 80%가 20%의 원인에서 비롯된다는 원칙.\n\nimport matplotlib.patches as mpatches\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 사망 원인에 따른 빈도수를 내림차순으로 정렬\nsorted_table = table.sort_values(by = '도수', ascending = False)\ncumulative_percentage = sorted_table['도수'].cumsum() / sorted_table['도수'].sum() * 100\n\n# Matplotlib를 사용하여 두 축을 가진 그래프 생성\nfig, ax = plt.subplots(figsize=(10, 8))  # 그래프 크기 설정\n\n\n# 각 질병의 빈도수를 막대 그래프로 표시\nsorted_table['도수'].plot(kind = 'bar', color = 'royalblue', ax = ax, \n                        width = 0.8, position = 0.5)\n\n# x축 레이블 설정\nax.set_xlabel('질병 종류', fontsize=14)\n\n# y축 레이블 설정\nax.set_ylabel('빈도수', fontsize=14)\n\n\n# 누적 상대도수를 선 그래프로 표시\nax2 = ax.twinx()\ncumulative_percentage.plot(color = 'black', ax = ax2, style = '-o', use_index = False)\n\n# y축 레이블 설정\nax2.set_ylabel('누적 상대도수 (%)', fontsize = 14)\n\n# y축 포맷 설정\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}%'))\n\n\n# 누적 상대도수와 빈도수의 범례를 표시\np_legend1 = mpatches.Patch(color = 'black', label = '누적 상대도수')\np_legend2 = mpatches.Patch(color = 'royalblue', label = '빈도수')\nplt.legend(handles = [p_legend1, p_legend2], loc = 'center right')\n\n\n# x 축 라벨 각도 조정\nax.set_xticklabels(sorted_table.index, rotation = 0)\n\n# 제목 설정\nplt.title(\"사망 원인\", fontsize = 20)\nplt.show()"
  },
  {
    "objectID": "IDA/IDA02.html#도수다각형",
    "href": "IDA/IDA02.html#도수다각형",
    "title": "2장: 표와 그림을 통한 자료의 요약",
    "section": "6. 도수다각형",
    "text": "6. 도수다각형\nFrequency Polygon\n도수분포표의 도수를 선으로 연결한 그래프.\n\n# 예제2: 정량 100 인 음료수 80 병을 임의로 추출, \n# 그 내용물의 측정된 양을 재어 자료를 수집하였다.(p.42)\n\n# 한글 폰트 설정\nplt.rc('font', family='Noto Sans KR')\n\n# 데이터\ndrink = np.array([101.8, 101.5, 101.8, 102.6, 101, 96.8, 102.4, 100, 98.8, 98.1, \n                  98.8, 98, 99.4, 95.5, 100.1, 100.5, 97.4, 100.2, 101.4, 98.7, \n                  101.4, 99.4, 101.7, 99, 99.7, 98.9, 99.5, 100, 99.7, 100.9, \n                  99.7, 99, 98.8, 99.7, 100.9, 99.9, 97.5, 101.5, 98.2, 99.2, \n                  98.6, 101.4, 102.1, 102.9, 100.8, 99.4, 103.7, 100.3, 100.2, \n                  101.1, 101.8, 100, 101.2, 100.5, 101.2, 101.6, 99.9, 100.5, \n                  100.4, 98.1, 100.1, 101.6, 99.3, 96.1, 100, 99.7, 99.7, 99.4, \n                  101.5, 100.9, 101.3, 99.9, 99.1, 100.7, 100.8, 100.8, 101.4, \n                  100.3, 98.4, 97.2])\n\n\n# 히스토그램 생성\nplt.figure()\nn, bins, patches = plt.hist(drink, bins = 10, facecolor = \"blue\", alpha = 0.3)\n\n\n# 빈 중심 계산\n# bins 리스트의 인접한 두 값의 평균을 구해서 중심을 계산\nx = [(bins[i] + bins[i + 1]) / 2 for i in range(len(bins) - 1)]\nw_bin = bins[1] - bins[0]\nx.insert(0, x[0] - w_bin)\nx.append(x[-1] + w_bin)\n\n\n# n의 시작과 끝에 0 추가\n# 시작과 끝에 빈도를 0으로 설정\nn = np.insert(n, 0, 0.0)\nn = np.append(n, 0.0)\n\n\n# 히스토그램 데이터 포인트 플로팅\nplt.plot(x, n, 'b-', marker = 'o')\n\n# 라벨 및 제목 추가\nplt.xlabel('음료', fontsize = 14)\nplt.ylabel('빈도수', fontsize = 14)\nplt.title('음료의 히스토그램', fontsize = 16)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA04.html",
    "href": "IDA/IDA04.html",
    "title": "4장: 두 변수 자료의 요약",
    "section": "",
    "text": "Reporting Date: July. 4, 2024\n조사 대상의 각 개체로부터 둘 또는 그 이상의 변수들을 동시에 관측하는 경우가 더 많다. 두 변수에 관한 관측값을 도표로 요약하고 해석하는 방법을 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA04.html#자료의-입력",
    "href": "IDA/IDA04.html#자료의-입력",
    "title": "4장: 두 변수 자료의 요약",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제5: 통계학과 신입생 51명의 키와 몸무게를 기록한 것이다.\n# 키와 몸무게의 표본상관계수를 구하고, 산점도를 그려라. (p.108)\n\nimport numpy as np\n\n# 키와 몸무게 자료의 입력\nheight = np.array([181,161,170,160,158,168,162,179,183,178,171,177,163,\n                   158,160,160,158,173,160,163,167,165,163,173,178,170,\n                   167,177,175,169,152,158,160,160,159,180,169,162,178,\n                   173,173,171,171,170,160,167,168,166,164,173,180]) \n\nweight = np.array([78,49,52,53,50,57,53,54,71,73,55,73,51,53,65,48,59,\n                   64,48,53,78,45,56,70,68,59,55,64,59,55,38,45,50,46,\n                   50,63,71,52,74,52,61,65,68,57,47,48,58,59,55,74,74])"
  },
  {
    "objectID": "IDA/IDA04.html#표본상관계수",
    "href": "IDA/IDA04.html#표본상관계수",
    "title": "4장: 두 변수 자료의 요약",
    "section": "2. 표본상관계수",
    "text": "2. 표본상관계수\nSample Correlation Coefficient\n산점도에서 점들이 얼마나 “직선에 가까운가” 의 정도를 나타내는 데 쓰이는 측도.\n두 변수 ( x, y ) 에 대하여 관측값 n개의 짝 ( x₁, y₁ ), ( x₂, y₂ ), …, (xn, yn) 이 주어진 때, 상관계수는 다음과 같이 계산된다:\n\n2-1. 공분산\nCovariance\n분산(Variance)은 한 변수의 데이터가 평균 주위에서 얼마나 흩어져 있는지를 나타낸다. 공분산은 “두 변수” 가 함께 변하는 정도를 나타낸다.\n공분산이 “양수” 이면, 두 변수는 “같은 방향” 으로 변한다. 공분산이 “음수” 이면, 두 변수는 “반대 방향” 으로 변한다. 공분산이 “0” 에 가까우면, 두 변수 간에 “선형 관계가 거의 없음” 을 의미한다.\n위 값들은 다음과 같은 수식을 통해 계산된다:\nx̄, ȳ 는 변수 x, y 의 표본 평균을 의미한다:\n표본상관계수는 두 변수의 직선관계의 정도(강도, 방향)를 나타내며, 다음과 같은 특징이 있다:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n계수가 “0” 에 가까울수록 두 변수 간의 직선의 관계가 “매우 약함” 을 의미한다. 표본상관계수의 단위는 없다.\n각 변수의 편차들의 곱을 사용해 계산할 때, 이 곱은 원래 변수들의 단위를 곱한 단위이지만, 분모에 각 변수의 분산을 곱한 값의 제곱근이 들어가기 때문에 단위가 사라지게 된다.\n위 연산은 단위를 제곱근 단위로 바꾸는 역할을 하며, 이로 인해 단위가 없는 숫자가 된다. 변수들의 단위에 영향을 받지 않아, 서로 다른 단위를 가진 변수들 간에도 관계를 비교할 수 있다.\n\n# 키와 몸무게의 표본상관계수\nnp.corrcoef(height, weight)[0][1]\n\n## 해석: 표본상관계수 𝑟이 약 0.74로 나왔다는 것은\n## 두 변수 사이에 강한 '양의 선형 관계'가 있음을 의미한다.\n\nnp.float64(0.7362765055636866)\n\n\n아래에서 설명할 산점도를 포함한 대부분의 그림 요약 방법은 “주관적” 일 수 있다. 반면, 표본상관계수는 “객관적” 인 수치 자료로, 이러한 문제를 “보완” 해준다.\n단, 직선이 아닌 다른 관계(곡선 등)가 있을 수 있으며, 이를 표본상관계수가 제대로 나타내지 못할 수 있다는 점을 유념해야 한다."
  },
  {
    "objectID": "IDA/IDA04.html#산점도",
    "href": "IDA/IDA04.html#산점도",
    "title": "4장: 두 변수 자료의 요약",
    "section": "3. 산점도",
    "text": "3. 산점도\nScatter Plot\n변수 x 를 “수평축” 에 놓고, 변수 y 를 “수직축” 에 놓은 후에 각 관측값의 찍을 좌표 위에 표시함 으로써 얻게 되는 그림.\n이를 통해, “두 변수 간의 관계” 를 시각적으로 대략 파악할 수 있다.\n\nimport matplotlib.pyplot as plt\n\n# 산점도 작성\nplt.figure(figsize=(8, 6))  # 그래프의 크기 (가로 8, 세로 6)\nplt.scatter(height, weight, color='slateblue')  # 산점도 색상 설정\n\nplt.xlabel('height (cm)')  # x축 레이블\nplt.ylabel('weight (kg)')  # y축 레이블\nplt.title('height(cm) and weight(kg)')  # 그래프의 제목\n\nplt.grid(True)  # 그리드 추가 (경계선)\nplt.show()\n\n\n\n\n\n\n\n\n\n참고용 블로그: 색상표 색상코드표 교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA06.html",
    "href": "IDA/IDA06.html",
    "title": "6장: 확률분포",
    "section": "",
    "text": "Reporting Date: July. 11, 2024\n5장에서 다룬 표본공간의 근원사건들은 특성을 표현하는 형태로 다뤘다. 이제는 확률변수를 중심으로 실험의 수치적 결과에 대해 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA06.html#확률변수",
    "href": "IDA/IDA06.html#확률변수",
    "title": "6장: 확률분포",
    "section": "1. 확률변수",
    "text": "1. 확률변수\nRandom Variable\n각각의 근원사건들에 실수값을 “대응시키는 함수”이며 X, Y, … 등으로 표시한다.\n확률변수가 가질 수 있는 “값의 개수” 가 유한하거나 무한이라도 “셀 수 있는 경우” 에 이를 “이산확률변수” 라고 한다.\n또한, 연속적인 구간에 속하는 모든 값을 다 가질 수 있는 “연속확률변수” 도 있다."
  },
  {
    "objectID": "IDA/IDA06.html#이산확률분포",
    "href": "IDA/IDA06.html#이산확률분포",
    "title": "6장: 확률분포",
    "section": "2. 이산확률분포",
    "text": "2. 이산확률분포\nDiscrete Probability Distribution\n확률변수가 갖는 값들과 그에 “대응하는 확률값” 을 나타내는 것으로, 나열된 표나 수식으로 표현되며, 보통은 “확률변수 X 의 분포” 라고 한다.\n\n2-1. 확률질량함수\nProbability Mass Function, PMF\n확률변수 X가 k개의 값 x₁, x₂, …, xk를 가질 때, 그에 대응하는 확률을 f(x₁), f(x₂), …, f(xk)라고 하면 X의 확률분포는 다음과 같다:\nf(x)는 확률변수 X 가 값 x 를 갖게 되는 확률 P ( X = x ) 을 나타낸다:\n이산확률변수 X 의 확률변수는 다음 조건을 만족해야 한다:\n모든 확률은 0 이상 1 이하의 값을 가진다. 확률변수가 가질 수 있는 모든 값에 대한 확률의 합은 1 이다."
  },
  {
    "objectID": "IDA/IDA06.html#이산확률변수의-평균과-표준편차",
    "href": "IDA/IDA06.html#이산확률변수의-평균과-표준편차",
    "title": "6장: 확률분포",
    "section": "3. 이산확률변수의 평균과 표준편차",
    "text": "3. 이산확률변수의 평균과 표준편차\n\n3-1. 기댓값\nExpected Value\nE ( X ) 는 확률변수 X 의 “기댓값(평균)” 또는 X 가 갖는 확률분포의 “모평균” 이라고 한다.\n뮤( M , μ: 그리스 알파벳의 열두째 글자)\n3-2. 모분산 Population Variance\n편차 (즉, 각 값이 기대값에서 얼마나 떨어져 있는지)를 제곱하고, 그 제곱된 값을 각 값이 발생할 확률로 가중평균하는 것이다.\nV a r ( X ) = ∑ ( 편차 ) ² × 확률:\n시그마(Σ, σ: 그리스어 알파벳의 열여덟째 글자)\n모분산의 간편식:\n기댓값을 알고 있다면, 직접적인 정의를 사용하지 않고 게산할 수 있다.\n\n\n3-3. 모표준편차\nPopulation Standard Deviation 모분산의 양의 제곱근으로 계산된다:\n모표준편차 ( σ ) 의 단위는 확률변수 X와 “동일” 하다. 반면 모분산 ( σ² ) 의 단위는 X 의 단위를 “제곱” 한 것이므로, 퍼진정도를 측정하는데 적절하지 않다.\n예를 들어, X 의 단위가 센티미터( cm )라면, 모분산 σ² 의 단위는 제곱센티미터( cm² )가 된다."
  },
  {
    "objectID": "IDA/IDA06.html#두-확률분포의-결합분포",
    "href": "IDA/IDA06.html#두-확률분포의-결합분포",
    "title": "6장: 확률분포",
    "section": "4. 두 확률분포의 결합분포",
    "text": "4. 두 확률분포의 결합분포\n하나의 실험에서도 2 개 이상의 측면에 대한 관측이 가능하다. 이 경우 그 2 가지 특성 간의 관계 여부 및 그 관계 정도에 대해 분석할 수 있다.\n\n4-1. 결합확률분포\nJoint Probability Distribution\n2개 이상의 확률변수가 동시에 특정한 값을 가질 확률을 나타내는 분포이다.\n2 개의 확률변수가 이산일 경우,\nX 가 취하는 값을 x₁, …, xm Y 가 취하는 값을 y₁, …, yn 이라고 할 때\nX 와 Y 의 결합확률분포는 모든 1 ≤ i ≤ m, 1 ≤ j ≤ n 에 대하여\n위 식을 구하므로써 결정되며, 다음과 같이 표현할 수 있다:\n\n\n4-2. 주변확률분포\nMarginal Probability Distribution\n결합확률분포에서 한 확률변수를 “고정” 하고 다른 변수의 분포를 고려하는 분포이다. 이는 2 개 이상의 확률변수 중 하나에 대한 “단일 확률분포” 를 얻기 위해 사용한다:\n각각의 주변확률을 이용해서 하나의 변수 때와 마찬가지로 구하면 된다:"
  },
  {
    "objectID": "IDA/IDA06.html#공분산과-상관계수",
    "href": "IDA/IDA06.html#공분산과-상관계수",
    "title": "6장: 확률분포",
    "section": "5. 공분산과 상관계수",
    "text": "5. 공분산과 상관계수"
  },
  {
    "objectID": "IDA/IDA06.html#공분산",
    "href": "IDA/IDA06.html#공분산",
    "title": "6장: 확률분포",
    "section": "5-1. 공분산",
    "text": "5-1. 공분산\nCovariance\n두 확률변수 X 와 Y 가 함께 변하는 정도를 측정한다.\nX, Y가 같은 방향으로 변화할 경우, (즉, 둘 다 증가하거나 둘 다 감소하는 경우) ( X − μX ​), ( Y − μY ​) 의 부호가 일치할 확률이 상대적으로 커진다. 따라서, 이에 대한 기댓값은 양수가 된다.\nX, Y가 다른 방향으로 변화할 경우, (즉, 한 변수가 증가할 때 다른 변수는 감소하는 경우) ( X − μX ​), ( Y − μY ​) 가 서로 다른 부호를 갖게 될 확률이 상대적으로 커진다. 따라서 이에 대한 기댓값은 음수가 될 것이다.\nX, Y 의 공분산은 아래와 같이 정의된다:"
  },
  {
    "objectID": "IDA/IDA06.html#상관계수",
    "href": "IDA/IDA06.html#상관계수",
    "title": "6장: 확률분포",
    "section": "5-2. 상관계수",
    "text": "5-2. 상관계수\nCorrelation Coefficient\n두 확률변수 간의 선형 관계의 강도와 방향을 측정한다. 이것은 공분산을 표준화한 형태로, −1 과 1 사이의 값을 가진다.\n상관계수의 성질:\n절댓값이 클수록 (1 또는 −1에 가까울수록) 점들은 기울기가 양수(또는 음수)인 직선에 가깝게 몰려 있다. 0보다 값이 클 경우, 점들이 좌하에서 우상방향으로 띠를 형성한다. 이때, 한 변수의 값이 크면 다른 변수의 값도 큰 경향을 가진다.\n정확한 선형관계 Y = aX + b 가 성립할 때, 상관계수의 값은 −1 또는 1 이다.\nX, Y 의 상관계수는 각 확률변수에 상수가 더해지거나 감해지는 것에 영향을 받지 않는다. 상수가 곱해진 경우, 그 부호에만 영향을 받는다.\n상수 c 와 d 의 부호가 다르면 상관계수의 부호가 반대가 된다."
  },
  {
    "objectID": "IDA/IDA06.html#두-확률변수의-독립성",
    "href": "IDA/IDA06.html#두-확률변수의-독립성",
    "title": "6장: 확률분포",
    "section": "6. 두 확률변수의 독립성",
    "text": "6. 두 확률변수의 독립성\n2 개의 확률변수 X, Y 가 독립이 되기 위해서는 X, Y 가 취하는 모든 쌍의 값 ( xi, yi ) 에 대해 아래 식을 만족해야 한다.\n두 확률변수 X, Y 가 서로 독립일 때, 아래의 식이 성립한다:\n단, 공분산과 상관계수가 0 이라는 사실이 항상 두 변수가 독립이라는 것을 보장하지 않는다. 이는 두 변수 간에 선형 관계가 없음을 의미하지만, 비선형 관계가 존재할 수 있다.\n두 확률변수가 독립일 경우, 공분산이 0 이 되므로 합과 차의 분산을 쉽게 계산할 수 있다.\n분산과 공분산의 정의를 이용하면: 공분산 항을 제외하여 다음과 같이 나타낼 수 있다:\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA08.html",
    "href": "IDA/IDA08.html",
    "title": "8장: 정규분포",
    "section": "",
    "text": "Reporting Date: July. 18, 2024\n6 장에서 언급되었던, 연속적인 값을 가지는 연속확률분포들 중에서 대부분의 통계학 이론의 기본이 되는 정규분포에 대해 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA08.html#자료의-입력",
    "href": "IDA/IDA08.html#자료의-입력",
    "title": "8장: 정규분포",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n!pip install numpy\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 7: 어느 대학교의 일반수학 중간고사 성적은\n# 분포가 평균이  63 이고, 분산이  100 인 정규분포를 따른다고 가정한다. (p.232)\n\n# (1) 50 점 이하의 학생은 몇 퍼센트나 되겠는가?\n\n# (2) 상위 10 %의 학생에게  A 를 준다고 하면 \n# 몇 점 이상이 되어야  A 를 받을 수 있겠는가?\n\n# ___________________________________________________________\n\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\n\nimport numpy as np \n\ndata1 = np.array([4001, 3927, 3048, 4298, 4000, 3445, \n                 4949, 3530, 3075, 4012, 3797, 3550, \n                 4027, 3571, 3738, 5157, 3598, 4749, \n                 4263, 3894, 4262, 4232, 3852, 4256, \n                 3271, 4315, 3078, 3607, 3889, 3147, \n                 3421, 3531, 3987, 4120, 4349, 4071, \n                 3683, 3332, 3285, 3739, 3544, 4103, \n                 3401, 3601, 3717, 4846, 5005, 3991, \n                 2866, 3561, 4003, 4387, 3510, 2884, \n                 3819, 3173, 3470, 3340, 3214, 3670, \n                 3694])\n                 \n# ___________________________________________________________\n\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\n\ndata2 = np.array([39.3, 14.8, 6.3, 0.9, 6.5, \n                 3.5, 8.3, 10.0, 1.3, 7.1, \n                 6.0, 17.1, 16.8, 0.7, 7,9, \n                 2.7, 26.2, 24.3, 17.7, 3.2, \n                 7.4, 6.6, 5.2, 8.3, 5.9, \n                 3.5, 8.3, 44.8, 8.3, 13.4, \n                 19.4, 19.0, 14.1, 1.9, 12.0, \n                 19.7, 10.3, 3,4, 16.7, 4.3, \n                 1.0, 7.6, 28.33, 26.2, 31.7, \n                 8.7, 18.9, 3.4, 10.0])\n\nRequirement already satisfied: numpy in c:\\users\\sinji\\anaconda3\\envs\\py310\\lib\\site-packages (2.2.5)"
  },
  {
    "objectID": "IDA/IDA08.html#연속확률분포",
    "href": "IDA/IDA08.html#연속확률분포",
    "title": "8장: 정규분포",
    "section": "2. 연속확률분포",
    "text": "2. 연속확률분포\nContinuous Probability Distribution\n연속확률변수 X 가 가질 수 있는 값들의 분포를 나타낸다. 연속확률변수: 특정 범위 내에서 모든 실수 값을 가질 수 있는 변수."
  },
  {
    "objectID": "IDA/IDA08.html#확률밀도함수",
    "href": "IDA/IDA08.html#확률밀도함수",
    "title": "8장: 정규분포",
    "section": "2-1. 확률밀도함수",
    "text": "2-1. 확률밀도함수\nProbability Density Function, PDF\n연속확률변수 \\(X\\) 의 확률분포는 확률의 밀도를 나타내는 \\(X\\) 의 확률밀도함수에 의해 결정된다. 아래의 조건을 만족하는 함수 \\(f(X)\\) 를 \\(X\\) 의 확률밀도함수라고 한다.\n\n연속확률변수에서 특정 값에 대한 확률은 0 이다. 예를 들어, [ 0, 1 ] 구간에 있는 실수 값은 무한히 많다. 이러한 경우, 특정한 하나의 값을 가질 확률은 무한히 작은 값이 되며, 이는 수학적으로 0 으로 표현된다.\n\n범위 : 확률밀도함수 f ( x ) 의 값은 항상 0 이상이어야 한다. 이는, 어떤 사건의 확률이 음수일 수 없다는 것을 의미한다.\n확률밀도함수는 이산확률분포의 확률함수와는 달리 확률을 나타내지 않는다. 그러므로, f ( x ) 가 1 보다 작아야 된다는 조건 은 필요가 없다.\n\nPDF 를 사용하여 특정 구간 [ a, b ] 에 속할 확률을 계산할 수 있다.\nPDF 를 전체 범위에 대해 적분 하면 1 이 되어야 한다.\n\n위 조건으로부터 다음과 같은 결론을 내릴 수 있다:\n어떤 구간의 확률을 구할 때에는 그 구간의 경계점이 포함되는가 포함되지 않는가에 영향을 받지 않는다."
  },
  {
    "objectID": "IDA/IDA08.html#정규분포",
    "href": "IDA/IDA08.html#정규분포",
    "title": "8장: 정규분포",
    "section": "3. 정규분포",
    "text": "3. 정규분포\nNormal Distribution\n평균 \\(μ\\) 와 분산 \\(σ^2\\) 의해서 그 분포가 확정된다. 그 확률밀도함수 ( PDF ) 의 대략적인 특성은 다음과 같이 표현할 수 있다:\n위 정규분포는 N ( μ, σ² )으로 표시할 수 있다. X 가 평균 μ 로부터 ± σ, ± 2σ, ± 3σ, ± 4σ 의 사이의 있을 확률은 다음과 같다.\n이를 통해 ± 4σ 이상 떨어진 데이터는 매우 드문 현상임을 알 수 있다. 그러므로, 정규분포의 ± 4σ 이상의 영역은 실질적인 분석에서 종종 무시할 수 있다."
  },
  {
    "objectID": "IDA/IDA08.html#정규분포의-특성",
    "href": "IDA/IDA08.html#정규분포의-특성",
    "title": "8장: 정규분포",
    "section": "3-1. 정규분포의 특성",
    "text": "3-1. 정규분포의 특성\nμ ± 3σ 안에 거의 모든 확률이 집중된다.\n3 - 2 . 표 준 정 규 분 포 ( Standard Normal Distribution ) 평균 ( μ ) 이 0 이고 표준편차 ( σ )가 1 인 특수한 정규분포.\n확률변수 Z 가 N ( 0, 1 ) 이라고 할 때, Z 는 0 을 중심으로 대칭인 분포를 갖게 되며,\n이를 이용해 다음과 같이 나타낼 수 있다:\n누적분포함수 ( CDF ) 는 특정 값 이하의 확률을 나타낸다.\nP ( Z ≤ b ) 는 Z 가 b 이하일 확률이고, P ( Z ≤ a ) 는 Z 가 a 이하일 확률이다. 따라서 a 에서 b 까지의 확률은 다음과 같다:\n\n3-3. 표준정규확률변수\nStandard Normal Random Variable\n표준정규분포 Z 에 관한 확률계산 방법을 일반 정규분포 X 의 확률계산에 적용할 수 있다. 일반 정규분포 X 를 표준정규분포 Z 로의 식변환을 통해 쉽게 계산할 수 있으며, 이를 표준화라고 한다.\n확률변수 X 가 N ( μ, σ² ) 일 때 표준화된 확률변수 Z 는 정규분포 N ( 0, 1 ) 을 따른다. X 를 표준화하여 Z 로 표현한 후 표준정규분포표를 이용하면 된다.\n예제 7의 ( 1 ) 중간고사 성적을 확률변수 X 라고 하면 주어진 정보를 바탕으로 다음과 같이 표현할 수 있다:\n평균 (μ) = 63, 분산 (σ²) = 100 = 10², 표준편차 (σ) = 10 정규분포표의 −1.3행, 0.00열의 값 = 0.0968\n\nfrom scipy.stats import norm\nprint(norm.cdf(x=50, loc=63, scale=10))\n\n## 해석: 50 점 이하의 학생의 비율은  0.0968 = 9.68% 이다.\n\n0.09680048458561036\n\n\n예제 7의 ( 2 ) x 점 이상의 학생들에게 A 를 준다고 하면 x 는 다음을 만족해야 한다: 상위 10 % = 0.10\n먼저, 표준정규분포표로부터 P [Z ≥ z] = 0.10 이 되는 z 값을 찾는다: 이 경우, 0.10 에 가까운 값을 주는 z = 1.28 을 고르면 된다:\n\nfrom scipy.stats import norm\nprint(norm.ppf(q=0.9, loc=63, scale=10))\n\n## 해석: 상위 10 %의 학생에게  A 를 준다고 하면 \n## 75.8 점 이상의 점수를 받은 학생에게 주면 된다.\n\n75.815515655446"
  },
  {
    "objectID": "IDA/IDA08.html#이항분포의-정규분포근사",
    "href": "IDA/IDA08.html#이항분포의-정규분포근사",
    "title": "8장: 정규분포",
    "section": "4. 이항분포의 정규분포근사",
    "text": "4. 이항분포의 정규분포근사\n대규모 시행에서 이항 분포를 정규 분포로 근사하는 방법. 7장의 초기하 분포나, 포아송 분포 마찬가지로, 정규 분포로도 근사하여 계산할 수 있다.\n이항 분포는 n 이 커짐에 따라 근사적으로 정규분포를 따르게 된다. 이때 정규분포의 평균과 분산은 이항분포의에서와 일치하여야 한다:"
  },
  {
    "objectID": "IDA/IDA08.html#연속성수정",
    "href": "IDA/IDA08.html#연속성수정",
    "title": "8장: 정규분포",
    "section": "4-1. 연속성수정",
    "text": "4-1. 연속성수정\nContinuity Correction\n이항 분포의 이산적인 값을 연속적인 정규 분포의 구간에 맞추는 작업. 보통 ± ½ ​를 가감하여 근삿값을 계산한다.\n\\[\nX ~ Bin (n, p) ≈ Y \\~ N (μ, σ²)\n\\]\n확률변수 X 가 이항분포, 즉 X ~ Bin ( n, p ) 이고, np, nq ( =1 − p ) 가 모두 클 경우 ( 보통 10 이상 ) X 는 근사적으로 평균이 np, 표준편차가 √npq 인 정규분포를 따른다."
  },
  {
    "objectID": "IDA/IDA08.html#정규분포가정의-조사",
    "href": "IDA/IDA08.html#정규분포가정의-조사",
    "title": "8장: 정규분포",
    "section": "5. 정규분포가정의 조사",
    "text": "5. 정규분포가정의 조사\n모집단의 분포가 정규 분포를 따른다는 가정을 조사하기 위해 사용할 수 있는 효과적인 그림. 정규점수그림 ( Normal Scores Plot ) 또는 정규확률그림 ( Normal Probability Plot ) 으로 불린다.\n\n5-1. 정규점수\nNormal Scores\n표본 데이터와 표준 정규 분포 ( 평균 0, 표준편차 1 ) 를 비교하여 데이터의 정규성을 평가하는 데 사용된다. 정규성( Normality ) : 데이터가 정규 분포를 따르는지를 의미한다.\n즉, 표준정규분포의 확률밀도함수 ( PDF ) 를 등확률 구간으로 나누어 주는 경곗값 ( z 값 ) 을 의미한다. 그림으로부터 분포의 형태를 알기 위해선 자료의 크기가 적어도 15 이상은 되어야 한다.\n\n\n5-2. 정규확률그림 그리는 순서\n자료를 작은 것부터 크기순으로 나열한다. 각 자료에 해당하는 점수를 계산한다. i 번째 순서의 자료와 i 번째 순서의 정규점수를 하나의 쌍으로 2 차원 공간상에 나타낸다.\n\n\n5-3. 정규그림의 해석\n정규분포를 따른다면, 양쪽의 값은 서로 가까울 것이라고 예상할 수 있다.\n\n\n5-3. 정규확률그림을 이용한 정규성 판정\n직선식일 경우, 정규분포의 가정이 타당하다고 볼 수 있다. (곡선 등) 직선식을 벗어날 경우, 가정이 의문시된다고 할 수 있다.\n\n# 예제 12: 정규확률그림을 그려라. (p.247)\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsm.qqplot(data1, line = \"s\")\n\n# \"s\"는 \"standardized\"를 의미한다. \n\n# 이 옵션은 플롯에 표준화된 선\n# (평균 0, 표준편차 1의 정규 분포)에 맞추어 선을 그린다.\n\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에 가까우므로 \n## 데이터가 정규분포를 따를 가능성이 높다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n\n5-4. 자료의 변환\n만약 추출된 표본이 정규확률그림 등에서 정규분포와 상당히 벗어난 것으로 판명되면, 자료의 변환을 통해 정규분포의 형태를 갖도록 시도해 볼 수 있다.\n\n# 예제 13: 어느 숲속에서 49그루의 나무의 체적을 측정한 자료이다.\n# 히스토그램을 그리고 적절한 변환을 하여 정규분포로 접근시켜 보아라. (p.249)\nimport matplotlib.pyplot as plt\n\nplt.hist(data2, bins=5, range=(0,50)) \nplt.xlabel('data2') \nplt.ylabel('Density') \nplt.title('Historam of data')\n\n## 해석: 아래 히스토그램을 보면 자료의 분포가 왼쪽으로 편중되어 있으므로\n## 정규분포가 아니라는 의심을 할 수 있다.\n\nText(0.5, 1.0, 'Historam of data')\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nsm.qqplot(data2, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\n## 해석: 데이터 포인트들이 직선에서 크게 벗어나\n## 데이터가 정규 분포를 따르지 않을 가능성이 있음을 확인할 수 있다.\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n이 경우, 큰 자룟값을 더 작게 만드는 과정을 수행한다:\n\n# 원 자료를 제곱근(체적^(0.5))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata3 = np.sqrt(data2)\nsm.qqplot(data3, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n# 원 자료를 네제곱근(체적^(0.25))으로 변환한 결과\nimport statsmodels.api as sm\n\ndata4 = np.power(data2, 0.25)\nsm.qqplot(data4, line = \"s\")\nplt.title(\"Normal Q-Q plot\")\n\nText(0.5, 1.0, 'Normal Q-Q plot')\n\n\n\n\n\n\n\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA10.html",
    "href": "IDA/IDA10.html",
    "title": "10장: 통계적 추론",
    "section": "",
    "text": "Reporting Date: July. 25, 2024\n추출된 표본으로부터 모집단의 일반적인 특성을 추론해내는 것을 통계적 추론이라고 하며, 이는 표본의 크기가 클 때 더 정확하게 성립한다."
  },
  {
    "objectID": "IDA/IDA10.html#자료의-입력",
    "href": "IDA/IDA10.html#자료의-입력",
    "title": "10장: 통계적 추론",
    "section": "1. 자료의 입력",
    "text": "1. 자료의 입력\n\n## 교재 출처 최하단에 표시 ##\n\n# 예제 13: 예제 1 에 주어진 자료에 근거해서 \n# 중학교 1학년 남학생의 평균키에 대한 95% 신뢰구간을 구하라. (p.311)\n\nimport numpy as np \n\nheight = np.array([163, 161, 168 , 161, 157, 162, 153, 159, 164, 170, \n                   152, 160, 157, 168, 150, 165, 156, 151, 162, 150, \n                   156, 152, 161, 165, 168, 167, 165, 168, 159, 156])\n                   \n\n#_____________________________________________________________________________\n\n# 예제 14: 예제 8의 검정을 시행하고 그 결과를 비교하여라. (p.313)\n\n# 예제 8: 앞의 예제 1에 주어진 중학생의 키 자료로부터 \n# 그 도시의 중학교 1학년 남학생의 평균키(m)가 다른 도시의 중학교 1학년 \n# 남학생의 평균키인 159 cm와 차이가 있다고 할 수 있는지 판단하라. (p.297)\n\n\n#_____________________________________________________________________________\n\n# 예제 15: 3 장의 예제 11 에 주어진 자료로부터 \n# 평균교통소음정도(μ)에 대한 98% 신뢰구간을 구하고, \n\n# 평균교통소음정도가 60 을 초과한다고 주장할 수 있는지 \n# 유의수준 5% 에서 가설검정을 실시하라. (p.314)\n\nnoise = np.array([55.9, 63.8, 57.2, 59.8, 65.7, 62.7, 60.8, 51.3, 61.8, 56.0, \n                  66.9, 56.8, 66.2, 64.6, 59.5, 63.1, 60.6, 62.0, 59.4, 67.2,\n                  63.6, 60.5, 66.8, 61.8, 64.8, 55.8, 55.7, 77.1, 62.1, 61.0, \n                  58.9, 60.0, 66.9, 61.7, 60.3, 51.5, 67.0, 60.2, 56.2, 59.4, \n                  67.9, 64.9, 55.7, 61.4, 62.6, 56.4, 56.4, 69.4, 57.6, 63.8])"
  },
  {
    "objectID": "IDA/IDA10.html#통계적-추론",
    "href": "IDA/IDA10.html#통계적-추론",
    "title": "10장: 통계적 추론",
    "section": "2. 통계적 추론",
    "text": "2. 통계적 추론\nStatistical Inference\n표본이 갖고 있는 정보를 분석하여 모수에 관한 결론을 유도하고, 모수에 대한 가설의 옳고 그름을 판단하는 것을 말한다.\n모집단의 일부인 표본으로부터 전체 모집단의 성질을 추론해내는 것이므로 100% 확실하다고 할 수는 없다. 따라서 통계적인 추론을 할 때에는 그 결론의 부정확한 정도를 반드시 언급하여야 하는데 이러한 정도를 수치로 표시할 수 있게 하는 도구로 앞에서 공부한 확률론과 표준분포 등이 이용된다.\n통계적 추론에는 두 가지 주요 방법이 있다:\n모수의 추정 모수에 대한 가설 검증"
  },
  {
    "objectID": "IDA/IDA10.html#모평균의-추정",
    "href": "IDA/IDA10.html#모평균의-추정",
    "title": "10장: 통계적 추론",
    "section": "3. 모평균의 추정",
    "text": "3. 모평균의 추정\nEstimation of Parameters\n모수 중 하나로 포함되는, 모집단의 평균에 대한 점추정과 구간추정을 다루고자 한다.\n\n3-1. 점추정\nPoint Estimation\n추정하고자 하는 하나의 모수에 대해, 여러 개의 확률변수를 사용하여 하나의 통계량을 만들고, 주어진 표본으로부터 그 값을 계산하여 하나의 수치를 제시하는 과정.\n모수 ( Parameter ) : 모집단의 실제 값으로, 우리가 알고 싶어하는 대상. 추정량 ( Estimator ) : 모수를 측정하기 위해 만들어진 통계량. 추정치 ( Estimate ) : 주어진 관측값으로부터 계산된 추정량의 실제 값.\n추정량은 하나의 확률변수이므로, 추출된 표본의 따라 그 값이 달라진다. 수치들의 변화의 정도는 추정량의 정확도와 관계가 있다. 이 정확도를 측정하는 도구 중 하나가 표준오차 ( 추정량의 표준편차 ) 이다.\n\n\n3-2. 표준오차\nStandard Error, SE\n추정량의 정확도를 평가하는 데 중요한 지표이며, 값이 작을수록 추정량이 모집단 모수를 더 정확하게 반영한다고 할 수 있다. 표본평균의 기댓값과 표준오차는, 모집단의 평균과 표준편차가 μ, σ 일 때 위와 같이 구할 수 있다.\n표본평균을 가지고 μ 를 추정할 경우, n 이 클수록 표준오차가 작아져 좀 더 정확한 추정이 가능하다. 그러나 표준오차 계산 시 σ 가 주어지지 않는 경우가 있다. 이 경우, σ 를 표본표준편차로 추정하여 사용할 수 있다.\n\n\n3-3. 구간추정\nInterval Estimation\n추정량의 분포를 이용하여 표본으로부터 모수 값을 포함하리라고 예상되는 구간을 제시하는 것. 이때 제시되는 구간을 신뢰구간이라고 한다.\n\n\n3-4. 신뢰구간\nConfidence Interval\n모집단의 어떤 모수를 추정하기 위해 계산된 범위. 신뢰구간은 ( L , U )의 형태로 이루어지며, 여기서 L 과 U 는 표본으로부터 계산된 통계량이다.\nL ( Lower bound ) : 신뢰구간의 하한값 U ( Upper bound ) : 신뢰구간의 상한값 따라서 표본마다 계산되는 신뢰구간은 서로 다를 수 있다.\n가장 확실한 신뢰구간은 항상 모수를 포함하는 구간이다. 그러나 이는 이론적으로는 가능하지만 실질적으로는 불가능하다. 그렇게 되기 위해서는 신뢰구간이 상당히 길어질 수 밖에 없다.\n이 경우, 신뢰구간 CI 는 매우 넓어질 수밖에 없다.\n항상 모수를 포함하는 신뢰구간은 실질적으로 너무 넓어서 유용하지 않다. 모수에 대한 정확한 정보를 얻으려면 신뢰구간을 가능한 한 줄일 필요가 있다.\n신뢰구간이 좁을수록 추정의 정확성이 높아진다.\n실용적인 측면에서 신뢰구간을 적절히 좁히기 위해, ” 모든 표본에서 항상 모수를 포함해야 한다 ” 는 엄격한 조건을 완화하고, 대부분의 경우에서 모수를 포함하도록 설정하는 것이 필요하다.\n신뢰구간이 모수를 포함할 확률을 1 보다는 작은 일정한 수준에 유지하여 구간의 길이를 줄이는 것이 바람직하다.\n이때 모수를 포함할 확률을 신뢰수준 ( Level of Confidence ) 또는 신뢰도라고 한다.\n\n\n3-5. 모평균 μ에 대한 신뢰구간\n여기에서는 신뢰구간을 계산하는 두 가지 경우를 설명한다:\n\n모집단의 표준편차 σ 를 알고 있는 경우:\n\n정규분포 개념을 이용하여 신뢰구간을 계산한다:\n신뢰수준 95 % 에 해당하는 정규분포의 임계값 Z α / 2 ​를 사용하여 신뢰구간의 범위를 설정한다. 예시 : α = 0.05 일 때, Z ₀.₀₅ / ₂ = Z ₀.₀₂₅ = 1.96\n정규분포는 평균 μ 와 표준편차 σ 를 알 때, 그 분포의 형태가 완전히 결정된다. 평균 μ 를 중심으로 좌우 대칭이며, σ 가 클수록 분포가 넓어진다. 이 특성 덕분에, 정규분포는 모수에 대해 많은 정보를 제공할 수 있다. 모집단이 정규분포를 따른다면, 표본 평균의 분포 역시 정규분포를 따른다.\n위의 식에서 괄호 안에 부등식을 풀어 쓰면:\n위 식을 μ 에 대해 정리하면:\n따라서:\n\n모집단의 표준편차 σ를 모르는 경우:\n일반적인 경우에 관심있는 모집단은 그 분포나 표준편차가 알려져 있지 않다. 이런 경우, 9장에서 다룬 중심극한정리를 이용한다.\n\n위 식은 약간의 수정이 필요한데, 이것은 11장을 참고하기를 바란다.\n이 경우에도, 확률 1 − α 는 근사적으로 얻어진다. n 이 클 때는 σ 대신 s 를 사용해도 확률값에 크게 영향을 주지 않는다.\n일반적으로 추정량의 기댓값이 추정하고자하는 모수값을 갖고 그 분포가 정규분포일 때 100 ( 1 − α ) % 신뢰구간은 다음과 같은 형태를 따른다:\n사용 상황에 따라 표준오차의 정의가 달라질 수 있다.\n\n\n3-6. 신뢰구간의 의미\n이 그래프는 주어진 모수에 대해 95% 신뢰구간을 표시하였다. 주어진 모수 (평균 100, 표준편차 10), 표본 크기 15를 사용하여 총 25개의 표본을 추출하였다.\n위 그래프와 같은 방식으로 표본을 계속 추출하고 신뢰구간을 계산하면, 그 신뢰구간들이 모평균을 포함하는 비율이 95% 에 가까워지는 것을 확인할 수 있다.\n\n\n3-7. 표본 크기의 결정\n많은 수의 표본을 추출하여 신뢰구간을 생성하는 것은 시간과 비용이 많이 소모된다. 따라서 우리가 원하는 정확도를 얻을 수 있는 범위 내에서 표본의 크기를 줄이는 것이 바람직하다.\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α ) % 가 되려면:\n표준화된 확률변수의 분포가 표준정규분포를 따르므로:\n위 사실로부터 아래 식이 성립해야 하며:\n이를 n 에 대하여 풀면 아래 식을 만족해야 한다:\n적정한 표본의 크기는 위의 부등식을 만족하는 ’ 최소의 정수 ’ 가 된다. 만약 모집단이 정규분포라는 가정이 없다면 표본의 크기는 중심극한정리를 이용할 수 있도록 30 이상이 되어야 한다."
  },
  {
    "objectID": "IDA/IDA10.html#모평균에-대한-검정",
    "href": "IDA/IDA10.html#모평균에-대한-검정",
    "title": "10장: 통계적 추론",
    "section": "4. 모평균에 대한 검정",
    "text": "4. 모평균에 대한 검정\n통계적 추론 중 하나인 모수에 대한 가설 검증 ( Testing Statistical Hypotheses ) 에 대해 다루고자 한다.\n\n4-1. 가설\nHypotheses 가설검증에는 2 개의 가설이 있다.\n대립가설 ( Alternative Hypothesis ; H ₁ ) : 입증하여 주장하고자 하는 가설.\n귀무가설 ( Null Hypothesis ; H ₀ ) : 대립가설을 입증할 수 없을 때, 대립가설을 무효화하면서 받아들이는 가설.\n\n\n4-2. 오류의 종류\n가설검증에서 내리는 판단이란 다음 2 가지 형태 중 하나로 나타난다. 상황에 따라 다르지만, 일반적으로 제1종 오류에 더 주의를 기울이게 된다.\n제1종 오류 ( Type I Error ; α ) : 귀무가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 없는데, 효과가 있다고 결론 내리는 경우.\n제2종 오류 ( Type II Error ; β ) : 대립가설이 참임에도 기각할 확률. 예시 : 실제로 신약이 효과가 있는데, 효과가 없다고 결론 내리는 경우.\n\n\n4-3. 검정통계량\nTest Statistic 표본 데이터를 요약하여 귀무가설을 검정하는 데 사용하는 값. 귀무가설이 참이라는 가정 하에 표본에서 계산된 값으로, 이 값이 귀무가설 하에서 얼마나 극단적인지를 평가한다.\n\n\n4-4. 기각역\nCritical Region 귀무가설을 기각할 수 있는 값들의 집합.\n검정 통계량이 기각역에 속할 경우 귀무가설을 기각한다. 아닐 경우 귀무가설을 기각하지 않는다.\nc 이하이면 H₀ 을 기각한다.\n가장 바람직한 기각역이란 아래 두 확률을 최소화하는 것이 될 것이다.\nα : 제 1 종 오류를 범하게 될 확률. β : 제 2 종 오류를 범하게 될 확률.\n위 두 확률은 다음과 같은 특징이 있다:\n( 1 ) α 와 β 는 서로 반비례 관계에 있다.\n( 2 ) α 는 너무 크게 설정하지 않는 것이 좋다. 너무 큰 α 는 제1종 오류의 확률을 높여 잘못된 결론을 내릴 가능성을 높인다.\n이를 방지하기 위해, 유의수준이라는 상한선을 둘 수 있다.\n\n\n4-5. 유의수준\nSignificance Level 일반적으로 사용되는 유의수준은 0.05 ( 5% ) 또는 0.01 ( 1% ) 이다. 이는 연구자가 제1종 오류의 확률을 낮추어 신뢰할 수 있는 결과를 얻기 위함이다.\n0.05 ( 5% ) : 가장 흔히 사용되는 유의수준. 제1종 오류를 5% 로 제한. 0.01 ( 1% ) : 더 엄격한 기준으로, 제1종 오류를 1% 로 제한. 0.10 ( 10% ) : 덜 엄격한 기준으로, 제1종 오류를 10% 로 제한.\n양측 검정 ( Two — tailed Test ) : 유의 수준 α 는 두 방향 ( 양쪽 끝 ) 에 분배되어 α / 2 씩 기각역을 형성한다.\n단측 검정 ( One — tailed Test ) : 유의 수준 α 는 한 방향에 집중되어 기각역을 형성한다.\n\n\n4-6. 모평균 μ에 대한 검 정\n표본의 크기가 클 때 모평균 μ 에 대한 가설 H ₀ : μ = μ₀ 을 검정하기 위한 검정통계량은 다음과 같다: 단 모집단의 표준편차 σ 가 주어져 있을 때 s를 σ 로 대체한다.\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n4-7. 유의확률\nSignificance Probability 주어진 검정통계량의 관측치로부터 H ₀ 을 기각하게 하는 최소의 유의수준.\nP – 값 ( P – Value )\n일반적으로 유의확률은 주어진 관측값을 경계점으로 하는 기각역의 유의수준으로 얻어진다. Z = z 일 때 각 기각역의 형태에 따라 P – 값을 구하는 식을 정리하면 다음과 같다:"
  },
  {
    "objectID": "IDA/IDA10.html#모비율에-대한-추론",
    "href": "IDA/IDA10.html#모비율에-대한-추론",
    "title": "10장: 통계적 추론",
    "section": "5. 모비율에 대한 추론",
    "text": "5. 모비율에 대한 추론\n모집단에서 특정 속성을 가진 개체의 비율을 추정하거나 검정하는 것을 의미한다.\n\n5-1. 점추정\n모비율에 대한 추정량으로 표본비율을 사용할 수 있다: 점추정량이 결정되면 그 추정량의 정확도를 알기 위해 표준오차를 계산할 필요가 있다.\n모집단의 크기가 매우 커서 그에 비해 표본의 크기가 작은 경우, X 의 분포는 반복 횟수 n , 성공의 확률 p 인 이항분포가 된다. 따라서, X 의 기댓값과 분산는 아래와 같다:\nE ( X ) = np Var ( X ) = npq\n표본비율의 표준오차는 다음과 같이 계산된다:\n이 식은 이항분포의 분산을 이용한 결과이다. ​​ 표본비율​의 분산 은 아래 식과 같다:\n표본비율의 표준오차는 이 분산의 제곱근으로 표현된다.\n\n\n5-2. 구간추정\n모비율 P 에 대한 구간추정을 하려면 P 의 추정량인 p̂ 의 분포를 알아야 한다.\nX 의 분포는 이항분포를 따르므로, 중심극한정리를 이용하여 표본 비율 p̂​ 의 분포를 근사적으로 정규분포로 취급할 수 있다:\n분모, 분자를 n으로 나누었을 때, 위와 같은 식이 나온다.\n위 구간이 p를 포함할 확률이 ( 1 − α ) 가 됨을 알 수 있다.\n\n\n5-3. 신뢰구간\np 를 추정량인 p̂ 으로 대체하면, 원하는 신뢰구간을 구할 수 있다.\n\n\n5-4. 표본크기의 결정\n오차가 d 이하가 될 확률이 최소한 100 ( 1 − α )% 가 되려면:\n표본 크기가 클 경우, 정규분포를 이용하여 위와 같은 식을 구할 수 있다. 따라서, 표본 크기는 아래 식을 만족해야 한다:\n\n\n5-5. 모비율 p에 대한 검정\n표본 크기가 클 때 모비율 p 에 대한 가설 H ₀ : p = p ₀ 을 검정하기 위한 검정통계량은 다음과 같다:\n검정통계량은 H ₀ 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 13의 ( 1 ) height 에 대한 요약 통계량 계산하기.\n\nxbar_h = np.mean(height);print(xbar_h) # 평균\n\nvar_h = np.var(height, ddof=1);print(var_h) # 분산 (자유도 1 사용)\n\nsd_h = np.std(height, ddof=1);print(sd_h) # 표준편차 (자유도 1 사용)\n\nmedian_h = np.median(height);print(median_h) # 중앙값\n\nmin_h = np.min(height);print(min_h) # 최솟값\n\nmax_h = np.max(height);print(max_h) # 최댓값\n\nsum_h = np.sum(height);print(sum_h) # 합계\n\nn = height.size;print(n) # 데이터 개수\n\n160.2\n35.88965517241378\n5.990797540596225\n161.0\n150\n170\n4806\n30\n\n\n예제 13의 ( 2 ) 신뢰구간을 구하기 위해, 아래 2가지 함수를 사용한다.\n추정량의 표준오차를 구하는 함수. 정규분포의 백분위수 함수.\n\nfrom scipy import stats \n\nse_h = stats.sem(height);print(se_h) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.05 / 2); print(z_alpha) # 95% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_h;print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_h - interval, xbar_h + interval]; print(CI) # 신뢰구간\n\n1.0937649834770078\n1.959963984540054\n2.1437399751659827\n[np.float64(158.05626002483402), np.float64(162.34373997516596)]\n\n\n이로부터 95% 신뢰구간은 160.2 ± 2.1437 임을 알 수 있다.\n예제 14 검정하고자 하는 가설은 H ₀ : μ = 159 대 H ₀ : μ ≠ 159 이며, 표본의 크기는 30 이상이다.\n\nzval = (xbar_h-159)/se_h;print(zval) # 가설 검정을 위한 z값\n\npval = 2 * (1 - stats.norm.cdf(zval));print(pval) # p값\n\n## 해석: p값이 크므로 귀무가설을 기각할 수 없음\n## → 평균키가 159cm와 통계적으로 유의한 차이가 없다고 본다\n\n1.097127827392377\n0.27258551722126834\n\n\nP–값: 실제 계산값과 약간의 차이는 있으나, 거의 비슷한 값이 나오는 것을 확인할 수 있다. (0.2714 ≈ 0.2725)\n예제 15의 ( 1 ) 신뢰구간의 신뢰수준이 98% 이므로 α = 200 을 이용하여, 표준오차와 Z α / 2 를 계산하고, 이로부터 오차범위값과 모집단 μ 에 대한 98% 신뢰구간을 구한다.\n\nxbar_n = np.mean(noise);print(xbar_n) # 평균\n\nsd_n = np.std(noise,ddof=1);print(sd_n) # 표준편차 (자유도 1 사용)\n\nn = noise.size;print(n) # 데이터 개수\n\n61.373999999999995\n4.780137902544961\n50\n\n\n검정하고자 하는 가설은 H ₀ : μ = 60 대 H ₁ : μ &gt; 60 이며, 표본의 크기는 30 이상이다.\n\nse_n = stats.sem(noise);print(se_n) # 표본표준오차\n\nz_alpha = stats.norm.ppf(1 - 0.02 / 2); print(z_alpha) # 98% 신뢰구간을 위한 z값\n\ninterval = z_alpha * se_n; print(interval) # 신뢰구간 계산을 위한 간격 계산\n\nCI = [xbar_n - interval, xbar_n + interval]; print(CI) # 신뢰구간\n\n0.6760135851792765\n2.3263478740408408\n1.5726427667045366\n[np.float64(59.801357233295455), np.float64(62.946642766704535)]\n\n\n\nzval = (xbar_n-60) / se_n;print(zval) # 가설 검정을 위한 z값\n\npval = stats.norm.sf(np.abs(zval));print(pval) # p값\n\n## 해석: P—값이 0.021로 0.05보다 작게 나왔으므로 \n## 유의수준 5% 에서 귀무가설을 기각할 수 있다.\n\n## 그러므로, 평균교통소음정도가 60 보다 크다고 할 수 있다.\n\n2.032503532655509\n0.021051353256926374\n\n\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "IDA/IDA12.html",
    "href": "IDA/IDA12.html",
    "title": "12장: 두 모집단의 비교",
    "section": "",
    "text": "Reporting Date: July. 31, 2024\n두 모집단의 비교를 위한 추론과정은 자료를 어떻게 수집하느냐에 따라 추론 방법이 달라진다. 대표적인 두 종류의 자료수집과정에 따른 추론방법을 다루고자 한다."
  },
  {
    "objectID": "IDA/IDA12.html#통계용어",
    "href": "IDA/IDA12.html#통계용어",
    "title": "12장: 두 모집단의 비교",
    "section": "2. 통계용어",
    "text": "2. 통계용어\n비교 연구 시 자주 사용되는 통계용어.\n실험단위 ( Experimental Unit ) : 실험의 대상. 반응값 ( Response ) : 실험 후 얻어지는 수치. 처리 ( Treatment ) : 비교하고자 하는 특성.\n예제12를 위 통계용어로 다음과 같이 설명할 수 있다:\n숲 지역 ⇨ 처리 1 , 도시 지역 ⇨ 처리 2 각각의 스캐너 측정값 ⇨ 실험단위\n그 측정값의 수치 ⇨ 반응값"
  },
  {
    "objectID": "IDA/IDA12.html#두-개-의-독-립-표-본",
    "href": "IDA/IDA12.html#두-개-의-독-립-표-본",
    "title": "12장: 두 모집단의 비교",
    "section": "3. 두 개 의 독 립 표 본",
    "text": "3. 두 개 의 독 립 표 본\n독립인 두 개의 표본으로부터 두 모집단, 혹은 두 가지의 처리효과를 비교하는 통계추론의 방법. 다음은 두 모집단으로부터 추출된 표본과 그로부터 계산되는 통계량을 정리한 것이다: 두 모집단으로부터 추출된 표본.\n위 표본으로부터 계산되는 통계량. 여기서 우리의 관심사는 두 모집단의 평균 반응값의 차이다.\n\n3-1. 모평균의 차에 대한 추론\n두 모평균의 차 ( μ 1 – μ 2 ) 에 대한 추론을 위해서는 두 표본평균의 차 ( x̄ – ȳ )를 이용한다. 두 표본의 크기 n 1, n 2 가 모두 큰 경우 ( 30 이상 )중심극한정리에 의해 두 표본평균은 근사적으로 정규분포를 따른다:\n평균이 μ​ 1 ± μ 2​ 이고, 분산이 σ 12 ​+ σ 22 ​인 정규분포를 따른다:\n이때, X 와 Y 는 서로 독립이다. 복호동순 : 식에서 부호를 2 개 이상 사용할 때, 부호를 앞에서부터 같은 순서로 적용하는 것. 따라서, 두 독립적인 정규분포 변수의 차를 표준화하면 표준정규분포를 따르게 된다: 위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다: 추정량 ± (z값) × (추정된 표준오차)\n두 표본의 크기 n 1, n 2 가 모두 30 이상일 때, 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n델타( Δ , δ: 그리스 알파벳의 네번째 글자)\n검정통계량은 H 0 이 맞을 때 N ( 0 , 1 ) 을 따른다. 각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-2. 모평균의 차에 대한 추론\n표본의 크기가 작을 경우, 두 모집단에 대하여 정규분포와 표준편차에 대한 가정이 필요하다.\n두 모집단이 모두 정규분포를 따른다. 두 모집단의 표준편차가 일치한다.\n값이 ½보다 작거나 2보다 큰 경우, 위 가정이 적절하지 못한다고 판단한다. 대부분의 경우, σ 를 모르므로 이를 추정하여야 한다.\nσ 에 대한 정보는 편차제곱합에 모두 들어 있다. 따라서 이 두 제곱합을 더하여 각각의 자유도의 합으로 나누어 σ 2 추정량으로 사용하게 된다.\n이를 공통분산 σ 2 의 합동추정량 ( Pooled Variance ) 이라 한다:\n위 식을 이용하여, ( μ 1 – μ 2 ) 에 대한 표준화된 확률변수는 다음과 같다:\n자유도가 ( n ₁ + n ₂ – 2 )인 t 분포를 따른다.\n위 분포를 바탕으로 ( μ 1 – μ 2 ) 에 대한 신뢰구간은 다음과 같다:\n( μ ₁ – μ ₂ ) 에 대한 신뢰구간은 추정량 ± ( t 값 ) × ( 추정된 표준오차 )의 형식에 의해 정리된다.\n두 모집단이 모두 정규분포를 따르고 두 모표준편차가 같은 때 가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n\n\n3-3. 모평균의 차에 대한 추론\n표본의 크기가 작고, 두 모집단의 표준편차가 일치하지 않을 경우, 근사적으로 t 분포를 따르며, 자유도는 ( n 1 – 1 )과 ( n 2 – 1 ) 중 작은 값이다.\n이 분포를 이용한 ( μ 1 – μ 2 ) 에 대한 추론방법은 다음과 같다:\n모평균 차에 대한 100 ( 1 − α ) % 신뢰구간은 근사적으로 위 식과 같다.\n신뢰구간의 경우, 그 구간이 넓어지는 경향이 있다. 따라서 실제 신뢰도는 100 ( 1 − α ) % 이상이 된다.\n가설 H 0 : μ 1 – μ 2 = δ 0 에 대한 검정통계량은 다음과 같다:\n검정의 경우, 기각역이 좁아지는 경향이 있다. 따라서 실제 유의수준이 α 이하가 되므로 귀무가설을 기각하지 못할 가능성이 높아진다."
  },
  {
    "objectID": "IDA/IDA12.html#짝비교",
    "href": "IDA/IDA12.html#짝비교",
    "title": "12장: 두 모집단의 비교",
    "section": "4. 짝비교",
    "text": "4. 짝비교\nMatched Pair Comparisons 실험 단위들이 비슷해야 한다는 점과 다양한 실험 단위들을 비교해야 한다는 점을 절충하는 접근법.\n짝지워서 각각의 쌍으로 만드는 방법:\n같은 쌍의 실험단위들은 서로 비슷해야 한다. 각 쌍 내에서 두 조건이 다르게 설정되어야 한다.\n짝 비교를 시행할 때의 자료의 형태:\n차의 표본평균과 분산은 다음과 같다:\nX i 와 Y i 는 서로 독립이 아니다. 서로 높은 상관관계를 가질 경우, 짝비교의 효과는 크다. 즉, 전체적인 변화의 폭 ( 변동성 ) 을 줄여 처리효과를 알아내기 수월한 짝비교를 할 수 있다.\n모평균 δ 에 대한 100 ( 1 − α ) % 신뢰구간은 다음과 같다:\n귀무가설 H 0 : δ = δ 0 에 대한 검정통계량은 다음과 같다:\nH ₀ 가 맞을 때 자유도가 ( n – 1 )인 t 분포를 따른다.\n자료가 짝지워져 있는 경우,두 개의 처리를 어떻게 배정할 것인가가 전혀 문제되지 않는다. 자료가 짝지워져 있지 않은 경우, 여러 조건들이 확률적으로 같은 정도로 영향이 미치도록 해야 한다. (어느 한쪽의 처리에만 영향을 주지 않아야 한다.)이와 같이 무작위로 배정하는 것을 랜덤화 ( Randomization ) 라고 한다."
  },
  {
    "objectID": "IDA/IDA12.html#두-모비율의-차에-대한-추론",
    "href": "IDA/IDA12.html#두-모비율의-차에-대한-추론",
    "title": "12장: 두 모집단의 비교",
    "section": "5. 두 모비율의 차에 대한 추론",
    "text": "5. 두 모비율의 차에 대한 추론\n두 모집단의 비율을 비교하는 추론하는 방법. 관심의 대상이 되는 어떤 특성의 모집단 1 의 비율을 p 1, 모집단 2 의 비율을 p 2 라고 할 때 두 모집단으로부터 크기가 n 1, n 2 인 표본을 추출하여 각각 특성이 A 인 것과 A 가 아닌 것으로 분류하였다고 가정한다.\n이때 A 를 ’ 성공 ‘, A 가 아닌 것을’ 실패 ’ 라고 하고 두 표본의 성공의 개수를 각각 X, Y 표현한다.\n두 모집단의 특성 A 의 비율을 각각 p 1, p 2 라고 하면 그 추정량은 각 표본으로부터 표본의 비율을 사용하게 된다:\n두 모비율의 차 ( p ₁ – p ₂ ) 의 추정량은 ( p̂ ₁ – p̂ ₂ ) 이 된다. ( p 1 – p 2 ) 에 대한 추론을 하기 위해서는 ( p̂ 1 – p̂ 2 ) 의 분포를 알아야 한다.\n표본의 크기 n 1, n 2 가 큰 경우, 아래 식이 근사적으로 성립한다:\n아래 식도 표본이 서로 독립이므로 정규분포로 근사된다:\n따라서 이를 표준화하면:\n두 확률변수 간의 차이를 표준화하는 것이다.\n위 식을 이용하여 ( p 1 – p 2 ) 에 대한 ( 1 − α ) % 신뢰구간은 다음과 같다:\n( 추정량 ) ± ( z 값 ) × ( 표준오차 )의 형식에 의해 정리된다.\n신뢰구간을 계산할 때, ( 제곱근 속의 ) 실제 모평균 차이 p 1, p 2 는 미지수이므로, 이를 표본 비율의 차이 p̂ 1, p̂ 2 로 대체하여 계산한다.\n\n5-1. 두 모비율의 검정\n표본의 크기가 클 때 두 모비율이 같은지를 검정하는 방법은 두 비율의 차이에 대한 검정을 통해 이루어진다.\n귀무가설 H 0 : p = p 0 을 검정하기 위해 (​ p̂ 1 – p̂ 2 ) 을 이용하게 된다. 이 가설이 맞을 경우의 통계량 분포는 다음과 같다:\np 는 모비율이 같은 두 모집단의 공통비율이다.\n통합된 두 표본으로부터 이를 추정하면:\n위 과정을 바탕으로 검정통계량을 정리할 수 있다:\np̂ 는 귀무가설하에서의 공통비율 p 의 추정량이고, 검정통계량은 H 0 가 맞을 때 근사적으로 N ( 0, 1 ) 을 따른다.\n각 대립가설에 대하여 유의수준 α 를 갖는 기각역은 다음과 같다:\n예제 12의 ( 1 ) 추정량의 표준오차 ( se ) 를 계산하기 위해 두 변수의 요약통계량 ( s 12, s 22 ) 을 구하는 파이썬 함수를 사용할 수 있다.\n\nvar1 = np.var(x, ddof=1);print(var1) # x의 표본 분산 (자유도=1)\nvar2 = np.var(y, ddof=1);print(var2) # y의 표본 분산 (자유도=1)\n\nn1 = len(x);print(n1) # x의 데이터 수\nn2 = len(y);print(n2) # y의 데이터 수\n\nse = math.sqrt(var1 / n1 + var2 / n2);print(se) # 표준오차\n\n48.06374040272343\n24.789102564102567\n118\n40\n1.0134334699544658\n\n\n표준오차 계산 ( 1.0134 )\n예제 12의 ( 2 ) 모평균의 차 μ 1 – μ 2 에 대한 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 z–값\nz_alpha = stats.norm.ppf(1 - 0.05 / 2);print(z_alpha)\ninterval_z = z_alpha * se; print(interval_z) # 신뢰구간 범위\n\n# x와 y의 평균을 계산\nxbar1 = np.mean(x); print(xbar1) \nxbar2 = np.mean(y); print(xbar2) \n\n# 두 평균의 차이를 계산\ndiff = xbar1 - xbar2; print(diff) \n\n# 신뢰구간\nCI_1 = [diff - interval_z, diff + interval_z]; print(CI_1)\n\n\n## 해석: 95%의 신뢰구간이 0을 포함하지 않으므로 (양측검정에서) \n## 유의수준 5%에서 두 수치가 같다는 귀무가설 (H₀ : μ1 = μ2) 을 기각할 수 있다.\n\n1.959963984540054\n1.986293101838208\n92.9322033898305\n82.075\n10.857203389830502\n[np.float64(8.870910287992295), np.float64(12.84349649166871)]\n\n\n스캐너 자료 μ ₁ – μ ₂ 의 신뢰구간은 10.857 ± 1.986 이다.\n예제 12의 ( 3 ) 두 모집단이 모두 정규분포를 따르고, 분산이 같다는 가정 하에서 합동분산추정량 및 추정량의 표준오차를 구하라.\n\n# 합쳐진 분산 계산\nspooled = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2); print(spooled) \n\n# 합쳐진 표준 오차 계산\nse_spooled = math.sqrt(spooled) * math.sqrt(1 / n1 + 1 / n2); print(se_spooled)\n\n42.24508094306821\n1.1891745810061622\n\n\n합동추정량 ( 42.245 ) / 표준오차 ( 1.189 )\n예제 12의 ( 4 ) 위 결과를 바탕으로 t – 분포의 백분위수 함수를 이용하여 95% 신뢰구간을 구하라.\n\n# 신뢰수준 95%에 해당하는 t–값\nt_alpha = stats.t.ppf(1 - 0.05 / 2, n1 + n2 - 2); print(t_alpha) \n\n# 신뢰 구간의 범위\ninterval_t = t_alpha * se_spooled; print(interval_t) \n\n# 신뢰구간\nCI_2 = [diff - interval_t, diff + interval_t]; print(CI_2)\n\n## 해석: (2)번의 정규분포를 이용한 신뢰구간보다 \n## 오차범위 값인 interval_t가 더 크므로 신뢰구간이 더 넓어졌음을 알 수 있다.\n## 이는 t–분포의 백분위수 값이 (정규분포의 백분위수 값보다) 더 크기 때문이다.\n\n1.9752875076954723\n2.3489616943304696\n[np.float64(8.508241695500033), np.float64(13.206165084160972)]\n\n\n신뢰구간은 10.857 ± 2.348 이다.\n\n교제: 통계학: 파이썬을 이용한 분석"
  },
  {
    "objectID": "TM/TM01.html",
    "href": "TM/TM01.html",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "",
    "text": "Reporting Date: March. 11, 2025\n웹 크롤링 개념 및 정적 크롤링 실습에 대해 다루고자 한다."
  },
  {
    "objectID": "TM/TM01.html#정형-데이터",
    "href": "TM/TM01.html#정형-데이터",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "1 . 정형 데이터",
    "text": "1 . 정형 데이터\n(Structured Data)\n일정한 형식을 갖춘 데이터로, 데이터베이스의 테이블처럼 행과 열로 정리된다.\n예: 엑셀, SQL 데이터베이스, 고객 정보(이름, 나이, 주소 등).\n룰세팅(Rule Setting) 데이터를 저장할 때 고정된 형식(테이블, 행/열 구조, 스키마 등)을 미리 정의하는 것.\n2 . 비정형 데이터 (Unstructured Data)\n형식이 일정하지 않아 체계적으로 저장하기 어려운 데이터.\n예: 텍스트(SNS 게시글, 이메일), 이미지, 동영상, 음성 데이터.\n주로 자연어 처리(NLP)나 텍스트 마이닝 등의 기법을 활용해 분석.\n3 . 반정형 데이터 (Semi-Structured Data)\n일정한 구조를 가지지만 완전히 정형화되지 않은 데이터. 태그나 특정한 형식(XML, JSON 등)을 포함하여 구조화가 가능하다.\n예: HTML, JSON, XML 파일, 로그 데이터.\n02 크롤링\n1 . 정적 크롤링 웹 페이지의 HTML 소스 코드를 직접 가져와서 필요한 데이터를 추출하는 방식.\n기본적으로, requests 라이브러리로 웹 페이지 HTML을 가져와 BeautifulSoup으로 데이터 추출한다.\n페이지 로딩 속도가 빠르고, 서버 부하가 적으나 JavaScript로 생성되는 데이터는 가져올 수 없다.\nHTML만으로 필요한 정보를 얻을 수 있다면 → 정적 크롤링이 유리하다.\n2 . 동적 크롤링 웹 브라우저를 실제로 실행하여 JavaScript로 로드되는 데이터까지 가져오는 방식.\n기본적으로, Selenium이나 Playwright 같은 브라우저 자동화 도구 사용한다.\nJavaScript 렌더링된 데이터를 포함하여 크롤링 가능하나, 속도가 느리고, 서버 부하가 높다.\nJavaScript로 데이터가 동적으로 로딩된다면 → 동적 크롤링이 필요하다.\n03 라이브러리 Jupyter Notebook에서 실행하는 명령어는 기본적으로 일반적인 Python 실행 환경에서도 동일하게 사용할 수 있다.\n(예: 터미널, 명령 프롬프트, 다른 IDE)\n1 . requests 파이썬에서 HTTP 요청을 보내고 응답을 받을 수 있는 라이브러리로, 주로 웹에서 데이터를 가져오거나 서버에 데이터를 전송하는 데 사용된다.\n웹사이트와 데이터를 주고받는 과정에서 사용되는 HTTP 프로토콜을 쉽게 다룰 수 있도록 도와준다.\n① GET 요청 - requests.get() 웹 페이지의 정보를 가져올 때 사용된다. 이는 브라우저에서 주소를 입력하고 페이지를 여는 것과 같은 동작이다.\n② POST 요청 - requests.post() 서버에 데이터를 전송할 때 사용된다. 회원가입, 로그인, 데이터 저장 등의 작업에서 활용된다.\n③ JSON 응답 처리 - response.json() 서버에서 JSON 형식의 데이터를 받으면, .json() 메서드를 사용하여 딕셔너리로 변환할 수 있다.\n2 . BeautifulSoup4 HTML/XML 문서를 파싱하여 원하는 데이터를 추출하는 라이브러리로, 문서를 구성하는 요소를 개별적인 구조(태그, 속성, 텍스트 등)로 나눈다.\n먼저, HTML 문서를 파싱하여 태그 간의 계층을 이해할 수 있는 트리 구조로 변환한다.\n이를 통해 특정 태그나 클래스에 접근할 수 있으며, CSS 선택자를 활용하여 원하는 요소를 쉽게 선택할 수 있다.\n또한, get_text() 메서드를 사용하면 태그 내부의 텍스트만 추출할 수 있어 데이터 정제 작업이 용이하다.\n3 . selenium 웹 브라우저를 자동으로 제어하는 라이브러리로, 클릭, 입력, 스크롤 등의 동작을 수행할 수 있다.\nJavaScript로 동적으로 변경되는 웹 페이지의 데이터도 가져올 수 있어 정적인 크롤링 방식보다 더 유연하다.\n이를 사용하려면 Chrome, Firefox 등 웹 브라우저에 맞는 드라이버가 필요하며, 이를 통해 실제 브라우저를 실행하고 조작할 수 있다.\n과거에는 웹 브라우저와 드라이버의 버전이 맞아야 했지만, 현재는 자동 업데이트 기능 덕분에 큰 문제가 없다.\n4 . pandas 데이터 분석 및 처리를 위한 필수 라이브러리로, CSV, Excel, JSON 등의 다양한 형식의 데이터를 데이터프레임으로 불러와 조작할 수 있다.\n또한, 결측값을 처리하거나 특정 조건에 따라 데이터를 필터링하고 정렬하는 등 정리 작업이 가능하다.\n뿐만 아니라, 데이터를 그룹화하여 분석할 수 있는 groupby() 기능, 기초 통계를 확인할 수 있는 describe() 메서드,\n특정 연산을 적용할 수 있는 apply() 메서드 등을 제공하여, 보다 효과적인 데이터 분석을 지원한다.\n04 정적 크롤링 다음은 네이버 뉴스 기사에 대해 정적 크롤링을 수행하는 코드이다.\n\n설치된 라이브러리를 불러오는 과정."
  },
  {
    "objectID": "TM/TM06.1.html",
    "href": "TM/TM06.1.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "Reporting Date: May. 20, 2025\n\n감성분석에 대해 다루고자 한다.\n01 감성 사전 다운\n1 . KNU\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com import json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\n2 . KoreanSentimentAnalyzer\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com import pandas as pd\n\n\n파일 경로\nfile_path = r’C:-master-master.csv’\n\n\nCSV 파일 읽기\nsenti_df = pd.read_csv(file_path, encoding=‘utf-8’) # 또는 encoding=‘cp949’ senti_df.head()\nimport pickle\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\ndoc_topic과 comment_topic이 포함된 파일\nf = open(file_path + ‘topic_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\nKNU 감성사전을 이용해서 텍스트 데이터에 감정 점수를 부여하는 Python 스크립트입니다. 각 텍스트가 긍정적인지, 부정적인지, 중립적인지를 파악하기 위해 사용됩니다.\nimport json import pandas as pd from tqdm import tqdm\n\n\n감정분석 JSON 데이터 (KNU 감성사전) 불러오기\nfile_path = r’C:-master-master’\nwith open(file_path + r’_info.json’, encoding=‘UTF-8’) as json_file: sentiword = json.load(json_file)\n\n\n감성 단어 리스트 및 점수 초기화\ns_word = [] values = [] score = []\n\n\n평균 계산 함수\ndef average(lst): return sum(lst) / len(lst)\n텍스트에서 감성 단어 찾고 점수 계산\n\n\n감성 점수 계산\nfor word in tqdm(data[‘doc’]): temp_s_word = [] # 본문에서 가져옴 temp_value = []\nfor s in sentiword:\n    if s['word'] in word:\n        temp_s_word.append(s['word'])\n        temp_value.append(int(s['polarity']))\n\ns_word.append(temp_s_word)\nvalues.append(temp_value)\n\nif len(temp_value) &gt; 0:\n    score.append(average(temp_value))\nelse:\n    score.append(0)\n\n\n결과 삽입\ndata = data.assign(sentiword=s_word, values=values, score=score) data\n가상 공간 안에서만 있는 것, 이를 저장 함.\nimport pickle import pandas as pd\n\n\n저장\nfile_path = r’C:\\’ with open(file_path + “total_docs_KNU.pkl”, “wb”) as f: pickle.dump(data, f)\n\n\n불러오기\nwith open(file_path + “total_docs_KNU.pkl”, “rb”) as f: ff = pickle.load(f)\n\n\n데이터프레임 복원\ntotal_docs = pd.DataFrame() total_docs[‘doc’] = ff[‘doc’] total_docs[‘doc_token_noun’] = ff[‘doc_token_noun’] total_docs[‘doc_topic’] = ff[‘doc_topic’] total_docs[‘comment_topic’] = ff[‘comment_topic’] total_docs[‘sentiword’] = ff[‘sentiword’] total_docs[‘values’] = ff[‘values’] total_docs[‘score’] = ff[‘score’]\ntotal_docs\ndoc_token_noun의 모든 단어가 감성 단어가 아니다. 그들 중 감성 단어를 sentiword로 불러온 것.\nfrom wordcloud import WordCloud import matplotlib.pyplot as plt\n\n\n폰트 경로 (Windows용 예시 - 나눔고딕)\nfont_path = r”C:.otf”\n\n\n토픽 개수만큼 반복\nnum_topics = total_docs[‘doc_topic’].nunique()\nfor topic_num in range(num_topics): # 해당 토픽의 문서 필터링 topic_docs = total_docs[total_docs[‘doc_topic’] == topic_num]\n# 토큰 리스트를 하나로 합치기 (flatten)\nall_tokens = sum(topic_docs['doc_token_noun'], [])\n\n# 문자열로 변환 (공백으로 연결)\ntext = ' '.join(all_tokens)\n\n# 워드클라우드 생성\nwordcloud = WordCloud(font_path=font_path, background_color='white', width=800, height=400).generate(text)\n\n# 시각화\nplt.figure(figsize=(10, 8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(f\"Topic {topic_num} Word Cloud\", fontsize=16)\n이 코드는 감성 점수(score)를 기준으로 각 토픽(doc_topic)에 대해 감정 분포를 분류하고 있습니다.\nif score &gt; 0.3: # 긍정 elif -0.3 &lt;= score &lt;= 0.3: # 중립 else: # 부정 여기서 “0.3”과 “-0.3”이라는 기준은 사용자가 임의로 정한 값 (threshold)입니다. → 즉, 이 기준이 정해진 절대값이 아니라, → 분석 목적에 따라 조정해야 하는 값이에요.\n🧠 사용자가 결정해야 할 것들 score의 값 범위가 어떻게 구성되어 있는가? 감성 점수가 -2 ~ 2인지, -1 ~ 1인지, -5 ~ 5인지 먼저 확인해야 합니다. 0.3이 의미 있는 경계값인가? 감성 점수의 분포가 대부분 -0.1 ~ 0.1이라면, 0.3은 너무 높은 기준일 수 있습니다. 반대로 감성 점수 범위가 크다면, 0.3은 너무 낮은 기준일 수 있죠. 목표에 따라 기준이 달라질 수 있음 예를 들어: 마케팅 분석이라면 조금만 긍정적이어도 긍정으로 간주 감정 민감도 분석이라면 더 엄격한 기준 적용 필요\n코드에서 senti_0 = [0, 0, 0, 0, 0, 0] 의미 이 리스트는 특정 토픽(topic 0, topic 1 등)에 대한 감성 점수 분포를 저장하려고 만든 것입니다. 즉, 각 인덱스가 감정의 강도나 구간을 나타냅니다.\n0 매우 긍정 1 긍정 2 중립 3 부정 4 매우 부정 5 기타 / 미정 (혹은 사용자 정의) 그런데 본문 코드에서는 실제로는 0, 2, 4만 사용되고 있죠? 이건 중간 단계일 가능성도 있고, 나중에 더 구체적인 등급으로 확장하기 위해 미리 6칸 확보해둔 것일 수도 있어요.\n🧠 핵심 포인트 senti_0, senti_1은 토픽별 감정 분포를 담기 위한 그릇입니다. 토픽이 더 늘어나면? → senti_2, senti_3, … 식으로 계속 만들어야겠죠. 또는 다음처럼 딕셔너리 구조로 더 깔끔하게 관리할 수도 있어요: senti_dict = {i: [0, 0, 0, 0, 0, 0] for i in total_docs[‘doc_topic’].unique()} ✅ 요약 항목 설명\nsenti_0, senti_1 토픽별 감정 분포 저장용 리스트 길이 6 감정 강도 6단계로 분류하려는 구조 (0~5) 사용자 정의 가능 목적에 따라 개수, 의미를 직접 설정 필요하시면 이 구조를 자동화해서 n개의 토픽에 대해 감정 분포를 계산하는 코드도 만들어드릴 수 있어요. 원하시나요?\n긍정 / 중립 / 부정의 빈도와 비율을 토픽별로 확인하려는 거군요.\n각 토픽(topic)마다 긍정: score &gt; 0.3 중립: -0.3 ≤ score ≤ 0.3 부정: score &lt; -0.3 해당 범주의 빈도수와 비율(%)을 구하기\n감정 빈도 doc_topic negative neutral positive\n0 12 56 32 1 8 30 62\n감정 비율 (%) doc_topic negative neutral positive\n0 12.0 56.0 32.0 1 8.0 30.0 62.0\n\n\n토픽별 감성 점수 분류 리스트 초기화 (긍정, 중립, 부정)\nsenti_0 = [0, 0, 0, 0, 0, 0] senti_1 = [0, 0, 0, 0, 0, 0] senti_2 = [0, 0, 0, 0, 0, 0] senti_3 = [0, 0, 0, 0, 0, 0]\nfor i in range(len(total_docs)): topic = total_docs[‘doc_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_0[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_0[2] += 1\n    else:\n        senti_0[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_1[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_1[2] += 1\n    else:\n        senti_1[4] += 1\nfor i in range(len(total_docs)): topic = total_docs[‘comment_topic’].iloc[i] score = total_docs[‘score’].iloc[i]\nif topic == 0:\n    if score &gt; 0.3:\n        senti_2[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_2[2] += 1\n    else:\n        senti_2[4] += 1\n\nelif topic == 1:\n    if score &gt; 0.3:\n        senti_3[0] += 1\n    elif -0.3 &lt;= score &lt;= 0.3:\n        senti_3[2] += 1\n    else:\n        senti_3[4] += 1\n지금 작성하신 코드는 토픽별 감정 분포 리스트에서 비율(%)을 1칸씩 띄워서 저장하는 방식입니다.\n📦 구조 요약 senti_0 = [긍정_빈도, 긍정_비율, 중립_빈도, 중립_비율, 부정_빈도, 부정_비율]\n인덱스 0, 2, 4: 빈도수 (count) 인덱스 1, 3, 5: 비율 (ratio, 혹은 percentage) 🔁 반복문 설명 for i in range(1, 7, 2): # i는 1, 3, 5 i-1 → 현재 비율을 계산할 빈도 인덱스 i → 비율을 저장할 인덱스 분모는 전체 감정의 합: 긍정 + 중립 + 부정 즉, 예를 들어:\nsenti_0[1] = senti_0[0] / (senti_0[0] + senti_0[2] + senti_0[4]) 이건 긍정 비율, 그다음 senti_0[3]은 중립 비율, senti_0[5]는 부정 비율이 되는 식입니다.\n\n\n감성 클래스별 비율 계산 (분모가 0일 경우 예외 처리 추가)\nfor i in range(1, 7, 2): if (senti_0[0] + senti_0[2] + senti_0[4]) != 0: senti_0[i] = senti_0[i-1] / (senti_0[0] + senti_0[2] + senti_0[4]) else: senti_0[i] = 0 # 분모가 0이면 비율을 0으로 설정\nif (senti_1[0] + senti_1[2] + senti_1[4]) != 0:\n    senti_1[i] = senti_1[i-1] / (senti_1[0] + senti_1[2] + senti_1[4])\nelse:\n    senti_1[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_2[0] + senti_2[2] + senti_2[4]) != 0:\n    senti_2[i] = senti_2[i-1] / (senti_2[0] + senti_2[2] + senti_2[4])\nelse:\n    senti_2[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\nif (senti_3[0] + senti_3[2] + senti_3[4]) != 0:\n    senti_3[i] = senti_3[i-1] / (senti_3[0] + senti_3[2] + senti_3[4])\nelse:\n    senti_3[i] = 0  # 분모가 0이면 비율을 0으로 설정\n\n\n토픽별 감성 비율 데이터프레임 생성\ngraph = pd.DataFrame( [senti_0, senti_1, senti_2, senti_3], index=[‘topic1’, ‘topic2’, ‘topic3’, ‘topic4’], columns=[[‘긍정’, ‘긍정’, ‘중립’, ‘중립’, ‘부정’, ‘부정’], [‘빈도’, ‘비율’, ‘빈도’, ‘비율’, ‘빈도’, ‘비율’]] )\ngraph\n🔍 왜 워드클라우드와 감정 점수(혹은 분류) 결과가 다를 수 있는가? 1. 워드클라우드는 감성 단어 필터 없이 모든 단어 사용 일반적으로 워드클라우드는 특정 토픽에서 자주 등장한 단어의 빈도만을 시각화합니다. 이 과정에서 감성 사전에 없는 중립 단어, 불용어(의미 없는 단어)들도 포함될 수 있습니다. 따라서 시각적으로 중요한 단어처럼 보여도 감성 점수 계산에서는 무시될 수 있습니다. 2. KNU 감성사전 기반 감정 점수는 ’등록된 감성 단어’만 사용 예: 좋다, 싫다, 기쁘다, 화나다 등만 감성 점수로 환산됨. 감성 사전에 없는 단어는 아무리 많이 나와도 score에 기여하지 않음. 3. 토픽의 특성과 감성 단어 간 연관성 결여 예를 들어, 주제는 부정적인 사건이라도 직접적으로 부정 단어(예: “나쁘다”, “불편하다”)가 없을 수 있음. 이 경우 토픽 자체는 부정적으로 보이지만, 감성 점수는 중립 혹은 긍정이 나올 수 있습니다. 4. 토픽 내 감성 단어 비율이 낮은 경우 감성 점수를 계산할 때 사용하는 감성 단어 수가 전체 단어에 비해 매우 적다면, score의 분포도 좁거나 왜곡될 수 있습니다. 이로 인해 score는 0 근처로 몰리거나, 예외적으로 높은 감성 단어 하나에 과도하게 영향받을 수 있습니다. ✅ 요약 요소특징감성 점수에 반영됨? 워드클라우드 주요 단어 빈도가 높은 모든 단어 ❌ 감성 단어만 반영됨 감정 점수(score) 감성사전에 있는 단어 기반 ✅ 해당 단어만 반영됨 감정 판단 정확도 단어 수, 감성 단어 존재 여부에 민감 상황에 따라 다름\n💡 개선 팁 워드클라우드 만들 때 감성 단어만 필터링해서 시각화할 수도 있습니다. python 복사편집 sentiment_words = [s[‘word’] for s in sentiword] topic_words = [word for word in topic_docs if word in sentiment_words] 감성 점수 외에도 TF-IDF 기반 상위 감성 단어 추출도 좋은 방법입니다. 감성 점수 분포와 함께 워드클라우드 결과를 비교 분석하면 더 풍부한 인사이트를 얻을 수 있습니다.\n01 감성 사전 다운\n1 . NRC\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com\n이 코드는 NRC 감성사전 (Korean NRC Emotion Lexicon)을 기반으로 각 문서의 감정값을 계산하는 과정입니다. 간단히 말하면, 문서에 등장하는 감성 단어를 찾아서 해당 감정 점수를 누적하는 구조입니다.\n아래에 코드의 의미를 단계별로 설명드리겠습니다:\n🔢 코드 설명 for i in range(1, len(nrc)): # NRC 감성사전의 각 단어에 대해 반복 nrc: NRC 감성사전을 담은 DataFrame입니다. nrc[‘Korean Word’]: 감성사전에 있는 한국어 단어. range(1, len(nrc)): 아마 첫 번째 행(헤더 또는 불필요한 데이터)을 생략하고자 1부터 시작한 것 같습니다. if nrc[‘Korean Word’][i] in word: 현재 문서(word)에 감성사전의 단어가 포함되어 있는지 확인. if len(nrc[‘Korean Word’][i]) &gt; 1: 글자 수가 1자인 경우(ex. “다”, “게”)는 보통 의미가 불분명하거나 너무 일반적이라 제외. 따라서 두 글자 이상인 감성 단어만 사용. temp_s_word.append(nrc[‘Korean Word’][i]) 해당 감성 단어를 temp_s_word 리스트에 저장 (이 문서에서 발견된 감성 단어 목록). b = list(map(int, nrc.iloc[i, 1:11].tolist())) nrc.iloc[i, 1:11]: 해당 단어에 대한 감정 점수들 (예: 긍정, 부정, 분노, 기쁨 등 10가지 감정). map(int, …): 감정 점수들이 문자열로 되어 있다면 정수로 변환. 결과적으로 b는 해당 단어의 10개 감정 점수 리스트. temp_value = [x + y for x, y in zip(temp_value, b)] temp_value: 현재 문서에서 감정 점수를 누적하는 리스트. b를 더해가며 문서 전체의 감정 점수를 계산. 🔍 이 코드의 목적 NRC 감성사전 기반 다중 감정 분석입니다. 단순히 긍·부정 점수만 계산하는 것이 아니라, 여러 감정 카테고리(기쁨, 슬픔, 분노 등)의 누적 점수를 구해서 문서의 감정 프로파일을 생성합니다.\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM05.html",
    "href": "TM/TM05.html",
    "title": "5장: 텍스트 데이터 마이닝",
    "section": "",
    "text": "Reporting Date: April. 15, 2025\n\n텍스트 데이터 마이닝에 대해 다루고자 한다.\n\n01 텍스트 데이터 마이닝\n광도들이 보석을 캐는 과정.\n노인 부양에 관한 가설 세우기.\n텍스트 데이터 전처리\n텍스트 마이닝의 핵심적인 시작 단계로, 데이터의 품질을 높이기 위한 여러 과정으로 구성된다.\n먼저, 데이터 수집 후에는 한글화, 결측치 처리, 단어 및 형태소 분석 등의 전처리를 진행합니다.\n한글화는 텍스트에서 한글 이외의 문자를 제거하거나 블랭크 처리하여 분석에 적합한 형태로 만드는 과정입니다.\n이때 특수기호는 유지하며 한글만 남기는 방식으로 필터링한다.\n이렇게 정제된 데이터는 피클(pickle) 파일 형태로 저장하며, 작업 시에는 파일 경로와 파일명을 명확히 지정해야 한다.\n예를 들어, 보험연수원에서 제공한 연금 관련 텍스트 데이터를 수년간 6개 채널에서 크롤링해 5개의 피클 파일로 저장한 사례가 있다.\n이 파일들은 병합한 후 인덱스를 지정해 다시 저장하며, 저장 경로는 작업 환경에 맞춰 지정해야 한다.\nimport pickle import pandas as pd import itertools import os import re\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n이후 분석을 위해 저장된 피클 파일을 다시 로드하여 활용한다.\nf = open(file_path + ‘total_doc.pkl’, “rb”) # 데이터 불러오기 data = pickle.load(f) f.close()\ndata # 문서 전체의 명사 리스트 확보\n\n단어들의 빈도 데이터 정제 과정에서는 불필요한 기호나 단어를 제거하고, 결측값은 일괄 삭제하며 인덱스를 재정비합니다. 예컨대 ‘샵’, ’펀드’와 같은 특정 요소는 정제 대상이 되며, 본문 일부 삭제 시 데이터의 일관성을 유지하기 위해 인덱스를 재조정합니다. 정제는 원본을 복사한 후 진행하는 것이 안전합니다.\n\n형태소 분석은 한글 데이터 분석에 필수적인 과정이며, 이는 텍스트를 의미 단위로 나누어주는 작업입니다. 형태소 분석을 위해서는 Java 설치와 버전 확인, 인터넷 환경 설정이 필요하며, 대표적으로 사용하는 라이브러리는 코모란(Komoran)입니다. 코모란은 GitHub에서 설치 가능하며, 설치 후 환경 변수 설정 및 보안 설정 등을 완료한 후 사용합니다.\n형태소 분석을 통해 본문에서 추출된 단어들은 토큰화 과정을 거쳐 리스트 형태로 정리됩니다. 이때 불용어(의미 없는 단어)를 제거하기 위해 스탑워드 리스트를 활용하며, 불용어와 일치하는 형태소는 제외합니다. 최종적으로 정제된 단어 리스트와 형태소 리스트는 데이터프레임 형태로 저장하고, 이를 다시 파일로 변환하여 보관합니다.\nimport itertools\n\n\n제목 리스트 언패킹\ntitle_noun = list(itertools.chain(*data[‘title_token_noun’])) print(title_noun[:15]) # 앞에서 5개 요소 출력\n\n\n본문 리스트 언패킹\ndoc_noun = list(itertools.chain(*data[‘doc_token_noun’])) print(doc_noun[:15])\n\n\n댓글 리스트 언패킹\ncomment_noun = list(itertools.chain(*data[‘comment_token_noun’])) print(comment_noun[:15])\n\n제목, 본문, 댓글 데이터 빈도\n\n\n\n빈도를 카운트하는 라이브러리\nfrom collections import Counter\ntitle_count = Counter(title_noun) # 리스트 원소의 개수가 계산됨 title_top = dict(title_count.most_common(100)) # 상위 100개 출력하기 title_top\n#—\ndoc_count = Counter(doc_noun) # 리스트 원소의 개수가 계산됨 doc_top = dict(doc_count.most_common(100)) # 상위 100개 출력하기 doc_top\n\n\n—\ncomment_count = Counter(comment_noun) # 리스트 원소의 개수가 계산됨 comment_top = dict(comment_count.most_common(100)) # 상위 100개 출력하기 comment_top\n\nimport csv\n\n\n\n제목별 빈도수 저장\nwith open(file_path + ‘\\title_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in title_top.items(): w.writerow([k, v]) # k, v -&gt; 딕셔너리의 key, value # 즉, 단어와 빈도\n\n\n본문별 빈도수 저장\nwith open(file_path + ‘\\doc_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in doc_top.items(): w.writerow([k, v])\n\n\n댓글별 빈도수 저장\nwith open(file_path + ‘\\comment_top.csv’, ‘w’) as f: w = csv.writer(f) for k, v in comment_top.items(): w.writerow([k, v])\n4 . 워드 클라우드 정제된 단어들을 기반으로 워드 클라우드를 그릴 수 있다.\n워드 클라우드는 단어의 빈도를 시각적으로 표현하는 기법으로, 가장 자주 등장한 단어를 강조하는 방식으로 표현된다.\n이때 Counter의 most_common 함수를 이용해 빈도를 계산하고, 원하는 형태의 마스크 이미지(예: 사람 모양, 네모형 등)를 적용해 시각화할 수 있다.\n백그라운드 설정, 폰트 다운로드, 컬러 맵 지정 등 세부 설정도 가능하며, 정보 전달력이 높은 네모 형태를 권장한다.\nimport matplotlib.pyplot as plt from wordcloud import WordCloud\nfont_path = r”C:.otf”\n\n\n워드클라우드 생성\nwordcloud = WordCloud( font_path=font_path, background_color=‘white’, colormap=“Accent”, width=600, height=400 ).generate_from_frequencies(doc_top)\nplt.figure(figsize=(8, 10)) plt.imshow(wordcloud) plt.axis(‘off’) plt.show()\nTF - IDF 결과 텀은 단어, 단어들의 빈도, 워드클라우드도 사용 이것이 결합된 형태가 TF - IDF임\n단어 뿐만아니라 문서 전체도 고려 한다.\n\n워드 클라우드의 문제점, 본문에서 자주 등장하는 것이 높은 가중치를 문맥에 따라서는 다른 의미(완전히 다른 단어)를 가질 수 있음 또한, 그 단어는 낮은 가중치일 수도 있음\n\n즉, 데이터가 진짜 말하고자 하는 것을 찾는 인사이트에서는 부적합할 수 있음.\n단순한 빈도만으로는 판별하기 애매하다.\n\n문서의 길이의 따라 사용 어휘의 중요도가 바뀔 수 있다.\n\n특정 단어가 포함된 문서가 몇 개가 되느냐? d=문서, t=단어, 특정 단어가 나타나는 문서수의 역수(역수의 로그를 취해준 개념)\n각 문서에 포함된 단어 카운트 - DTM 행렬\n이는 특정 문서에서만 많이 나오지만 전체 문서에서는 적은 단어와 전체적으로 많이 나오게 분포하지만 개별 문서에서는 적게 나오는 단어 2가지가 존재하고 그 중 후자가 더 중요한 가중치를 가진다.\n로그를 취해서 소수점으로 나오고 TF-IDF를 곱한다. 이떄 여기저기 많이 나오면 상대적인 가중치가 비슷하고 낮게 나옴\n먼저, 단어들을 문자열로 만들어 주어야 한다. 명사들의 문자열 리스트 만들고, sklearn 가져오기.\n\n\n명사들의 문자열 구성\ndoc_noun = [] for i in range(0, len(data[“doc_token_noun”])): doc_noun.append(’ ’.join(data[‘doc_token_noun’][i])) # 각 문서의 명사들을 str로 연결\n너무 희박한 것들은 제외할 수도 있다.(최소치, 최대치)\n\n\n텍스트 문서 모음을 단어 tf-idf 행렬로 변환\nfrom sklearn.feature_extraction.text import TfidfVectorizer vec_y = TfidfVectorizer(min_df=0.01, max_df=0.95)\n\n\n문서의 1% ~ 95%로 나타나는 단어들을 고려\nY = vec_y.fit_transform(doc_noun) print(Y)\n10번째 문서 21번째 단어이다. +1\nk개의 평균을 갖는다는 것. 비지도, 타겟X 타겟이 있어, 예측을 시도하는 지도학습과는 달리 어떤 패턴을 가진 그룹이 있는지를 보려는 것.\n구조화, 군집 분석을 시도하는 것.\n임의의 k개의 중심점을 지정, 각각의 개별 데이터를 가장 가까운 곳으로 할당시킴 이 거리를 유클리드의 거리를 한다.\n그 그룹이 생성되면 그 그룹 안에서 새로운 중심점을 찾음 그 중심점을 가지고 위의 일련의 과정을 더 이상 중심점이 움직이지 않을 때까지 반복한다.\n합리적인 k를 찾는 방법 - 대표적으로 엘보우 기법 팔굽치 처럼 꺾이는 지점을 k값으로 정하는 것.\n2개에서 6개 정도가 타당하다 너무 적거나 많으면 의미가 없음.\n거리에 대한 SSE 손실함수 구하는 과정 10번 반복\nimport os os.environ[“OMP_NUM_THREADS”] = “2” # 선택 사항\nimport matplotlib.pyplot as plt from sklearn.cluster import KMeans\ndef elbow(X): sse = []\nfor i in range(1, 10):\n    km = KMeans(n_clusters=i, n_init=10, \n                algorithm='lloyd', random_state=0)\n    km.fit(X)\n    sse.append(km.inertia_)\n    print(i)\n\nplt.plot(range(1, 10), sse, marker='o')\nplt.xlabel('K')\nplt.ylabel('SSE')\nplt.xticks(range(1, 10))\nplt.show()\nelbow(X)\nconda install -c conda-forge pyldavis\nmodel_y = KMeans(n_clusters=2, algorithm=‘lloyd’, random_state=0) # 모델 정의 model_y.fit(Y) # 모델 학습\nprint(“Doc Top terms for each cluster”) order_centroids = model_y.cluster_centers_.argsort()[:, ::-1] # 클러스터 중심 정렬 terms_y = vec_y.get_feature_names_out() # 단어 목록\nfor i in range(2): # 두 개의 클러스터에 대해 반복 print(“Cluster %d:” % i) for ind in order_centroids[i, :50]: # 각 클러스터의 상위 50개 단어 출력 print(‘%s’ % terms_y[ind]) print(‘’)\n데이터 프레임의 형식\nimport pandas as pd\n\n\n클러스터 중심에서 가장 중요한 단어 인덱스 정렬\norder_centroids = model_y.cluster_centers_.argsort()[:, ::-1] terms_y = vec_y.get_feature_names_out()\n\n\n각 클러스터의 상위 50개 단어 수집\ntop_terms = {}\nfor i in range(2): # 클러스터 수만큼 반복 top_terms[f’Cluster {i}’] = [terms_y[ind] for ind in order_centroids[i, :50]]\n\n\nDataFrame으로 변환\ndf_top_terms = pd.DataFrame(top_terms) df_top_terms\n1 . 제목2 더 나아가 워드 클러스터링과 토픽 모델링을 통해 텍스트의 의미 구조를 분석할 수 있다.\n워드 클러스터링은 문서 내 단어 빈도를 기반으로 단어들을 군집화하는 방법으로, TF (Term Frequency) 및 IDF (Inverse Document Frequency) 값을 활용해 중요 단어를 판단합니다.\n이후 유클리디안 거리 기반의 K-means와 같은 알고리즘으로 최적의 군집을 형성합니다.\n토픽 모델링은 문서 집합에서 주제를 추출하는 기법으로, LDA(Latent Dirichlet Allocation) 같은 확률 기반 모델을 활용합니다.\n혼잡도 그래프와 일관성 지표 등을 통해 토픽 수를 결정하고, 각 토픽의 특징을 평가합니다. 이를 통해 시스템화된 분석 체계를 구축할 수 있습니다.\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM03.1.html",
    "href": "TM/TM03.1.html",
    "title": "3장: 네이버 카페 크롤링",
    "section": "",
    "text": "Reporting Date: April. 01, 2025\n\n네이버 카페 크롤랑에 대해 다루고자 한다.\n합칠려면 모든 변수가 동일하게 들어가야 한다.\nfrom selenium import webdriver # 브라우저 자동화 from bs4 import BeautifulSoup as BS # html 내용 파싱 from selenium.webdriver.common.by import By # 다양한 방법으로 엘리먼트를 찾기 from selenium.webdriver.common.keys import Keys # Keys 클래스 가져오기(키보드 입력 제어)\nimport pandas as pd # 데이터 조작 및 분석 import datetime # 날짜와 시간 연산 import requests # Http 요청을 보내기 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 속도 조절 import re # 정규 표현식 사용\n\n\n\ndriver = webdriver.Chrome() driver.get(‘https://search.naver.com/search.naver?ssc=tab.cafe.all&sm=tab_jum&query=%EB%85%B8%EC%9D%B8+%EB%B6%80%EC%96%91&nso=so%3Ar%2Cp%3Afrom20240301to20240325’)\n\n\n\ntitle_list = [] url_list = []\n\n검색 결과에서 모든 제목 링크 요소 가져오기 (스크롤 다운 포함)\nfor _ in range(2): # 5번 스크롤 내리기 (필요에 따라 조절 가능) driver.execute_script(“window.scrollTo(0, document.body.scrollHeight);”) # 스크롤 맨 아래로 이동 time.sleep(1) # 데이터 로딩을 기다리기 위해 1초 대기\ntitles = driver.find_elements(By.XPATH, “//*[@id='main_pack']/section/div[1]/ul/li/div/div[2]/div[2]/a”)\nfor i, title_element in enumerate(titles, start=1): # 1부터 카운트 시작 try: title_list.append(title_element.text) # 제목 추가 url_list.append(title_element.get_attribute(“href”)) # URL 추가\nexcept Exception as e:\n    print(f\"오류 발생: {e}\")  # 오류 메시지 출력\n\nif i % 10 == 0:  # 진행 상황 출력 (10개 단위)\n    print(f\"진행 중: {i}개 완료\")\nprint(“데이터 수집 완료!”) # 최종 완료 메시지 출력\n\n\n\n\n\n본문, 좋아요 수, 댓글 수, 댓글, 이미지 수, 영상 수를 저장할 리스트 초기화\nnew_doc = []\nlike_cnt = []\ncomment_cnt = []\ncomment_list = []\nimg_cnt = []\ndiv_cnt = []\n\n\n카페 글 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(f”window.open(‘{url_path}’)“) # 새 탭에서 URL 실행 driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(2)  # 2초 대기\n\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, 'iframe')  # 카페 iframe 찾기\n    \n    if len(iframes) &gt; 0:\n        # iframe 전환\n        driver.switch_to.frame('cafe_main') # ifame의 첫부분\n        html = driver.page_source           # html 가져오고\n        soup = BS(html, 'html.parser')      # html 파싱하라\n\n        # 본문 추출\n        try:\n            a = soup.find('div', class_='article_viewer').get_text() # 값을 가져와라\n        except:\n            # 본문을 찾지 못할 경우\n            a = 'null'\n\n        # 좋아요 수 추출\n        try:\n            b = soup.find('em', class_='u_cnt _count').get_text()\n        except:\n            b = 'null'\n\n        # 댓글 수 추출\n        try:\n            c = soup.find('strong', class_='num').get_text()\n        except:\n            c = 'null'\n\n        # 댓글 추출\n        try:\n            d = \"\\n\".join([t.get_text() for t in soup.find_all('span', class_='text_comment')])\n        except:\n            d = 'null'\n\n        # 이미지 수 추출\n        e = len(soup.find_all('img', class_='se-image-resource'))\n\n        # 영상 수 추출\n        f = len(soup.find_all('div', class_='pzp-ui-dimmed pzp-dimmed pzp-pc_dimmed'))\n\n        # iframe에서 기본 컨텐츠로 전환\n        driver.switch_to.default_content()\n    else:\n        a, b, c, d, e, f = 'null', 'null', 'null', 'null', 0, 0  # iframe이 없을 경우 기본값\n\n    # 데이터 저장\n    new_doc.append(a)\n    like_cnt.append(b)\n    comment_cnt.append(c)\n    comment_list.append(d)\n    img_cnt.append(e)\n    div_cnt.append(f)\n\nexcept Exception as e:\n    # 오류 발생 시 기본값 저장\n    new_doc.append('null')\n    like_cnt.append('null')\n    comment_cnt.append('null')\n    comment_list.append('null')\n    img_cnt.append(0)\n    div_cnt.append(0)\n    print(f\"Error occurred at index {i}\")\n\nfinally:\n    # 현재 열린 탭 닫기\n    driver.close()\n    time.sleep(0.3)  # 0.3초 대기\n    driver.switch_to.window(driver.window_handles[0])  # 첫 번째 탭으로 복귀\n\n# 매 10번째 URL마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"진행 상황: {i + 1}/{len(url_list)}\")\n\n\n\n\n\n크롤링 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame() # 초기화 raw_data[‘title’] = title_list # 제목 리스트 raw_data[‘doc’] = new_doc # 본문 리스트 raw_data[‘like’] = like_cnt # 좋아요 수 리스트 raw_data[‘comment_cnt’] = comment_cnt # 댓글 수 리스트 raw_data[‘comment_list’] = comment_list # 댓글 리스트 raw_data[‘img’] = img_cnt # 이미지 수 리스트 raw_data[‘div’] = div_cnt # 영상 수 리스트 raw_data[‘ch’] = ‘naver’ # 채널 정보 raw_data[‘ch2’] = ‘cafe’ # 채널 정보 (세부)\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양cafe.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양cafe.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양cafe.csv”, index=False, encoding=“utf-8-sig”)\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM03.0.html",
    "href": "TM/TM03.0.html",
    "title": "3장: 네이버 블로그 크롤링",
    "section": "",
    "text": "Reporting Date: March. 18, 2025\n\n동적 크롤링을 위한 준비 및 네이버 블로그 크롤링 실습에 대해 다루고자 한다.\n01 자바 설치 방법\n1 . 파이썬과 자바의 관계 일반적으로 파이썬은 자바 없이 독립적으로 실행할 수 있다.\n하지만, 특정 라이브러리(예: JPype, PySpark, Jython 등)는 자바(Java)를 필요로 한다.\n따라서 사용하려는 기능이 자바 기반이라면, 먼저 자바가 설치되어 있어야 한다.\n2 . 자바 설치 여부 확인 Anaconda 프롬프트 실행한 다음 명령어 입력 후 실행.\n\n자바가 설치되어 있다면 버전 정보가 출력됨.\n\n\n“java is not recognized…” 오류가 발생하면 자바가 설치되지 않은 것임.\njava -version\n3 . 내 컴퓨터에 자바 설치하기 Oracle 공식 홈페이지에서 JDK 다운로드 설치 후, 환경 변수를 설정해야 한다.\nDownload the Latest Java LTS Free\nSubscribe to Java SE and get the most comprehensive Java support available, with 24/7 global access to the experts.\nwww.oracle.com\n환경 변수 설정 (Windows 기준)\n제어판 → 시스템 및 보안 → 시스템 → 고급 시스템 설정 고급 탭 → 환경 변수 버튼 클릭 시스템 변수에서 “새로 만들기” 클릭 변수 이름: JAVA_HOME 변수 값: C:Files-XX.X.X (설치된 JDK 경로 입력) Path 변수 편집 → ;%JAVA_HOME%추가\nAnaconda 프롬프트 또는 명령 프롬프트에서 다시 입력하여 정상적으로 출력되는지 확인한다.\n02 Selenium을 사용한 동적 크롤링\n1 . Selenium 설치 웹 브라우저에서 동적 크롤링 시 가장 많이 사용하는 패키지.\n과거에는 웹드라이버 버전에 맞는 경로를 지정해줘야 했지만, 현재는 패키지의 새버전에 의해 자동적으로 맞춰진다.\npip install selenium\n2 . 웹드라이버 다운로드 사용하는 브라우저에 맞는 WebDriver를 다운로드해야 한다.\nChrome 다운로드 및 설치 - 컴퓨터 - Google Chrome 고객센터\n도움이 되었나요? 어떻게 하면 개선할 수 있을까요? 예아니요\nsupport.google.com 다운로드한 WebDriver를 실행 파일 경로에 두거나, Python 코드에서 직접 경로를 지정해야 한다.\n03 네이버 블로그 크롤링\n\n라이브러리 불러오기.\n\nfrom selenium import webdriver # 웹 브라우저 자동화 from bs4 import BeautifulSoup as BS # HTML 및 XML 파싱\nimport pandas as pd # 데이터 조작 및 분석 import requests # HTTP 요청을 보내기 위한 모듈 import datetime # 날짜 및 시간 연산 import pickle # 파이썬 객체 직렬화 import time # 코드 실행 간격 조절 import re # 정규 표현식을 사용하여 문자열 처리\n\n\nSelenium에서 다양한 방법으로 HTML 요소를 찾기\nfrom selenium.webdriver.common.by import By\n\nSelenium을 이용한 네이버 블로그 검색 자동화.\n\n\n\n크롬 드라이버 실행\ndriver = webdriver.Chrome()\n\n\n네이버 블로그 검색 페이지로 이동\n\n\n검색할 키워드 지정 및 데이터 수집기간 설정한 뒤\n\n\n복사한 URL을 붙여 넣으면 되며, 아래 코드는 가독성을 위해 일부러 줄바꿈을 시도함\ndriver.get(’’’ https://search.naver.com/search.naver? ssc=tab.blog.all&query=%EB%85%B8%EC%9D%B8%20%EB%B6%80%EC%96%91 &sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20240301to20240325’’’.replace(“”, ““))\nURL 가져오는 방법\n실행 화면\n\n웹 페이지 자동 스크롤 함수.\n\ndef doScrollDown(whileSeconds): start = datetime.datetime.now() # 스크롤 다운 시작 시간 설정 end = start + datetime.timedelta(seconds=whileSeconds) # 스크롤 다운 종료 시간\nwhile True:\n    # 페이지 맨 아래로 스크롤 다운\n    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n    time.sleep(1) # 1초 대기\n\n    # 종료 시간에 도달하면 반복 종료\n    if datetime.datetime.now() &gt; end:\n        break\n        \ndoScrollDown(2) # 스크롤 다운 시간 설정\n\n웹 페이지에서 제목과 URL 추출하기.\n\n\n\n제목과 URL을 저장할 리스트 초기화\ntitle_list = [] url_list = []\n\n\n현재 페이지에서 클래스명이 ’title_link’인 요소들을 찾음\ntitles = driver.find_elements(By.CLASS_NAME, ‘title_link’)\nfor i, title_element in enumerate(titles): try: # 요소에서 제목을 추출하여 title_list에 추가 title_list.append(title_element.text) # 요소에서 URL을 추출하여 url_list에 추가 url_list.append(title_element.get_attribute(‘href’)) except: print(“오류 발생”) # 예외 발생 시 출력 continue # 오류가 발생해도 다음 요소 처리 계속 진행\n# 10번째 항목마다 진행 상황 출력\nif (i + 1) % 10 == 0:\n    print(f\"{i + 1}개 수집 완료\")\n\n블로그 본문 및 메타데이터 크롤링 자동화.\n\n\n\n1] 크롤링 데이터 저장 리스트 초기화\nnew_doc, like_cnt, comment_cnt, comment_list, img_cnt, div_cnt = [], [], [], [], [], []\n\n\n2] 블로그 본문 크롤링\nfor i in range(len(url_list)): url_path = url_list[i] # URL 불러오기 driver.switch_to.window(driver.window_handles[0]) # 첫 번째 탭으로 이동 driver.execute_script(“window.open(‘{}’)”.format(url_path)) # 새 탭 열기(URL 실행) driver.switch_to.window(driver.window_handles[1]) # 두 번째 탭으로 이동\ntime.sleep(1)  # 1초 대기\ntry:\n    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n    d = ''     # 댓글 변수 초기화\n\n    # 댓글 영역의 HTML 코드 가져오기\n    if len(iframes) &gt; 0:            # iframes의 존재 확인\n        driver.switch_to.frame(0)       # 첫 번째 iframe으로 전환 및 내용 가져옴\n        html = driver.page_source       # HTML 코드 가져와 변수 저장\n        soup = BS(html, \"html.parser\")  # 저장된 코드 파싱 및 soup 생성\n\n        # 3] 블로그 본문 추출\n        try:\n            a = soup.find(\"div\", class_=\"se-main-container\").get_text()\n        except: # 블로그 본문을 찾지 못할 경우\n            a = soup.find(\"div\", id=\"postListBody\")     # 일반 블로그에 경우\n            a = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(a)) # 정규표현식 -&gt; 한글만 남김\n\n        # 4] 좋아요 수 추출\n        try:\n            b = soup.find(\"em\", class_=\"u_cnt_count\").get_text()\n        except:\n            b = \"null\"\n\n        # 5] 댓글 수 추출\n        try:\n            c = soup.find(\"em\", id=\"commentCount\").get_text()\n        except:\n            c = \"null\"\n\n        # 6] 댓글 추출\n        try: # 댓글을 모두 보기 위해 버튼 클릭\n            comment = driver.find_elements(By.CLASS_NAME, \"btn_arr\")\n            comment[-1].click()  # 마지막 댓글 버튼 클릭\n            time.sleep(1)\n            commentLen = len(driver.find_elements(By.CLASS_NAME, \"u_cbox_page\"))\n            d = \"\\n\".join([comment.text for comment in driver.find_elements(By.CLASS_NAME, \"u_cbox_text_wrap\")])\n        except:\n            d = \"null\"\n\n        # 7] 이미지 및 영상 수 추출\n        e = len(soup.find_all(\"img\", class_=\"se-image-resource egjs-visible\"))\n        f = len(soup.find_all(\"div\", class_=\"pzp-ui-dimmed pzp-dimmed pzp-pc__dimmed\"))\n\n        # 8] 데이터 리스트에 추가\n        new_doc.append(a)\n        like_cnt.append(b)\n        comment_cnt.append(c)\n        comment_list.append(d)\n        img_cnt.append(e)\n        div_cnt.append(f)\n\n        driver.switch_to.default_content()  # 기본 콘텐츠로 전환\n    else:\n        # 데이터가 없을 경우 빈 값 추가\n        new_doc.append(' ')\n        like_cnt.append(' ')\n        comment_cnt.append(' ')\n        comment_list.append(' ')\n        img_cnt.append(' ')\n        div_cnt.append(' ')\n\nexcept Exception as e:\n    # 예외 발생 시 에러 메시지와 함께 빈 값 추가\n    print(f\"Error at {url_path}: {e}\")\n    new_doc.append(' ')\n    like_cnt.append(' ')\n    comment_cnt.append(' ')\n    comment_list.append(' ')\n    img_cnt.append(' ')\n    div_cnt.append(' ')\n\ndriver.close()  # 현재 탭 닫기\ntime.sleep(0.3)  # 0.3초 대기\n\n# 매 10번마다 진행 상황 출력\nif (i+1) % 10 == 0:\n    print(f\"진행 상황: {i+1}/{len(url_list)}\")\n\n데이터프레임으로 변환.\n\n\n\n크롤링한 데이터를 데이터프레임으로 변환\nraw_data = pd.DataFrame({ “title”: title_list, “doc”: new_doc, “like”: like_cnt, “comment_cnt”: comment_cnt, “commnet_list”: comment_list, “img”: img_cnt, “div”: div_cnt, “ch”: “naver”, “ch2”: “blog” })\n\n\n데이터프레임을 pickle 파일로 저장\nfile_path = “C:/Users/jkl12/텍스트마이닝/” # 슬래시 사용 with open(file_path + “노인부양blog.pkl”, “wb”) as f: pickle.dump(raw_data, f)\n\n\n크롬 드라이버 종료\ndriver.quit()\n\n\n저장된 pickle 파일을 불러오기\nwith open(file_path + “노인부양blog.pkl”, “rb”) as f: temp_file = pickle.load(f)\n\n\n데이터프레임을 CSV 파일로 저장\ntemp_file.to_csv(file_path + “노인부양blog.csv”, index=False, encoding=“utf-8-sig”) # 파일 경로 지정 file_path = r”C:.csv”\n\n\nCSV 파일 불러오기\ndf = pd.read_csv(file_path, encoding=“utf-8-sig”) df\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM04.html",
    "href": "TM/TM04.html",
    "title": "4장: 크롤링 데이터 전처리",
    "section": "",
    "text": "Reporting Date: April. 01, 2025\n\n크롤링 데이터의 통합 및 전처리에 대해 다루고자 한다.\n너저문한 데이터를 정리\n개행문자는 스페이스바를 눌렸기에 생기는 것임. 이를 처리하는 것이 전처리 과정임.\n의미있는 인사이트를 얻기 위해서 정리\n원데이터, 쿠팡에서 들어오는 데이터는 트랜젝션데이터(거래 데이터) 어떤 고객이 가입, 비가입, 어떤 카드로 결제, 회원가입 정보(최소한의 정보), 배송을 위한 성명, 휴대폰 번호, 본인인증 정도, - 고객의 프로파일링, 또는 데모그래픽 데이터 품목명, 가격명, 시간대, 카드 정보 – 정형데이터 이러한 필드로 저장됨, 댓글 정보 – 비정형 데이터, 사람마다 쓰는 댓글 양이 다름 고객의 반응을 보기 위해 전처리를 시도함, 이는 IT팀, 마케팅팀, MD가 사용\n성별을 구별하는 방법\n룰세팅을 하여 전처리 필요 없이 데이터를 뽑아서 써야 됨 여기선 가공 변수를 만드는 것이 필요함(예: 특정한 시간대에서 발생되는 매출)\n품목별 페이지에 대한 로그분석, 리뷰 데이터, 댓글의 패턴, 쿠키로 데이터 가져오기 SKT 전화요금제,만 있을 경우, 전화거래량 패턴 분석 정도 밖에 할 수 없음. 이름으로 성별을 판별 –\n추정을 하는 것임\n02 정규표현식\nimport re text = ‘core core883core’ re.findall(r’, text)\n\n단어 중간에 있는디\nre.findall(r’’, text)\nre.findall(r’1’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’, text)\ntext = ‘12 month 365 days 2023?’ re.findall(r’’, text) # 숫자에 해당되는 걸 다 가져와\nre.findall(r’+’, text) # 숫자를 제외한 모든 것\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자를 제외한 모든 문자출력\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text\ntext = ‘Wow! 999. This is a wonderful place.’ w_text = re.findall(r’+’, text) w_text # 특수문자까지 포함하여 출력\n03 데이터 합치기\n\n라이브러리 불러오기\n\nimport pandas as pd import pickle import os import re\n\n데이터 병합하기\n\n\n\n파일 저장 위치\nfile_path = r’C:\\’\n\n\npkl 파일 로드 함수\ndef pklopen(text): f = open(file_path + ‘{}.pkl’.format(text),“rb”) a = pickle.load(f) f.close() return a\n\n\n수집된 데이터\ndata1 = pklopen(‘노인부양blog’) data2 = pklopen(‘노인부양cafe’) data3 = pklopen(‘노인부양cafe2’)\n\n\n데이터 결합\n\n\n행(row) 방향으로 데이터를\n\n\n밑으로 합치는(concatenate) 방식\ndata = pd.concat([data1, data2, data3])\n\n\n수집된 데이터가 모두 같은 컬럼 구조를 갖는다면,\n\n\n위에서 아래로 이어붙이는 방식으로 결합된다.\n\n\n각 채널 사이즈 확인\ndata.groupby([‘ch’, ‘ch2’]).size()\n\n인덱스 재설정하기\n\n\n\n인덱스를 0, 1, 2, …로 초기화하고,\n\n\n기존 인덱스는 새로운 열로 남기지 않도록 하는 명령어.\n\n\n주로 데이터 정제 후 인덱스를 깔끔하게 맞출 때 사용된다.\ndata = data.reset_index(drop=True) data\n채널별 수집한 데이터의 병합 결과\n04 데이터 전처리\n\n한글화\n\n정제, 정규화, 토큰화의 3단계를 거친다. 비정형 데이터일 경우,\n문서 날리기\n100 정열\nf = open(file_path + ‘노인부양병합’, ‘wb’) pickle.dump(data, f) f.close()\nf = open(file_path + ‘노인부양병합’, ‘rb’) docs = pickle.load(f) f.close() docs\n\n\n병합된 데이터를 피클 파일로 저장 및 출력한 것으로\n\n\n아래와 같이 파일이 저장된 것을 확인할 수 있다.\n\n\n제목, 본문, 댓글의 한글화 및 특수문자 제거\nfor i in range(len(docs)): docs.loc[i, ‘title’] = re.sub( r”[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]“,”“, str(data.loc[i, ‘title’]))\ndocs.loc[i, 'doc'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'doc']))\n\ndocs.loc[i, 'comment_cnt'] = re.sub(\n    r\"[^0-9]\", \"\", str(docs.loc[i, 'comment_cnt']))\n\ndocs.loc[i, 'comment_list'] = re.sub(\n    r\"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", str(data.loc[i, 'comment_list']))\ndocs\n\n\n조건에 맞는 row만 남기기\ndocs = docs[ ~( (docs[‘doc’].str.len() &lt; 2) | (docs[‘doc’].str.isspace()) )].reset_index(drop=True) 안에 ^를 넣을 때 한글만 가져오기\n\n문자형으로는 카우트 할 수 없으므로\n\n\n\nlike, comment_cnt, img, div의 데이터 타입을 숫자로 변환\n\n\n변환 중 에러 발생 시 NaN으로 처리\ndocs[‘like’] = pd.to_numeric(docs[‘like’], errors=‘coerce’).astype(‘Int64’) docs[‘comment_cnt’] = pd.to_numeric(docs[‘comment_cnt’], errors=‘coerce’).astype(‘Int64’) docs[‘img’] = pd.to_numeric(docs[‘img’], errors=‘coerce’).astype(‘Int64’) docs[‘div’] = pd.to_numeric(docs[‘div’], errors=‘coerce’).astype(‘Int64’)\ndocs\n원데이터와 값이 일치하는 지 확인하기.\n\n숫자형 결측치로 단어, 또는 문장, 형태소 분석 어간, 어근 어조 어미, 단순 띄어쓰기만으로는 힘듦, 형태소 분석을 쓰는 것임\n\n자립 형태소, 의존형태소\nOkt, 메캅, 코모란, 한나눔, 꼬꼬마\nfrom tqdm import tqdm # 진행상황 시각화 from konlpy.tag import Komoran\n\n\nKomoran 형태소 분석기 초기화\nkomoran = Komoran() # 클래스의 인스턴스 지정 # 이는 형태소 분석기 하나를 준비해서 계속 쓰기 위함이다.\n\n형태소 분석을 하고 명사들을 리스트로 저장\n\n이름은 단순하게 지정해도 상관없지만 너무 이름이 단순하면 이를 구별하거나 변수를 이해하기 어려으므로 다른 사람도 알아볼 수 있도록 객관적으로 판단하여 룰 세팅을 하는 것이 좋음\ntitle_token_list = [] # 제목의 형태소를 담아낼 리스트 title_token_noun = [] # 제목의 명사를 담아낼 리스트\nfor i in tqdm(range(len(docs))): # for문 - :\n# komoran.pos() 메서드를 사용하여 형태소 분석 실시\npos = komoran.pos(str(docs['title'][i]))\n\n# komoran.nouns() 메서드를 사용하여 추출하고 리스트에 저장\nnoun = [term for term in komoran.nouns( # 명사만 추출하며,\n    str(docs['title'][i])) if len(term) &gt; 1] # 명사의 길이는 2 이상이어야 한다.\n\ntitle_token_list.append(pos)\ntitle_token_noun.append(noun)\n리스트 이름구조 형태내용\ntitle_token_list [[(‘단어’, ‘품사’), …], …] 모든 형태소와 품사 정보 title_token_noun [[‘명사’, ‘명사’], …] 2글자 이상 명사만\n\n본문 토큰화 # 본문 형태소 및 명사 리스트 doc_token_list = [] doc_token_noun = []\n\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['doc'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['doc'][i])) if len(term) &gt; 1]\n\ndoc_token_list.append(pos)\ndoc_token_noun.append(noun)\n\n\n댓글 형태소 및 명사 리스트\ncomment_token_list = [] comment_token_noun = []\nfor i in tqdm(range(len(docs))):\npos = komoran.pos(u'{}'.format(docs['comment_list'][i]))\n\nnoun = [term for term in komoran.nouns(\n    u'{}'.format(docs['comment_list'][i])) if len(term) &gt; 1]\n\ncomment_token_list.append(pos)\ncomment_token_noun.append(noun)\n형태소 분석만으로는 전처리가 끝났다고 볼 수 없다 아래 쓸대없는 불용어 때문에 찾고자 하는 문맥을 못 볼 수 있다.\n예를 들면, 광고글이 있는데 이는 광고글을 쓴 자가 정성스럽게 알맞은 단어 (의미없는 개행 문자 등만을 나열하지 않는) 말 그대로의 정돈된 글이기 떄문에 이를 제거하려면 일일이 광고글을 제거해야 한다.\n그러므로, 불용어 처리까지 해야 한다.\n다만, 불용어 처리에도 의미가 있는 명사를 제거하지 않도록 주의해야 한다. 예를 들어, ‘la’ 라는 문자만 본다면 의미없는 불용어라고 착각할 수 있다. 그러나 이는 LA를 의미하며, 미국 현지에서는 la, La, lA, LA와 같이 다양하게 사용되는 것으로 나타났다.\n따라서 이러한 불용어 처리 전에는 혹은 처리 중에 이러한 용어들이 나온다면 즉시 문서나 원데이터를 들여다봐서 실제로 그 값이 어떤 문맥상에서 어떤 의미를 지니는지를 확인해야 한다.\n어휘에 대한 이해를 할 수 있어야 한다.\n\n불용어 사전 다운받기\n\nstopwords-ko/stopwords-ko.txt at master · stopwords-iso/stopwords-ko\nKorean stopwords collection. Contribute to stopwords-iso/stopwords-ko development by creating an account on GitHub.\ngithub.com # 불용어 사전 기반 불용어 리스트 정리 f = open( file_path + “stopwords-ko.txt”, “r”, encoding=“UTF-8”) # UTF-8 인코딩으로 불용어 파일 열기\nst = f.readlines() # 한 줄씩 읽어서 리스트에 저장 f.close()\n\n\n줄 끝 개행 문자 제거\nst = [word.strip() for word in st] st\n\n불용어 사전 깔끔하게 만들기 stw = [word.strip() for word in st if word.strip() != ’’] stw\n나만의 불용어 사전 만들기 # 사용자가 정의한 불용어 추가 # 목적: 순수한 노인부양과 관련된 이야기 수집 # 광고글을 제외하기 위한 사용자 지정 불용어 사전 # 사용자가 정의한 불용어 추가 user_stopwords = [ ‘노인’, ‘부양’, ‘무자’, ‘양의’, ‘기초’, ‘노인학’, ‘계급’, ‘보험’, ‘고령’, ‘경제’, ‘바탕’, ‘국가’, ‘어르신’,‘지역’, ‘생각’, ‘포함’, ‘사업’, ‘한부모’, ‘일상생활’, ‘국민’, ‘확인’, ‘우리나라’, ‘적용’, ‘위해’, ‘기본’, ‘수준’, ‘예방’, ‘방법’, ‘주택’, ‘가능’, ‘방안’, ‘진행’, ‘행위’, ‘등의’, ‘대한민국’, ‘내년’, ‘개념’, ‘모집’, ‘개선’, ‘자격증’, ‘대상자’, ‘자격’, ‘과제’, ‘토론’, ‘청주’, ‘감소’, ‘증가’, ‘대의’, ‘추천’, ‘자부’, ‘경우’, ‘게시판’, ‘자금’, ‘본인’, ‘사람’, ‘연령’, ‘등급’, ‘활동’, ‘정부’, ‘평균’, ‘일반’, ‘파일’, ‘자의’, ‘더보’, ‘주간’, ‘기대’, ‘결과’, ‘통해’, ‘인가’, ‘자료’, ‘두레’, ‘포트’, ‘사이트’, ‘회원’, ‘다운’, ‘추가’, ‘완성’, ‘포인트’, ‘다운로드’, ‘충전’, ‘신규’, ‘제휴’, ‘작성’, ‘이벤트’, ‘저도’, ‘바우’, ‘해주’, ‘아래’, ‘링크’, ‘자가’, ‘해주시’, ‘등록’, ‘특례’, ‘네이버’, ‘구부’, ‘다이’, ‘이얼’, ‘마나’, ‘한일’, ‘서로’, ‘이다’, ‘현재’, ‘해서’, ‘댓글’, ‘하기’, ‘니다’, ‘이하’, ‘안녕하세요’, ‘해도’, ‘오늘’, ‘하면’, ‘키메’, ‘고맙습니다’, ‘이고’, ‘제가’, ‘내세’, ‘가요’, ‘만세’, ‘이노’, ‘때문’, ‘블로그’, ‘블로거’, ‘카페’, ‘만원’, ‘보내기’, ‘질문’, ‘재가’, ‘한국’, ‘세계’, ‘사회’, ‘가족’, ‘기준’, ‘서비스’, ‘장기’]\n\n\n\n불용어 리스트 확장\nstw.extend(user_stopwords)\n\n\n불용어 리스트 CSV 파일로 저장\nimport csv\nwith open(‘불용어 리스트’, “w”) as file: writer = csv.writer(file) writer.writerow(stw)\n\n정리된 불용어를 각문서의 제목, 본문, 댓글에서 제거 for word in stw: for i in range(len(title_token_noun)): # 제목에서 불용어 제거 while word in title_token_noun[i]: title_token_noun[i].remove(word)\n# 본문에서 불용어 제거\nwhile word in doc_token_noun[i]:\n    doc_token_noun[i].remove(word)\n\n# 댓글에서 불용어 제거\nwhile word in comment_token_noun[i]:\n    comment_token_noun[i].remove(word)\n\n\n\n문서파일 docs에 적용\ndocs[‘title_token_noun’] = title_token_noun # 제목 명사 리스트 docs[‘title_token_list_pos’] = title_token_list # 형태소+품사 리스트\ndocs[‘doc_token_noun’] = doc_token_noun # 본문 명사 리스트 docs[‘doc_token_list_pos’] = doc_token_list # 형태소+품사 리스트\ndocs[‘comment_token_noun’] = comment_token_noun # 본문 명사 리스트 docs[‘comment_token_list_pos’] = comment_token_list # 형태소+품사 리스트\n\n불용어를 제거한 최종 파일 저장 및 불러오기 # pickle로 저장 (최초 1회만 실시) import pickle with open(file_path + “total_doc.pkl”, “wb”) as f: pickle.dump(docs, f)\n\n\n\npickle로 다시 불러오기\nwith open(file_path + “total_doc.pkl”, “rb”) as f: data = pickle.load(f)\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM06.0.html",
    "href": "TM/TM06.0.html",
    "title": "6장: 감성 분석",
    "section": "",
    "text": "Reporting Date: May. 13, 2025\n\n감성분석에 대해 다루고자 한다.\n01 제목1 [ID 사용하기] 감성 사전, 말에는 감성이 있다. 단어에 관한 감성 분석, 태도, 성향, 의견 한 개인의 감정이라도 이것도 많아지면 사람들이 생각하는 한 방향이 됨\n극성, 과학에서 말하는 극성\n어휘 기반은 수동으로 구축, 도메인(산업군, 어떤 카테고리 또는 영역인가?) 같은 언어, 단어가 도메인에 따라 다른 감성을 가질 수 있다.\n사전 기반은 이미 누군가가 만들어 놓은 감성 사전을 사용하는 것\n한국어 사전, 한국어, 국문학을 전공한 사람들이 참여했을 것이다. 리커트 척도? 보통이라는 의미를 담는 3점을 고른 생각이 같지 않을 것이라는 것이다. 정량적인 분석이 가능해야 리커트 척도라고 볼 수 있다. 즉, 각 등간 간격이 모두 동일해야 한다는 것이다.\n디테일하게 보고 싶으면 에뮬렉스를 사용.\n1 . 제목2 감성 분석을 위해 군산대학교 감성사전 웹사이트를 참고한다.\nKNU 한국어 감성사전\ndilab.kunsan.ac.kr\n또는 깃허브로 바로 이동해도 된다.\nGitHub - park1200656/KnuSentiLex: KNU(케이앤유) 한국어 감성사전\nKNU(케이앤유) 한국어 감성사전. Contribute to park1200656/KnuSentiLex development by creating an account on GitHub.\ngithub.com\n이 사이트는 다음과 같은 이유로 감성 분석에 유용하다.\nKNUSL 감성 사전의 특징 국내에서 구축한 한글 감성 어휘 사전으로, 한국어 감성 분석에 특화되어 있다. 각 단어에 감성 점수(긍정/부정/중립 등급)를 부여할 수 있다. 다양한 도메인(예: 리뷰, 뉴스 등)에 맞춘 감성 어휘 제공한다. 데이터는 연구 및 학습 목적으로 자유롭게 다운로드 가능 (단, 출처 명시 필요)\nDownload ZIP 클릭.\n다운로드 된 파일 압축 풀기.\ndata 폴더로 들어간 다음 SentiWord_info 파일 경로 복사하기\n감성 사전 JSON 파일을 읽어서 단어만 추출해보기.\nimport json\nfile_path = r’C:-master-master_info.json’\nwith open(file_path, ‘r’, encoding=‘utf-8’) as f: json_data = json.load(f)\n\n단어 리스트 생성\nword_list = [] for item in json_data: # 리스트 반복 word_txt = item[‘word’] word_list.append(word_txt)\n\n\n결과 출력 (앞 10개만 보기)\nprint(word_list[:10])\nKOSAC 감성사전 다운로드\nGitHub - mrlee23/KoreanSentimentAnalyzer: 한국어 감성 분석기\n한국어 감성 분석기. Contribute to mrlee23/KoreanSentimentAnalyzer development by creating an account on GitHub.\ngithub.com\n\n\nvalues 긍정부정중립\n01 제목1 [ID 사용하기]\n1 . 제목2\nNRC Emotion Lexicon\nImpact Some notable ways in which the NRC Emotion Lexicon has made impact include: First of its kind: It was the first word-emotion association lexicon, with entries for eight basic emotions as well as positive and negative sentiment. It still remains the\nsaifmohammad.com\n\n[출처] 파이썬기반 SNS텍스트 데이터마이닝 개정판"
  },
  {
    "objectID": "TM/TM02.html",
    "href": "TM/TM02.html",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "",
    "text": "Reporting Date: March. 11, 2025\n웹 크롤링 개념 및 정적 크롤링 실습에 대해 다루고자 한다."
  },
  {
    "objectID": "TM/TM02.html#정형-데이터",
    "href": "TM/TM02.html#정형-데이터",
    "title": "2장: 네이버 뉴스 기사 제목 크롤링",
    "section": "1 . 정형 데이터",
    "text": "1 . 정형 데이터\n(Structured Data)\n일정한 형식을 갖춘 데이터로, 데이터베이스의 테이블처럼 행과 열로 정리된다.\n예: 엑셀, SQL 데이터베이스, 고객 정보(이름, 나이, 주소 등).\n룰세팅(Rule Setting) 데이터를 저장할 때 고정된 형식(테이블, 행/열 구조, 스키마 등)을 미리 정의하는 것.\n2 . 비정형 데이터 (Unstructured Data)\n형식이 일정하지 않아 체계적으로 저장하기 어려운 데이터.\n예: 텍스트(SNS 게시글, 이메일), 이미지, 동영상, 음성 데이터.\n주로 자연어 처리(NLP)나 텍스트 마이닝 등의 기법을 활용해 분석.\n3 . 반정형 데이터 (Semi-Structured Data)\n일정한 구조를 가지지만 완전히 정형화되지 않은 데이터. 태그나 특정한 형식(XML, JSON 등)을 포함하여 구조화가 가능하다.\n예: HTML, JSON, XML 파일, 로그 데이터.\n02 크롤링\n1 . 정적 크롤링 웹 페이지의 HTML 소스 코드를 직접 가져와서 필요한 데이터를 추출하는 방식.\n기본적으로, requests 라이브러리로 웹 페이지 HTML을 가져와 BeautifulSoup으로 데이터 추출한다.\n페이지 로딩 속도가 빠르고, 서버 부하가 적으나 JavaScript로 생성되는 데이터는 가져올 수 없다.\nHTML만으로 필요한 정보를 얻을 수 있다면 → 정적 크롤링이 유리하다.\n2 . 동적 크롤링 웹 브라우저를 실제로 실행하여 JavaScript로 로드되는 데이터까지 가져오는 방식.\n기본적으로, Selenium이나 Playwright 같은 브라우저 자동화 도구 사용한다.\nJavaScript 렌더링된 데이터를 포함하여 크롤링 가능하나, 속도가 느리고, 서버 부하가 높다.\nJavaScript로 데이터가 동적으로 로딩된다면 → 동적 크롤링이 필요하다.\n03 라이브러리 Jupyter Notebook에서 실행하는 명령어는 기본적으로 일반적인 Python 실행 환경에서도 동일하게 사용할 수 있다.\n(예: 터미널, 명령 프롬프트, 다른 IDE)\n1 . requests 파이썬에서 HTTP 요청을 보내고 응답을 받을 수 있는 라이브러리로, 주로 웹에서 데이터를 가져오거나 서버에 데이터를 전송하는 데 사용된다.\n웹사이트와 데이터를 주고받는 과정에서 사용되는 HTTP 프로토콜을 쉽게 다룰 수 있도록 도와준다.\n① GET 요청 - requests.get() 웹 페이지의 정보를 가져올 때 사용된다. 이는 브라우저에서 주소를 입력하고 페이지를 여는 것과 같은 동작이다.\n② POST 요청 - requests.post() 서버에 데이터를 전송할 때 사용된다. 회원가입, 로그인, 데이터 저장 등의 작업에서 활용된다.\n③ JSON 응답 처리 - response.json() 서버에서 JSON 형식의 데이터를 받으면, .json() 메서드를 사용하여 딕셔너리로 변환할 수 있다.\n2 . BeautifulSoup4 HTML/XML 문서를 파싱하여 원하는 데이터를 추출하는 라이브러리로, 문서를 구성하는 요소를 개별적인 구조(태그, 속성, 텍스트 등)로 나눈다.\n먼저, HTML 문서를 파싱하여 태그 간의 계층을 이해할 수 있는 트리 구조로 변환한다.\n이를 통해 특정 태그나 클래스에 접근할 수 있으며, CSS 선택자를 활용하여 원하는 요소를 쉽게 선택할 수 있다.\n또한, get_text() 메서드를 사용하면 태그 내부의 텍스트만 추출할 수 있어 데이터 정제 작업이 용이하다.\n3 . selenium 웹 브라우저를 자동으로 제어하는 라이브러리로, 클릭, 입력, 스크롤 등의 동작을 수행할 수 있다.\nJavaScript로 동적으로 변경되는 웹 페이지의 데이터도 가져올 수 있어 정적인 크롤링 방식보다 더 유연하다.\n이를 사용하려면 Chrome, Firefox 등 웹 브라우저에 맞는 드라이버가 필요하며, 이를 통해 실제 브라우저를 실행하고 조작할 수 있다.\n과거에는 웹 브라우저와 드라이버의 버전이 맞아야 했지만, 현재는 자동 업데이트 기능 덕분에 큰 문제가 없다.\n4 . pandas 데이터 분석 및 처리를 위한 필수 라이브러리로, CSV, Excel, JSON 등의 다양한 형식의 데이터를 데이터프레임으로 불러와 조작할 수 있다.\n또한, 결측값을 처리하거나 특정 조건에 따라 데이터를 필터링하고 정렬하는 등 정리 작업이 가능하다.\n뿐만 아니라, 데이터를 그룹화하여 분석할 수 있는 groupby() 기능, 기초 통계를 확인할 수 있는 describe() 메서드,\n특정 연산을 적용할 수 있는 apply() 메서드 등을 제공하여, 보다 효과적인 데이터 분석을 지원한다.\n04 정적 크롤링 다음은 네이버 뉴스 기사에 대해 정적 크롤링을 수행하는 코드이다.\n\n설치된 라이브러리를 불러오는 과정."
  },
  {
    "objectID": "DAP/DAP03.html",
    "href": "DAP/DAP03.html",
    "title": "군집화",
    "section": "",
    "text": "Reporting Date: December. 02, 2025\n\n1 . 제목2 상관관계가 있을 수 있는 p개의 변수들을 선형 변환을 통해 상관관계가 없는 새 인공변수(주성분)들로 변환\n10차원의 데이터는 변수 10개. 이런 고차원 데이터에 선형 변환을 하여 주요 성분을 추출하고 차원도 축약시킨다. 그러기 위해 일단 직교변환, 새로운 좌표계로 데이터를 재배치한다.\n단, 분산은 커야 한다. 이는 다른 축으로 옮기더라도 손실되지 않도록 하라는 의미이다. 즉, 분산은 건드리지 마라.\n가중계수. y를 이용한 선형 변환을 먼저 시도 그래서 서로 상관된어 있는 변수들 간의 복잡한 구조를 분석하고자 한다.\n관찰변수들을 선형변환시켜 주성분이라고 분리는 서로 상관되어 있지 않는 새로운 인공 변수를 생성함. 또한 인공변수는 해석할 수 있어야 된다.\n원분산을 최대한 유지하면서, 그게 나의 분포니까. 그래서 유지해야 됨. 직교하는 새로운 축을 찾아 차원 축약\nX : (p*n), p개의 변수, n개의 관측치 z1 = v11x + … + v1pxp = vt1 X해서 zp개로 나올 것이다. 다만 이것을 전부 사용하진 않는다. 따라서 매트릭스의 형태로 나타낸다.\n굉장히 중요한말. z는 x를 u라는 새로운 축에 사영시켰다.max{Var(Z)} = max{Var(vTx)}공분산 행렬을 이용하여 식을 세운다. 다만 v가 너무 많음 그래서 제약 조건을 둠. || v || = VTV = 1라그랑지 승수가 나온다. 이것을 이용해서 미분등의 과정을 거친다. L = VT 시그마 V - 람다 (VTV -1)그러면 v가 각각 지워지면서 (시그마 - 람다)V = 0이 된다. 여기까지 오면 이제 고유값을 분해하는 함수이다.고유벡터의 정의에 의해 V는 X의 공분산 행렬 시그마의 고유 벡터이며, 람다는 시그마의 고유값 시그마의 고유벡터를 주성분 이라고 한다. v1, v2는 방향, 람다는 크기z1의 분산이 람다1이 된다. 이는 흥미로운 흐름이다.대체로 v1이 90이상의 분산을 가져가게 된다. 나머지 끝에 있는 건 버린다. 너무 다 가져갈려고 하면 똑같으며 이는 차원 축약 한 건 의미가 없기 때문.\n시맨트릭(대칭) 매트릭스를 사용. 그럼 제곱으로 바뀐다. 그렇게 공분산 행렬을 만들고. 공식에 의해서 v1, v2, 람다1, 람다2를 찾는다. 2차원으로 할 건지, 1차원으로 할껀지 선택을 한다. 주성분을 이용하여 원데이터를 새롱누 좌표계로 변환.\n\n표준화, 방법은??\n공분산 행렬, 행렬로 계산할 땐 차원으로 표현 p(세로) n(가로) pn와 np 매트릭스 곱하기\n고유값, 고유벡터 계산 변수의 차원 만큼 v가 나옴.\n가장 큰 고유값에 대항하는 고유 벡터로 새로운\n그럼 5개 각 차원에 해당되는 좌표값이 5개 나올 것이다.\n\nz1의 첫번째 원소 의미 원데이터의 첫번째 데이터 n1이 첫번쨰 고유벡터 v1과 내적된 결과가 -.2152 따라서 그 값은 n1을 v1이라는 축에 사용하여 어디에 위치하는지를 나타내는 값(스칼라)\n그걸 그림으로 표현하면 삼각형으로 그려진다. z1값은 삼각형의 밑변에 해당되는 정도의 길이를 가진다.\n이제 Cov(Z)에서는 각 차원에 해당되는 3개의 분산이 나올 것이다.\n그래서 이것에 최종 설명은 부호로 어떤 요소에서 영향을 주는 구나 라는 걸 안다. 그럼 그 요소를 고려해서 축을 결정할 때 이 정보를 사용한다.전처리에서의 차원 축소와 PCA의 차원 축소의 차이?로직은 알고 있어야 된다. 어떻게 흘러가는지."
  },
  {
    "objectID": "DAP/DAP02.html",
    "href": "DAP/DAP02.html",
    "title": "다변량 분석",
    "section": "",
    "text": "Reporting Date: November. 18, 2025\n\n[주제] 에 대해 다루고자 한다.\n1 . 제목2 다중회귀 모델로 커퍼가 되었기 때문에 학부에서는 안 배웠다. 군집 분석은 텍스트데이터마이닝 k-민스와 같다고 할 수 있지만 그것은 아니다.\n비지도 학습으로서 타겟이 없으며,\n군집의 개수, 내용, 구조 등이 완전하게 알려지지 않는 상태에서 개체들 아이의 유사성 또는 거리에 근거하여 군집을 형성하고 형성된 군집의 특성을 파악해 군집들 간의 관계 분석\n유클리드 거리, 맨해튼 거리, 코사인 유사도 등 측도를 달라도 거리로 본다는 점이 공통이다. 지표는 실루엣 계수, 엘보우 기법 등이 있다.\n보통은 구형 모양이 가장 안정된 형태\n\n군집이 긴 모양\n개체 A,B 구 군집 사이의 고리 역할을 하는 경우\n\n금융(신용카드 고객 행동 패턴 분석)에선 세그멘테이션이 중요하다. 그외에 활용 분야 마케팅(고객 세분화), 헬스케어(환자 그룹 분류), 제조업(불량 패턴 탐지)\n계층적 군집 분석, 또는 계보적 군집 분석 [최단, 최장, 형균, 중심, 중위수]연결법 Ward의 방법\n과정\n\n먼저 가장 가까운 2개의 개체를 묶어 하나의 군집을 만든다. 딱 하나만 나머지 개체는 각각이 하나의 군집을 이루도록 한다.\n그리고 그 상태에서 가까운 것들을 조금씩 묶어 간다.\n군집들 간 거리의 측도를 기준으로 각 단계마다 한 쌍씩 병합되어, 최종적으로 개체들을 모두 묶어 하나의 군집을 만드는 단계까지 방봅\n\n각 특징\n최단 - 가장 가까운, 덴드로그램에서 긴 병합과정 최장 - 가장 먼 두 데이터, 덴드로그램에서 더 균형 잡힌 클러스터 구조 평균 - 두 데이터의 평균을 만들고 그걸 기반으로 거리 잼 중심 - 클러스터의 중심점을 계산, 중간 정도의 안정적인 군집 형성 중위수 - 거리를 중위값 기준 Ward의 방법 - 가장 많이 쓰임, 분산을 최소화하는 방식\nscipy.cluster.hierarshy import l\nprotein.csv\n최단 연결법(싱글)은 군집이 긴 모양에선 쓰지 않는 것이 좋음. 덴드로그램 시각화 함\n데이터의 개수 및 빈도까지 고려된다. 단순 거리만 고려되지 않는다. 최단 연결법의 알고리즘, D_0에서 각 거리는 대칭이다. 매트릭스이고, 거리가 가장 짧은 것들을 본다. 예를 들어, 0과 1이 가장 짧으므로 하나의 군집이 된다. d1 d2의 거리는 1이다. 둘을 마지한다. 본래 4X4이였다면. 그러면 행렬이 3X3로 줄어든다. d1 d2는 c1이 된다. 또 다시 2X2로 된다. 이런 식으로 계산된다.\nd(p1, p2) = 루트(x2 - x1)^2 + (y2 - y1)^2 계산식 해보기.\n최장 연결법(컴플리트), 좀 더 분류를 크게 잘 함. 이것의 알고리즘도 최단과 시작은 비슷, 여기서 계산만 최장으로 한다. 각 알고리즘 기법의 결과 차이가 다르진 않음 그저 그룹잉하는 관점에 차이임.\n평균 연결법(어버리지)이전보다 군집이 더 세분화됨. 자신만에 센터를 만든다는 다는 것이 차이\n군집 분석은 주관적인 경향이 많음. 현업에서는 경험적을 우선으로 해야 됨. 자신이 배운 통계 솔루션을 너무 들이밀면 안됨. 그 이유는 데이터 결과는 감성적인 패턴이 안나오기 때문.\n회사에서는 마스터 세그멘테이션을 지정한다. 고객을 대상으로 1년마다 갱신함.\n신용은 가맹점 수수료 밖에 없음, 그러나 금융은 이자가 있음 그래서 금융이 더 많이 벌음. 그래서 각각의 세그멘테이션을 만들어야 되고, 개별로 비교해봐야 된다.\n각 요소마다 단위가 다르다면, 표준화하여 진행한다.\n중심 연결법도 이전과 마찬가지로 매트릭스를 만든다. X1과 X2가 있을 때 XX` = D(5*5)으로 계산해서 각 차원의 수가 들어든다. 군집의 분류를 어디부터 짤라야 하는가? 이것이 가장 중요하고 애매하다. 그래서 이를 도와주는 기법이 있다.\n그래서 군집 분석은 통계학 분석 치고는 주관성이 약간 들어간 학문이다.\nk-평균 군집분석\n자주 사용하는 이유는 가장 효과적이고 대표적인 모델이기 때문에 그렇다.\n최적분리 군집 방법의 절차\n먼저 군집 초기값을 선택한다. 임의로 고르든 규칙적으로 고르든. 그 이후 초기 군집을 형성한다. 초기값들과의 거리를 계산된다. 이때 각 개체가 할당될 때마다 해당 군집의 중심이 그 군집에 속하는 걔체들의 평균 벡터로 다시 계산됨. 센터의 변동이 있음, 이제 별로 변동이 없어질 때까지 군집 중심들의 변화가 일정 수준 이하가 될 때까지 같은 과정을 반복한다. 시간은 많이 걸리나 가장 이상적인 군집분석이다. 각 군집과 구성원들을 묶는 그 방식을 조금씩 바꿔서 진행하는 것이다.\n그럼 먼저 군집의 수를 잡는 방식으로 엘보우 기법을 사용한다. 엘보우의 경사도가 의미하는 것은 군집내에 제곱합이다. 굽집내 변이의 합계를 의미한다. 그 값이 작을수록 좋음을 나타내며 이는 동질이다. WSS가 급격히 감소하다가 완만하게 되는 지점이 적절한 군집의 개수에 대한 후보이다.\n단위가 다를 수 있다. 데이터를 표준화시키고 다시 보는 방법도 가능하다. 다만, 유의할 것은 y축의 정보 손실이 발생할 가능성도 있다.\n군집 3가지를 보고 싶으나 3차원이야. 그래서 차원을 축약해서 간단하게 보는 방법이 차원 축소이다. 그리고 각 클러스터가 라벨로 표시된다. 사실상 각 군집을 예측한 것이다. 이 포인트는 이 군집 안에 들어갈 것이다라고.\n군집수 성능 지표(군집 결정 기중)\n실루엣 계수는 개별 데이터가 할당된 군집 내에 데이터와 얼마나 가깝게 군집화 되어 있는지, 그리고 다른 군집에 있는 데이터와는 얼마나 멀리 있는지를 보는 것.\n범위는 -1부터 1까지 마치 상관계수와 같다. 1에 가까울수록 좋은 것이다. 거리는 분산에 의해 결정된다.\n붓꽃 데이터는 이미 다 조사를 했는데. 3종이 이미 되어 있다. 그러나 다시 분석을 하는 것은 이 기법으로 정말로 실효성이 있는가 에대한 가설 실험인 것이다.\n엘보우 기법과 실루엣 계수간에 차이가 날 수 있다. 이것도 고민되는 이유이다. 갭 통계량, 약간 값을 벙튀기 하는 것. 관측된 군집간 변이와 균일 분포하에서의 군집간 변이의 기댓값을 비교하는 것으로 그 값이 클때 군집화가 잘 되었을 을 나타낸다.\n부스트랩 반복횟수 지정(반복횟수를 크게 하면 시간이 많이 걸리지만 더 좋은 결과를 얻을 수 있다.)\n값이 클수록 좋은 것이다. 그러나 만약 그보다 작은 값을 선택했다면? 과적합, 떄문에 그렇다. 이전에 엘보우과 실루엣 계수도 비교해서 결정해야 한다. 기울기도 보고 값의 크기를 더 많이 본다.\n즉 보여지는 이론적 배경과 수치보다 현업의 이해도와 융통성이 있어야 된다. 유연하다는 것과 거짓정보를 주는 것은 다른다. 전문가의 의견을 많이 들어서 다 포함해야 한다. 애초에 사람들에게 쓰기 위해서 이론 수학보다 유연하게 만든 학문이기에.\n분석사례 - NbClust (군집수를 결정하는 추가적인 통계량 제공) 왜 이 많은 걸 볼까? 그건 각 통계량마다 헛점들이 많기 때문이다. 만약에 2, 3가지로 봐서 명확하게 나오면 괜춘. 근데 애매하면 꼭 여러가지 기법을 총동원해서 최적의 군집을 찾고 그 성능과 신뢰도까지 보여줄 수 있어야 한다.\n가우시안 매트릭스 모델(거리가 아닌 확률로 구한다)\n베이지안 군집 분석: 여러개의 혼합된 모델일 경우에 좋다. 다변량 가우시안 모델이다.\n베이지안 GMM은 클래스를 사용해서 구현. 군집 수에 대한 불확실성을 반영\n검토하는 직관적인 방법 다른 통계적 방식에 비해 상대적으로 굉장히 복잡한 과정을 거친다.\n특이값에 민감하다, 이유는 거리 기반이기 때문에 디스턴스 기반이기 떄문에 그렇다.\n동일한 자료에 대해 다른 가정에 기초를 둔 여러 가지 군집 방법을 적용했을 때 대부분의 방법에서 제공된 결과가 유사한 결과가 나오는 가?\n주어진 자료를 임의적으로 두 부분으로 나누고 각 부분을 독립적으로 군집시키는 방법 이때 군집들이 안정되어 있다면 그 결과는 유사\n어떤 군집 분석 방법에 의하여 얻어지는 군집의 안정성을 알아보기 위해 몇 개의 일부 변수를 뺐다 꼈다해보기. 제거하여 군집의 구조에 어떤 영향을 미치게 될 것인가를 고찰하는 방법.\n파셜 주성분 분석.(교수님 박사 논문)"
  }
]