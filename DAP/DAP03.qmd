---
title: "주성분 분석(PCA)"
format: html
---

> Reporting Date: December. 02, 2025

---

`Principal Component Analysis, PCA`<br>
상관관계가 존재할 수 있는 (p)개의 관찰 변수를 **선형 변환**하여<br>
서로 상관관계가 없는 새로운 인공 변수(주성분)를 생성하는 통계적 기법이다.<br>

고차원 데이터에서는 변수 간 상관 관계가 분석과 해석을 복잡하게 만들 수 있으므로,<br>
PCA를 통해 데이터 구조를 단순화하고, **차원 축소**를 수행함으로써 데이터 시각화, 노이즈 제거, 변수 간 다중공선성 문제 해결이 가능하다.<br>

---

# 01 목표 및 원리

## 1. 주성분 생성
주성분 분석의 목적은 다음과 같이 정의할 수 있다:

1. 상관관계가 존재할 수 있는 $p$ 개의 원본 관찰 변수
   $X = [x_1, x_2, \dots, x_p]$ 를 **선형 변환(linear transformation)**하여,
   서로 상관관계가 없는 새로운 변수 집합
   $Z = [z_1, z_2, \dots, z_p]$ 를 생성한다.

2. 변환된 변수 $Z$ 는 서로 **직교(orthogonal)**하므로 상관성이 제거되며,
   이를 통해 차원 축소와 데이터 구조 파악을 보다 효과적으로 수행할 수 있다.

3. 각 주성분 $z_k$ 는 **원자료의 분산을 최대한 보존**하도록 선택되며,
   주성분의 순서는 설명하는 분산의 크기에 따라 결정된다.

## 2 표준화
`Standardization`<br>
PCA 수행 시 변수의 단위와 스케일이 다를 경우, 표준화를 먼저 수행해야 한다.<br>
표준화 방법:

$$
x'_i = \frac{x_i - \bar{x}}{s_x}, \quad i = 1,2,\dots,n
$$

* $x_i$ : 원 변수 값
* $\bar{x}$ : 변수의 평균
* $s_x$ : 변수의 표준편차

표준화 후 변수는 평균 0, 분산 1을 갖게 되어 PCA에서 각 변수의 영향력이 균등하게 반영된다.


데이터 행렬은 다음과 같이 표현한다.

* **원본 데이터 행렬:** $X \in \mathbb{R}^{p \times n}$

  * $p$: 관찰 변수의 수
  * $n$: 관측치(샘플)의 수

## 3. 선형 변환

주성분 $z_k$ 는 원본 변수의 **선형 결합**으로 정의되며 다음과 같이 표현된다.

$$
z_k = a_{1k} x_1 + a_{2k} x_2 + \dots + a_{pk} x_p
= \mathbf{a}_k^T \mathbf{x}, \quad k = 1, 2, \dots, p
$$

* $\mathbf{x} \in \mathbb{R}^p$: 원본 데이터 벡터
* $\mathbf{a}_k \in \mathbb{R}^p$: k번째 주성분의 로딩(loading) 벡터
* $\mathbf{a}_k^T \mathbf{a}_j = 0 ( k \neq j )$: 주성분 간 직교성

> 예시: 첫 번째 관측치 (n_1)의 첫 번째 주성분 좌표 (z_{1,1} = x_1^T v_1 = -0.2152)
> → (n_1)이 첫 번째 주성분 축 (v_1) 상에서 어디에 위치하는지를 나타내는 스칼라 값이다.

## 4. 분산 최대화

첫 번째 주성분 $z_1$: **전체 분산을 최대한 설명하는 방향**이 되도록 선택.<br>
두 번째 주성분 $z_2$: $z_1$ 과 직교한다는 제약 아래 **남아 있는 분산을 최대한 설명하는 방향**으로 정의됨.<br>
이러한 방식으로 각 주성분은 계층적으로 결정된다.

따라서 k번째 주성분의 로딩 벡터 $\mathbf{a}_k$ 는 다음 최적화 문제를 통해 구해진다.

**일반식 (k번째 주성분)**

$$
\mathbf{a}*k
= \arg\max*{\mathbf{a}}
\mathrm{Var}(\mathbf{a}^T \mathbf{x}),
\quad
\text{subject to }
\mathbf{a}^T \mathbf{a} = 1,;
\mathbf{a}^T \mathbf{a}_j = 0,; j < k
$$

**특수식 (첫 번째 주성분)**<br>
첫 번째 주성분은 이전 주성분이 없으므로 직교 조건이 필요 없습니다.

$$
\mathbf{a}_1
= \arg\max_{\mathbf{a}}
\mathrm{Var}(\mathbf{a}^T \mathbf{x}),
\quad
\text{subject to }
\mathbf{a}^T \mathbf{a} = 1
$$

* $|\mathbf{a}|^2 = \mathbf{a}^T \mathbf{a} = 1$: Lagrange 승수법 적용을 위한 제약 조건

## 5. 공분산 행렬과 고유값 분해

1. **Lagrange 승수법 적용 → 고유값 문제 도출**

첫 번째 주성분에 대해 Lagrange 함수를 구성한다.

$$
L(\mathbf{a},\lambda)
= \mathbf{a}^T \Sigma \mathbf{a} - \lambda (\mathbf{a}^T\mathbf{a}-1)
$$

편미분하여 최적 조건을 구하면, 다음 고유값 문제가 얻어진다.

$$
\frac{\partial L}{\partial \mathbf{a}} = 2\Sigma\mathbf{a} - 2\lambda \mathbf{a} = 0
\quad \Rightarrow \quad \Sigma \mathbf{a} = \lambda \mathbf{a}
$$

즉, **주성분 로딩 벡터는 공분산 행렬의 고유벡터**이다.
* **추가**: 여기서 $|\mathbf{a}|^2 = 1$ 조건은 **단위벡터(normalized vector)**로 만들어 주성분 크기 비교가 가능하게 하는 역할을 한다.

2. **공분산 행렬 계산**

데이터 행렬 $X \in \mathbb{R}^{p \times n}$ 에 대해 공분산 행렬은 다음으로 정의된다.<br>
$\Sigma$는 실대칭 행렬이므로 고유값 분해가 가능하다.

$$
\Sigma = \frac{1}{n-1} XX^T
$$

* **추가**
  * $X \in \mathbb{R}^{p \times n}$ 가 아닌 $n \times p$ 형태라면<br>
    $\Sigma = \frac{1}{n-1} X^T X$ 로 계산해야 함.<br>
    → 구현 시 데이터 행렬 차원 주의
  * $\Sigma$ 는 대칭 행렬(Symmetric matrix)<br>
    → 고유벡터는 서로 직교(orthogonal)하며 정규화 가능

3. **고유값·고유벡터 해석 → 주성분의 분산**

고유값 분해에서 얻어진 값:

$$
\Sigma \mathbf{a}_k = \lambda_k \mathbf{a}_k
$$

* $\mathbf{a}_k$: k번째 주성분 방향(loading vector)
* $\lambda_k$: 해당 주성분이 설명하는 분산

주성분 순서: 고유값 크기 순서대로 정렬<br>
→ $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p$

다음과 같이 분산과 연결된다:
$$
\mathrm{Var}(z_k) = \lambda_k
$$

## 6. 차원 축소
`Dimensionality Reduction`<br>
PCA는 고차원 데이터에서 **분산을 많이 설명하는 주성분만 선택**하여 차원을 축소하는 기법이다.<br>
원래의 $p$ 차원 데이터 중 정보를 가장 많이 보존하는 상위 $m$ 개의 주성분( $m < p$ )을 선택한다.

1. **설명되는 분산 비율**
`EVR, Explained Variance Ratio`<br>
공분산 행렬의 고유값 $\lambda_k$ 는 k번째 주성분이 설명하는 분산을 의미한다.<br>
따라서 EVR은 다음과 같이 정의된다.

$$
\text{EVR}_k
= \frac{\lambda_k}{\sum_{i=1}^p \lambda_i},
\quad k = 1,2,\dots,p
$$

2. **누적 설명 분산 비율**
`Cumulative EVR`<br>
차원 축소의 선택 기준은 누적 EVR이다.

$$
\text{Cumulative EVR}_m
= \sum*{k=1}^m \text{EVR}_k
$$

일반적으로 **전체 분산의 80~90%를 설명하는 m개의 주성분**을 선택하면<br>
정보 손실 없이 효과적인 차원 축소가 가능하다.

3. **새로운 좌표계로의 사영**
`projection`<br>
선택된 상위 $m$ 개의 주성분 벡터

$$
A_m = [\mathbf{a}_1, \dots, \mathbf{a}_m] \in \mathbb{R}^{p \times m}
$$

사용하여 원본 데이터 $X \in \mathbb{R}^{p \times n}$ 를 **새로운 좌표계(주성분 축)**로 투사(projection)한다.
각 관측치 $x_i \in \mathbb{R}^p$ (i번째 열) 는 다음과 같이 $m$ 차원 좌표 $z_i \in \mathbb{R}^m$ 로 변환된다:

$$
z_i = A_m^T x_i
$$

* $m$ : 선택된 주성분 수 (차원 축소 후)
* $z_i$ : 관측치 (i)의 주성분 좌표
* 예: 5개의 주성분을 선택하면 각 관측치는 5차원 좌표를 가지게 됨

$$
Z_m = A_m^T X \in \mathbb{R}^{m \times n}
$$

* $A_m = [\mathbf{a}_1, \dots, \mathbf{a}_m]$: 상위 m개의 주성분 로딩 벡터
* $Z_m$: 축소된 차원에서의 데이터 표현

각 주성분은 원본 데이터의 분산을 최대한 유지하며 서로 직교하기 때문에,<br>
$Z_m$ 은 **상호 독립적인 새로운 좌표계에서의 데이터**가 된다.

4. **해석**
  * 각 주성분은 원본 변수들의 **선형 결합**으로 구성되므로,<br>
  가중치(로딩) 벡터 $\mathbf{a}_k$ 를 확인하면 해당 주성분이 어떤 변수에 의해 형성되었는지 해석 가능하다.
  * 예: $\mathbf{a}_1 = [0.5, 0.5, -0.3, \dots]$ → 첫 번째 주성분은 첫 두 변수의 기여도가 높음을 의미
  * 필요 시, 시각화를 통해 각 주성분과 원자료 변수의 관계를 분석 가능.
   
  * (z_{i,k})는 관측치 (i)를 주성분 (k) 축에 사영한 값
  * 시각화 시, 좌표 값의 크기는 축 방향으로 관측치가 얼마나 멀리 위치하는지를 나타냄
  * 고유벡터 성분의 부호와 크기는 원 변수들이 주성분에 미치는 영향도를 나타냄
  * 부호는 방향성을 의미하며, 절대 크기를 통해 상대적 기여도를 판단 가능

  * 각 관측치의 (z_{i,k}) 값은 주성분 축 상의 위치를 나타냄
  * 2차원 또는 3차원 시각화 시, 좌표 값은 관측치가 새로운 축 상에서 가지는 위치와 변동성을 직관적으로 보여줌
  * 예: (z_{1,1})을 x축으로, (z_{1,2})를 y축으로 하여 관측치를 점으로 표현하면, 주성분 상에서의 데이터 분포를 시각화 가능

5. **실무 활용**
* PCA는 **고차원 데이터의 시각화, 노이즈 제거, 변수 간 다중공선성 해결, 머신러닝 전처리**에 활용됨
* 실무 적용 시 유의사항:

  1. 원자료 변수 단위가 서로 다른 경우 **표준화(Standardization)** 필요
  2. PCA는 **비지도 학습**이므로 목적 변수(y)를 고려하지 않음
  3. 차원 축소 후 선택된 주성분이 실제 업무 의미와 일치하는지 확인 필요
  4. 주성분 해석 시, 원본 변수와의 관계를 반드시 확인하여 의미 있는 인사이트를 확보

6. 전처리 차원 축소 vs PCA 차원 축소

| 구분 | 전처리 차원 축소          | PCA 차원 축소                    |
| -- | ------------------ | ---------------------------- |
| 목적 | 불필요 변수 제거, 단위 조정 등 | 분산 최대 보존, 상관관계 제거, 통계적 차원 축소 |
| 방식 | 경험적/규칙적            | 선형 변환 기반, 수학적 최적화            |
| 결과 | 원 변수 일부 제거         | 주성분으로 변환된 새로운 좌표             |
| 활용 | 데이터 정제, 단순화        | 시각화, 피처 선택, 노이즈 제거, 모델 전처리   |

---

# 03 Biplot

다변량 데이터에서 차원 축소를 수행한 후 반드시 고려해야 하는 중요한 시각화 도구이다. 이 플롯에서는 주성분 축을 기준으로 데이터 포인트를 시각화하며, 동시에 주성분 벡터를 원래 변수의 방향과 크기를 나타내는 화살표로 표시한다. 각 원변수 벡터에는 레이블을 붙여 변수의 의미를 명확히 부여할 수 있다. 파이썬에서는 전용 함수가 제공되지 않기 때문에, 화살표는 보통 임의로 지정하여 추가한다. 이를 통해 주성분 축과 데이터 분포를 중심으로 각 변수가 어느 방향으로, 얼마나 퍼져나가는지를 확인할 수 있다.

전통적인 PCA 해석에서는 제1축과 제2축의 방향성을 수학적 계산으로 구분하지만, biplot에서는 화살표가 가리키거나 겹치거나 맞닿은 군집 간에서 변수가 유사하게 증가하거나 감소하며, 반대 방향에 위치한 경우에는 전혀 다른 변동성을 보인다. 이러한 시각적 해석 방식은 컨설팅 분야에서 자주 활용되며, 제1축과 제2축에 대한 라벨링과 4분면 구분을 통해 해석을 보다 직관적으로 수행할 수 있다.

---

데이터가 적으므로 데이터 증강을 통해 정규분포를 따르는 노이즈 생성한다. 비율 서로 알맞게 설정. 강건성을 보기 위해서. 여러개 중에 비교하여 월드의 효용성 입증. 2로도 한 번 해보기 어떤 패턴이 나타나는지?? 가우시안 메소드, 베이지안 메소드.(약간 효능감이 떨어진다) 가중치(중심점)가 0.5이상인 것만 표시함 k-mean, 가우시안, 베이지안의 혼동행렬을 출력. 해석 방식이 조금 달라짐. 결과적으로 k-mean, 가우시안이 보다 효능감 있게 분리한다. 여기서 차이가 발생하는 데 이 이유는 각각 중심기준으로 원형태, 타원 형태로 그 기준이 달라서 차이가 나는 것임. 그러나 베이지안이 너무 디테일하게 한다고 특성을 무시하면서 억지로 그룹을 맞추려고 하는 것은 그러면 안된다. 
--------------------- 
시나리오, 할수 있다 없다를 떠나서. CEO관점에서 뭐가 궁금한가? 가설을 여러개 세우고 묶는다. 그리고 단계별로 가지치기하면서 한다. 뭐가 빠져있지? 뭘 붙이지? 어떤 식을 세워야 하는가? 
------------------ 
종이나 변수에 대해 자세히 설명하기 갭통계량에서는 2를 할 이유는 없다, 적당히 큰 것이 좋기 때문. 기울기가 기준이 아님.

미국 대학의 최신의 데이터를 가져와서 쓰기.
증강한 데이터 패턴. 보여주기, 원데이터의 기초통계량과의 차이 등.
미적분학, 행렬, 벡터.
몇 백만, 십만개로 해야 됨. 

아이디어.
데이터 주입. 그럼 분석결과와 시각화 나옴. 클릭하면 파이썬 코드 나옴.

SVM에서도 라그랑주 승수법으로 풀어주는 기법이 있다. RBF커널도 사용. 가장 성능이 좋다고 나옴.
