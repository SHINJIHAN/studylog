---
title: "PCA"
format: html
---

> Reporting Date: December. 02, 2025

---

# 01 주성분 분석
Principal Component Analysis, PCA<br>
상관관계가 존재할 수 있는 (p)개의 관찰 변수를 **선형 변환**하여<br>
서로 상관관계가 없는 새로운 인공 변수(주성분, principal component)를 생성하는 통계적 기법이다.<br>

고차원 데이터에서는 변수 간 상관 관계가 분석과 해석을 복잡하게 만들 수 있으므로,<br>
PCA를 통해 데이터 구조를 단순화하고, **차원 축소**를 수행함으로써 데이터 시각화, 노이즈 제거, 변수 간 다중공선성 문제 해결이 가능하다.<br>

---

## 1. PCA의 목표 및 원리

### 1.1 주성분 생성
주성분 분석의 목적은 다음과 같이 정의할 수 있다:
1. $p$ 개의 원본 관찰 변수 $X = [x_1, x_2, \dots, x_p]$ 를 선형 변환하여 새로운 변수 $Z = [z_1, z_2, \dots, z_p]$ 를 생성한다.
2. 새로운 변수 $Z$ 는 서로 **직교(orthogonal)**하며, 즉 서로 상관관계가 없다.
3. 각 주성분 $z_k$ 는 원자료 분산(variance)을 최대한 유지하도록 선택된다.

### 1.2 수학적 정의

주성분 $z_k$ 는 원본 변수들의 선형 결합으로 정의된다:

$$
z_k = a_{1k}x_1 + a_{2k}x_2 + \dots + a_{pk}x_p = \mathbf{a}_k^T \mathbf{x}, \quad k=1,2,\dots,p
$$

여기서
* $\mathbf{x} \in \mathbb{R}^p$ : 원본 데이터 벡터
* $\mathbf{a}_k \in \mathbb{R}^p$ : k번째 주성분의 가중치 벡터(loading)
* $\mathbf{a}_k^T \mathbf{a}_j = 0 \quad (k \neq j)$ : 주성분 간 직교성

첫 번째 주성분 (z_1)는 **원자료 분산의 최대치**를 설명하도록 선택되며, 
두 번째 주성분 (z_2)는 남은 분산을 최대한 설명하도록 설정한다.<br>

즉, (k)-번째 주성분은 다음 최적화 문제를 통해 구한다:<br>

$$
\mathbf{a}*k = \arg\max*{\mathbf{a}} \mathrm{Var}(\mathbf{a}^T \mathbf{x}), \quad \text{subject to } \mathbf{a}^T \mathbf{a} = 1 \text{ and } \mathbf{a}^T \mathbf{a}_j = 0, j<k
$$

### 1.3 공분산 행렬과 고유값 분해

주성분은 데이터의 공분산 행렬 $\Sigma = \mathrm{Cov}(X)$ 의 고유값(eigenvalue) 분해를 통해 구할 수 있다.

$$
\Sigma \mathbf{a}_k = \lambda_k \mathbf{a}_k
$$

* (\lambda_k) : k번째 고유값, 해당 주성분이 설명하는 분산의 크기
* (\mathbf{a}_k) : k번째 고유벡터, 주성분 로딩(loading)

**설명되는 분산 비율(Explained Variance Ratio, EVR)**은 다음과 같이 계산된다:

$$
\text{EVR}*k = \frac{\lambda_k}{\sum*{i=1}^p \lambda_i}
$$

이를 기준으로 차원 축소를 위해 몇 개의 주성분을 선택할지 결정한다.

## 2. 차원 축소
Dimensionality Reduction<br>
PCA는 고차원 데이터에서 **주요 성분만 선택**하여 차원을 축소할 수 있다.

* 예: 10차원 데이터((p=10)) → 상위 2~3개의 주성분 선택
* 선택 기준: **설명되는 분산의 누적 비율(Cumulative Explained Variance Ratio)**
  $$
  \text{Cumulative EVR}*m = \sum*{k=1}^m \text{EVR}_k
  $$
  
  80~90% 이상의 분산을 설명하는 주성분을 선택하면 대부분의 정보 손실 없이 차원 축소 가능

차원 축소 후에는 **새로운 좌표계(new coordinate system)**에서 데이터를 재배치하게 된다.<br>
각 주성분은 원자료의 분산을 최대한 유지하며, 상호 독립성을 가진 새로운 축 역할을 한다.

## 3. 주성분 해석

* 각 주성분은 원본 변수들의 **선형 결합**으로 구성되므로,<br>
  로딩 벡터 (\mathbf{a}_k)를 확인하면 해당 주성분이 어떤 변수에 의해 형성되었는지 해석 가능
* 예: (\mathbf{a}_1 = [0.5, 0.5, -0.3, \dots]) → 첫 번째 주성분은 첫 두 변수의 기여도가 높음을 의미

## 4. 실무 활용

* PCA는 **고차원 데이터 탐색**, **시각화**, **노이즈 제거**, **머신러닝 전처리** 등 다양한 분야에서 활용됨
* 유의사항:

  1. 원자료 변수 단위가 서로 다를 경우, **표준화(Standardization)** 필요
  2. PCA는 **비지도 학습**이므로 목적 변수 (y)를 고려하지 않음
  3. 주성분 해석 시, 원본 변수와의 관계를 반드시 확인하여 의미 있는 인사이트를 확보

## 5. 데이터 및 목표

주성분 분석(PCA)은 상관관계가 존재할 수 있는 (p)개의 관찰 변수로 이루어진 데이터 (X)를, 
서로 상관관계가 없는 새로운 변수(주성분)로 선형 변환하여 차원을 축소하거나 데이터 구조를 이해하는 통계적 기법이다.

* 데이터 행렬: $X \in \mathbb{R}^{p \times n}$, 여기서
  * $p$ : 관찰 변수의 수
  * $n$ : 관측치 수

* 목표: 고차원 데이터를 선형 변환하여 새로운 좌표계(u)로 사영(projection)하고, 
  분산을 최대한 유지하면서 직교하는 주성분 $Z = [z_1, z_2, \dots, z_p]$ 를 생성한다.

## 6. 주성분 생성

### 6.1 선형 변환

각 주성분 $z_k$ 는 원본 변수들의 선형 결합으로 정의된다.

$$
z_k = v_{1k} x_1 + v_{2k} x_2 + \dots + v_{pk} x_p = \mathbf{v}_k^T X, \quad k = 1,2,\dots,p
$$

* $\mathbf{v}_k \in \mathbb{R}^p$ : k번째 주성분의 가중치 벡터(loading)
* $\mathbf{v}_i^T \mathbf{v}_j = 0 (i ≠ j)$ : 주성분 간 직교성 유지

### 6.2 분산 최대화

첫 번째 주성분 $z_1$ 는 데이터의 분산을 최대화하도록 선택한다.

$$
\mathbf{v}*1 = \arg\max*{\mathbf{v}} \mathrm{Var}(\mathbf{v}^T X), \quad \text{subject to } |\mathbf{v}|^2 = 1
$$

여기서 $|\mathbf{v}|^2 = \mathbf{v}^T \mathbf{v} = 1$ 은 Lagrange 승수법을 적용하기 위한 제약 조건이다.

## 7. 공분산 행렬과 고유값 문제

1. 데이터의 공분산 행렬을 계산한다:

$$
\Sigma = \frac{1}{n-1} X X^T \quad (\Sigma \in \mathbb{R}^{p \times p}, \text{대칭행렬})
$$

2. 첫 번째 주성분의 최적화 문제를 Lagrange 승수법으로 정리하면:

$$
L(\mathbf{v}, \lambda) = \mathbf{v}^T \Sigma \mathbf{v} - \lambda (\mathbf{v}^T \mathbf{v} -1)
$$

3. Lagrange 함수에 대해 $\mathbf{v}$ 로 편미분하면 다음 고유값 문제를 얻는다:

$$
\Sigma \mathbf{v} = \lambda \mathbf{v}
$$

* $\mathbf{v}$ : 공분산 행렬 (\Sigma)의 고유벡터, 주성분 방향
* $\lambda$ : 대응하는 고유값, 주성분의 분산 크기

  * $\mathrm{Var}(z_k) = \lambda_k$

첫 번째 주성분 $v_1$ 는 전체 분산 중 가장 큰 비율을 설명하며, 나머지 주성분은 남은 분산을 순차적으로 설명한다.

## 8. 차원 축소

* PCA에서 모든 주성분을 사용하는 것은 불필요하며, 상위 (m < p)개의 주성분만 선택한다.
* 선택 기준: **설명되는 분산 비율(Explained Variance Ratio, EVR)**

$$
\text{EVR}*k = \frac{\lambda_k}{\sum*{i=1}^p \lambda_i}, \quad k = 1,2,\dots,p
$$

* 누적 분산 비율(Cumulative EVR):

$$
\text{Cumulative EVR}*m = \sum*{k=1}^{m} \text{EVR}_k
$$

* 일반적으로 80~90% 이상의 분산을 설명하는 주성분을 선택하여 차원 축소 수행.
* 차원 축소 후, 데이터는 **새로운 좌표계(u)로 사영(projection)**되며, 
  이때의 좌표는 주성분 값 $Z_m \in \mathbb{R}^{m \times n}$ 로 표현된다.

## 9. 주성분 해석

1. 각 주성분은 원자료 변수의 선형 결합으로 구성되므로, 가중치 벡터 $v_k$ 를 통해 의미를 해석할 수 있다.
2. 예: $\mathbf{v}_1 = [0.5, 0.4, -0.1, \dots]$ → 첫 번째 주성분은 첫 두 변수가 주요 기여를 한다는 의미
3. 필요 시, 시각화를 통해 각 주성분과 원자료 변수의 관계를 분석 가능.

## 10. 실무 적용

* PCA는 **고차원 데이터의 시각화, 노이즈 제거, 변수 간 다중공선성 해결, 머신러닝 전처리**에 활용됨
* 실무 적용 시 유의사항:

  1. 변수 단위가 서로 다른 경우 **표준화(Standardization)** 필요
  2. PCA는 **비지도 학습**으로 y값을 고려하지 않음
  3. 차원 축소 후 선택된 주성분이 실제 업무 의미와 일치하는지 확인 필요

---

# 02

## 1. 데이터 및 목표

주성분 분석(PCA)은 여러 관찰 변수 간 상관관계가 존재하는 데이터를, 서로 상관관계가 없는 새로운 변수(주성분)로 선형 변환하여 차원을 축소하거나 데이터 구조를 이해하기 위한 통계적 기법이다.

* 원본 데이터 행렬: (X \in \mathbb{R}^{n \times p})

  * (n) : 관측치(샘플) 수
  * (p) : 변수 수

* 목표: 원본 변수들의 선형 결합으로 이루어진 **주성분** (Z = [z_1, z_2, \dots, z_p])를 생성하며, 주요 정보(분산)를 최대한 보존하면서 차원 축소를 수행한다.

---

## **2. 표준화(Standardization)**

PCA 수행 시 변수의 단위와 스케일이 다를 경우, 표준화를 먼저 수행해야 한다.
표준화 방법:

[
x'_i = \frac{x_i - \bar{x}}{s_x}, \quad i = 1,2,\dots,n
]

* (x_i) : 원 변수 값
* (\bar{x}) : 변수의 평균
* (s_x) : 변수의 표준편차

표준화 후 변수는 평균 0, 분산 1을 갖게 되어 PCA에서 각 변수의 영향력이 균등하게 반영된다.

---

## **3. 공분산 행렬 계산**

PCA는 공분산 행렬 (\Sigma \in \mathbb{R}^{p \times p})를 이용하여 변수 간 분산 구조를 파악한다.

[
\Sigma = \frac{1}{n-1} X^T X \quad \text{(표준화된 데이터 X 기준)}
]

* (X^T X) : (p \times p) 행렬
* (\Sigma_{ij}) : 변수 (i)와 변수 (j) 간 공분산
* 대칭 행렬(Symmetric matrix)이며, 고유값 분해를 통해 주성분을 결정한다.

> **참고:** 행렬 차원과 전치 여부를 명확히 해야 한다. 원 자료가 (n \times p)라면 (X^T X)로 (p \times p) 공분산 행렬 생성.

---

## **4. 고유값(Eigenvalue)과 고유벡터(Eigenvector) 계산**

공분산 행렬 (\Sigma)를 고유값 분해하면 다음과 같이 정의된다:

[
\Sigma \mathbf{v}_k = \lambda_k \mathbf{v}_k, \quad k = 1,2,\dots,p
]

* (\mathbf{v}_k \in \mathbb{R}^p) : k번째 주성분의 **고유벡터(loading vector)**
* (\lambda_k) : 대응하는 **고유값**, 주성분의 분산 크기
* 변수 수 (p)만큼 고유벡터가 존재하며, 고유벡터는 서로 직교(orthogonal)함

> 첫 번째 주성분 (\mathbf{v}_1)는 데이터 분산을 최대한 설명하며, 두 번째 이후 주성분은 직교 조건을 만족하면서 남은 분산을 설명한다.

---

## **5. 주성분 좌표 계산**

각 관측치 (x_i \in \mathbb{R}^p)는 선택된 주성분 벡터 (\mathbf{V}_m = [\mathbf{v}_1, \dots, \mathbf{v}_m])로 변환되어, 새로운 좌표 (z_i \in \mathbb{R}^m)를 갖는다:

[
z_i = X_i \mathbf{V}_m
]

* (m) : 선택된 주성분 수 (차원 축소 후)
* (z_i) : 관측치 (i)의 주성분 좌표
* 예: 5개의 주성분을 선택하면 각 관측치는 5차원 좌표를 가지게 됨

---

## **6. 차원 축소(Dimensionality Reduction)**

* 모든 (p)개의 주성분을 사용할 필요는 없음
* 상위 (m < p)개의 주성분만 선택하여 차원 축소 수행
* **설명되는 분산 비율(Explained Variance Ratio, EVR)**를 기준으로 선택:

[
\text{EVR}*k = \frac{\lambda_k}{\sum*{i=1}^p \lambda_i}, \quad
\text{Cumulative EVR}*m = \sum*{k=1}^{m} \text{EVR}_k
]

* 일반적으로 누적 분산이 80~90% 이상이 되는 주성분을 선택

---

## **7. 실무적 적용**

1. **표준화 필수**: 변수 단위가 다른 금융, 헬스케어, 마케팅 데이터에 중요
2. **차원 축소**: 데이터 시각화, 노이즈 제거, 머신러닝 전처리에 활용
3. **주성분 해석**: 고유벡터 로딩을 확인하여 원변수 기여도 파악
4. **주관적 판단 반영 가능**: 선택된 주성분 수, 해석 가능성을 실무 상황에 맞게 결정

---
---

# 03

## **1. 데이터 설정**

주성분 분석은 (p)개의 변수와 (n)개의 관측치를 가진 데이터 행렬 (X \in \mathbb{R}^{n \times p})에 대해 수행한다.

* (n) : 관측치 수
* (p) : 변수 수

목표는 원본 데이터 (X)를 서로 상관관계가 없는 새로운 변수(주성분) (Z = [z_1, z_2, \dots, z_p])로 선형 변환하여, 데이터 구조를 분석하고 차원을 축소하는 것이다.

---

## **2. 주성분 좌표(Projection) 계산**

각 관측치 (x_i \in \mathbb{R}^p)는 고유벡터 (v_k \in \mathbb{R}^p)를 사용하여 주성분 좌표 (z_{i,k})로 변환된다:

[
z_{i,k} = x_i^T v_k
]

* (z_{i,k}) : 관측치 (i)가 (k)번째 주성분 축 (v_k) 상에서 갖는 위치(스칼라)
* (v_k) : 공분산 행렬 (\Sigma)의 고유벡터, 주성분의 방향을 결정
* (z_i = [z_{i,1}, z_{i,2}, \dots, z_{i,m}])는 선택된 (m)개의 주성분 좌표

> 예시: 첫 번째 관측치 (n_1)의 첫 번째 주성분 좌표 (z_{1,1} = x_1^T v_1 = -0.2152)
> → (n_1)이 첫 번째 주성분 축 (v_1) 상에서 어디에 위치하는지를 나타내는 스칼라 값이다.

---

## **3. 공분산 행렬 및 분산**

주성분으로 변환된 데이터 (Z)의 공분산 행렬:

[
\text{Cov}(Z) = \frac{1}{n-1} Z^T Z = \Lambda = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_p)
]

* (\Lambda) : 대각행렬, 각 대각 성분 (\lambda_k)는 (k)번째 주성분의 분산
* 주성분은 서로 직교(orthogonal)하므로, 비대각 요소는 0
* 분산 크기 (\lambda_k) → 해당 축이 데이터의 변동성을 얼마나 설명하는지 판단

---

## **4. 주성분 해석**

1. **스칼라 값 의미**

   * (z_{i,k})는 관측치 (i)를 주성분 (k) 축에 사영한 값
   * 시각화 시, 좌표 값의 크기는 축 방향으로 관측치가 얼마나 멀리 위치하는지를 나타냄

2. **고유벡터 로딩(Loading) 해석**

   * 고유벡터 성분의 부호와 크기는 원 변수들이 주성분에 미치는 영향도를 나타냄
   * 부호는 방향성을 의미하며, 절대 크기를 통해 상대적 기여도를 판단 가능

3. **차원 선택**

   * 분산 (\lambda_k)가 큰 주성분부터 선택하며, 누적 분산 비율(Cumulative Explained Variance)을 기준으로 결정
     [
     \text{Cumulative EVR}*m = \frac{\sum*{k=1}^m \lambda_k}{\sum_{k=1}^p \lambda_k}
     ]

---

## **5. 시각적 해석**

* 각 관측치의 (z_{i,k}) 값은 주성분 축 상의 위치를 나타냄
* 2차원 또는 3차원 시각화 시, 좌표 값은 관측치가 새로운 축 상에서 가지는 위치와 변동성을 직관적으로 보여줌
* 예: (z_{1,1})을 x축으로, (z_{1,2})를 y축으로 하여 관측치를 점으로 표현하면, 주성분 상에서의 데이터 분포를 시각화 가능

---

## **6. 전처리 차원 축소 vs PCA 차원 축소**

| 구분 | 전처리 차원 축소          | PCA 차원 축소                    |
| -- | ------------------ | ---------------------------- |
| 목적 | 불필요 변수 제거, 단위 조정 등 | 분산 최대 보존, 상관관계 제거, 통계적 차원 축소 |
| 방식 | 경험적/규칙적            | 선형 변환 기반, 수학적 최적화            |
| 결과 | 원 변수 일부 제거         | 주성분으로 변환된 새로운 좌표             |
| 활용 | 데이터 정제, 단순화        | 시각화, 피처 선택, 노이즈 제거, 모델 전처리   |

---

## **7. 실무적 적용**

1. 관측치 투영값 (z_{i,k}) → 데이터 구조 분석, 이상치 탐지
2. 공분산 행렬 및 고유값 → 주요 축 선택, 차원 축소
3. 로딩 벡터 → 변수 중요도 및 방향성 파악
4. 시각화 → 2~3차원 투영을 통한 클러스터 구조 이해
5. 전처리 vs PCA 구분 → 차원 축소 목적과 방법에 맞게 활용

---

