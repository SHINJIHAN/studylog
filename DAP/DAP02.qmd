---
title: "Clustering"
format: html
---

> Reporting Date: November. 18, 2025

---

# 01 군집 분석
**비지도 학습(Unsupervised Learning)** 기법의 한 유형이다.<br>

사전에 정의된 타겟 변수(종속 변수)가 존재하지 않는 데이터로부터<br>
**데이터 간 유사성 또는 거리(distance)**를 기반으로 군집(cluster)을 형성하는 방법론이다.<br>

이는 데이터가 어떠한 구조를 내재하고 있을 것으로 가정하되,<br> 
그 구조의 형태—군집의 개수, 모양, 분포—가 **사전에 알려져 있지 않은 상태**에서 적용된다.<br>

군집 분석의 핵심 목적은 다음 두 가지로 요약된다.<br>
1. **군집 형성(Clustering)**: 개체들 간 거리 계산을 통해 자연스러운 그룹을 형성<br>
2. **군집 해석(Cluster Interpretation)**: 형성된 군집의 특성과 군집 간 관계 구조를 분석하여 의미를 도출

## 1. 거리(유사성) 측정 방법론

군집 분석에서 가장 기초적이며 중요한 요소는<br> 
**데이터 간 거리(distance) 또는 유사성(similarity) 계산 방식**이다.<br>
거리 측정 방식에 따라 군집 결과는 크게 달라지므로, 데이터의 특성(연속형/희소벡터/텍스트 등)에 따라 적절한 측도를 선택해야 한다.<br>

측도의 형태가 다르더라도, 군집 분석에서는<br> **"거리 기반으로 개체 간 유사성을 정의한다는 점"**이 공통적이다.

### 1.1 유클리드 거리
`Euclidean Distance`<br>
연속형 변수에서 가장 일반적으로 사용되는 거리 척도로, **L2 노름(Norm)**에 해당한다.<br>
두 관측치 $x_i, x_j \in \mathbb{R}^p$ 사이의 유클리드 거리는 다음과 같이 정의된다.

$$
d_{\mathrm{euclid}}(x_i, x_j) = \sqrt{\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}
$$

예: 2차원 데이터 $p_1=(x_1, ~y_1), ~~~p_2=(x_2, ~y_2)$ 의 경우, 다음과 같이 계산할 수 있다.

$$
d(p_1, ~~~p_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$
<br>

#### **(1) 데이터 표준화**

각 변수의 단위가 상이할 경우, 거리 계산이 왜곡될 수 있으므로 표준화가 필요하다.

$$
x' = \frac{x - \mu}{\sigma}
$$

- $\mu$: 평균
- $\sigma$: 표준편차

#### **(2) 병합적 계층 군집 분석 절차**

1. 모든 개체를 단일 군집(singleton cluster)으로 초기화한다.
   $$
   C_1 = {x_1}, C_2 = {x_2}, \dots, C_n = {x_n}
   $$

2. 현재 존재하는 군집들 간 거리 행렬 $D_0$ 를 계산한다.
   * 행렬은 대칭이며, 각 원소 $d(C_i,C_j)$ 는 군집 $C_i, C_j$ 간 거리이다.

3. 거리 행렬에서 가장 가까운 군집 쌍 $(C_p,C_q)$ 을 선택하여 병합한다.
   * 행렬 크기는 1줄씩 감소하며, 새로운 군집 $C_{new}=C_p \cup C_q$ 가 생성된다.

4. 새로운 군집과 나머지 군집 간 거리를 연결법(Linkage Method)에 따라 재계산한다.

5. 이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.

### 1.2 맨해튼 거리
`Manhattan Distance`<br>
**L1 노름** 기반 거리로, 고차원 데이터에서 유리할 수 있다.

$$
d_{\text{manhattan}}(x_i, x_j) = \sum_{k=1}^{p}\left|x_{ik} - x_{jk}\right|
$$

### 1.3 코사인 유사도
`Cosine Similarity`<br>
텍스트 마이닝 분야에서 주로 사용되며, 벡터 방향의 유사성을 측정한다.

$$
\text{cos}(x_i, x_j) = \frac{x_i \cdot x_j}{|x_i||x_j|}
$$

코사인 **거리(Cosine Distance)**는 다음과 같이 정의된다.

$$
d_{\text{cosine}} = 1 - \text{cos}(x_i, x_j)
$$

---

## 2. 군집 형성의 구조적 특징

### 2.1 안정적 군집 형태

k-means 기반 군집화 모델은 군집을 수학적으로 **구형(spherical)** 구조로 가정한다.<br>
k-means의 목적함수는 다음을 최소화한다.<br>

$$
\min_{C_1, ..., C_K} \sum_{k=1}^{K} \sum_{x_i \in C_k} |x_i - \mu_k|^2
$$

이는 각 군집 중심(centroid)으로부터의 제곱거리 최소화를 가정하므로,<br>
군집이 **타원형이 아닌 구형에 가까울 때** 성능이 가장 안정적이다.

### 2.2 비정형 군집의 문제 사례

아래의 경우는 k-means 모델에서 성능이 저하되는 대표 사례이다.<br>

1. **군집의 형태가 길고 가는 모양(elongated cluster)인 경우**<br>
   * 구형 중심 거리 기준으로는 정확히 분리되지 않는다.

2. **개체 A, B가 서로 다른 군집 사이에서 '다리' 역할을 하는 중간 위치에 존재하는 경우**<br>
   * 두 군집이 실제로 분리되어 있어도 k=2 가정에서 중심이 왜곡된다.

이와 같은 경우에는 DBSCAN, 계층적 군집 등<br>
**모양 제약이 없는 알고리즘**이 더 유리하다.

## 3. 군집의 품질 평가 지표
군집 해석과 군집 수 결정에서 다양한 지표가 사용된다.

### 3.1 실루엣 계수
`Silhouette Coefficient`<br>

개체 $i$ 에 대해<br>
* $a(i)$: 같은 군집 내 평균 거리<br>
* $b(i)$: 가장 가까운 다른 군집과의 평균 거리

실루엣 값은 다음과 같다:

$$
s(i) = \frac{b(i) - a(i)}{\max {a(i), b(i)}}
$$

실루엣 계수는 군집의 **응집도(cohesion)**와 **분리도(separation)**를 동시에 평가하는 지표이다.

### 3.2 엘보우 기법
`Elbow Method`<br>
SSE(Sum of Squared Errors)의 감소율을 관찰하여 **변곡점(elbow)**을 최적 군집 개수로 간주한다.<br>
수식은 k-means 목적함수와 동일하다.

---

## 4. 알고리즘 선택과 실무적 전처리 요건

실무에서는 단순히 거리를 계산하여 k-means를 적용하는 것이 아니라,<br>
다음과 같은 요소가 필수적으로 고려된다.

1. **정규화/표준화(Scaling)**:<br>
   * 변수 간 단위 차이로 인한 거리 왜곡 방지<br>

2. **차원 축소(PCA, t-SNE 등)**:<br>
   * 고차원에서의 거리 희석 현상 해결<br>

3. **거리 측정 방식 선택**:<br>
   * 텍스트 → 코사인<br>
   * 연속형 수치 → 유클리드<br>
   * 이상치 존재 → 맨해튼<br>

4. **알고리즘 선택**<br>
   * 구형 군집 → k-means<br>
   * 임의 형태의 군집 → DBSCAN<br>
   * 계층적 구조 중요 → Hierarchical Clustering<br>

---

## 5. 실무 적용 분야
군집 분석은 다양한 산업 분야에서 핵심 기법으로 활용된다.

1. **금융** `Finance`<br>
   * 신용카드 소비 패턴 분석
   * 리스크 기반 고객 세그멘테이션
   * 사기 탐지(비정상 패턴 발견)

2. **마케팅** `Marketing`<br>
   * 고객 세분화(Customer Segmentation)
   * 구매 행동 기반 타겟 마케팅
   * 추천 시스템의 사용자 군집화

3. **헬스케어** `Healthcare`<br>
   * 환자 유형 분류
   * 질병 패턴 분석
   * 개인 맞춤형 치료 전략 개발

4. **제조업** `Manufacturing`<br>
   * 불량 패턴 탐지
   * 공정 조건 기반 군집화
   * 유지보수(Preventive Maintenance) 최적화

군집 분석은 특히 **세그멘테이션(Segmentation)** 분야에서 실무적 가치가 매우 높다.

---

# 02. 계층적 군집 분석
`Hierarchical Clustering`<br>
비지도 학습(Unsupervised Learning)의 대표적 기법으로,<br>
개체 간 **유사도** 또는 **거리(distance)** 정보를 기반으로 군집을 단계적으로 형성하거나 분해하여 데이터의 구조적 관계를 탐색하는 데 사용된다.<br>

계층적 군집 분석은 다음 두 방식으로 구분된다.
* **병합적(Agglomerative)** 방식
* **분할적(Divisive)** 방식
<br>

이 중 실무와 연구 대부분에서는 **계산이 단순하고 직관적**이라는 이유로 **병합적 방법**이 가장 널리 사용된다. 병합적 방법을 **계보적 군집 분석(Agglomerative Hierarchical Clustering)**이라고 부르기도 한다.<br>

병합적 계층 군집은 모든 개체를 각각 하나의 단일 군집(singleton cluster)으로 시작한다. 이후 개체 간 거리 또는 유사도를 기준으로 가장 가까운 두 군집을 반복적으로 병합하며, 최종적으로 전체 개체가 하나의 군집이 될 때까지 과정을 이어간다. 이 과정은 **덴드로그램(dendrogram)**으로 시각화할 수 있어 데이터의 구조적 관계를 직관적으로 파악하는 데 도움이 된다.<br>

또한 계층적 군집은 **군집 수(K)를 사전에 지정할 필요가 없다는 점**에서 탐색적 데이터 분석(Exploratory Data Analysis, EDA)에 특히 유용하다. 덴드로그램을 통해 적절한 군집 수를 시각적으로 판단할 수 있어 패턴 탐색, 구조 이해, 잠재적 그룹 확인 등에 효과적으로 활용된다.


## 1. 병합적 계층 군집 분석의 절차

### 1.1 초기 단계
분석 대상 개체가 $n$ 개라고 할 때, 초기에는 모든 개체가 단독 군집으로 간주된다.

$$
C_1 = {x_1}, C_2 = {x_2}, \ldots, C_n = {x_n}
$$

이후 각 군집 간 거리(유사성)가 거리 행렬(distance matrix)로 표현되며,<br> 
이 행렬은 군집 병합 과정에서 매 단계 재계산된다.

### 1.2 단계별 병합(algo) 과정

각 단계에서는 다음 두 규칙이 반복적으로 적용된다.<br> 

1. **현재 존재하는 모든 군집 쌍 중 가장 가까운 군집을 찾는다.**
   $$
   (C_p, C_q)=\arg\min_{C_i, C_j} d(C_i,C_j)
   $$

2. **해당 두 군집을 하나의 군집으로 병합한다.**
   $$
   C_{new}=C_p \cup C_q
   $$

3. **병합 후, 새로운 군집과 다른 군집 간의 거리를 '연결법(Linkage Method)'에 따라 재계산한다.**<br>
이 과정이 반복되어 최종적으로 하나의 군집으로 통합된다.
$$
n \rightarrow n-1 \rightarrow n-2 \rightarrow \cdots \rightarrow 1
$$

이러한 병합 과정을 시각적으로 나타낸 것이 **덴드로그램(Dendrogram)**이며,<br>
수평선의 높이(height)는 해당 병합 단계에서의 군집 간 거리 혹은 이질성(Heterogeneity)을 나타낸다.

## 2. 연결법
`Linkage Methods`<br>
군집 간 거리 계산 방식은 계층적 군집 분석의 결과에 직접적으로 영향을 미치는 핵심 요소이다.<br> 
아래는 대표적 연결법들의 **정의, 수학적 공식, 특징, 구조적 영향**을 상세히 정리한 것이다.

### 2.1 최단 연결법
`Single Linkage`<br>
두 군집 간 최소 거리(minimum pairwise distance)를 사용한다.
$$
d_{\text{single}}(C_i,C_j)=\min_{x \in C_i,, y \in C_j} d(x,y)
$$

**특징:**
* **Chain Effect(사슬 현상)** 발생 가능성이 높음
  (길게 늘어지는 패턴이 나타나며, 여러 개체가 얇은 줄처럼 연결되어 있는 구조)
* 개별 데이터들이 사슬처럼 연결되어 길게 늘어난 형태의 군집이 형성될 수 있음
* 군집 모양 취약: 좁고 길게 늘어진(Elongated) 군집에서는 적합하지 않음
* 잡음과 이상치 민감: 외곽 점(outlier)에 의해 군집 구조가 쉽게 왜곡됨

**실무적 주의:**
* 단순 거리만 고려하므로, 실제 데이터의 밀도나 분포를 충분히 반영하지 못할 수 있음
* 군집 결과가 직관적이지 않거나 왜곡될 수 있으므로, 데이터 특성을 고려하여 다른 연결법과 병행 평가 필요

### 2.2 최장 연결법
`Complete Linkage`<br>
두 군집 간 최대 거리(maximum pairwise distance)를 사용한다.
$$
d_{\text{complete}}(C_i,C_j)=\max_{x \in C_i,, y \in C_j} d(x,y)
$$

**특징:**
  * 군집 내부가 조밀(compact)하게 유지됨
    (각 군집 내 데이터 간 최대 거리를 고려하기 때문)
  * 이상치와 잡음의 영향을 Single linkage 대비 상대적으로 덜 받음
    (**균형 잡힌 군집 구조(Balanced Cluster Structure)** 생성)
  * 덴드로그램 상에서 병합 높이가 일정하게 유지되어 구조가 시각적으로 균형 있게 나타남

**실무적 고려:** 
  * 분류 경계가 명확해야 하는 경우 유용
  * 군집 간 거리 기준이 엄격하여, 너무 작은 군집이 과도하게 분리되는 경우 주의 필요

### 2.3 평균 연결법
`Average Linkage / UPGMA`<br>
군집 간 모든 개체 쌍의 거리 평균을 사용한다.

$$
d_{\text{average}}(C_i,C_j)
= \frac{1}{|C_i|\cdot |C_j|}
\sum_{x\in C_i}\sum_{y\in C_j} d(x,y)
$$

**특징:**
* 군집 간 전체적 거리 구조를 반영
  (단일 연결법의 사슬 현상 + 최장 연결법의 지나친 조밀화를 완화)
* 극단적 이상치에 대한 민감도가 단일/최장 연결법보다 낮음
* 평균 기반 병합으로 군집 내 구조를 보다 세밀하게 반영
* 모든 데이터 쌍의 평균 거리 기반으로 병합이 이루어짐
* 병합 높이가 극단적으로 치우치지 않고, 일정한 간격을 유지하며 균형 있는 시각적 구조를 보여줌

### 2.4 중심 연결법 
`Centroid Linkage`<br>
각 군집의 중심(centroid)을 계산한 뒤 중심 간 거리로 측정.

$$
\text{군집} C_i \text{의 중심}: \mu_i=\frac{1}{|C_i|}\sum_{x\in C_i} x
$$

$$
\text{군집 간 거리}: d_{\text{centroid}}(C_i,C_j)=|\mu_i-\mu_j|
$$

**특징:**
* 중심 계산과 거리 계산이 행렬 연산으로 처리 가능하여 구현 용이
* 군집이 선형적으로 분리되거나 중심 기반 구조가 뚜렷한 경우 성능이 우수
* 중심만 계산하면 되므로, 반복 연산이 많은 대규모 데이터에서 계산이 비교적 효율적
* 중심 이동으로 인해 Single/Complete/Average보다 덴드로그램의 구조가 덜 안정적일 수 있음
* 역병합(Reversal) 발생 가능성 높음
  (군집 병합 후 새로운 중심이 기존 거리 구조를 뒤흔들어
  덴드로그램 높이가 역전되는 비단조성(non-monotonicity) 문제가 발생하기 쉬움)
* 컷(cut) 기준의 주관성
  (덴드로그램의 높이가 단조 증가하지 않아, 
  군집 수 결정 시 절단 시점 판단이 더 주관적일 수 있음)

### 2.5 중위수 연결법 
`Median Linkage`<br>
두 군집 중심의 중위값(median)을 기반으로 정의하며, Centroid linkage와 유사하나 중심 계산 방법이 다르다.

$$
\text{병합 후 새로운 중심}: \mu_{new}=\frac{1}{2}(\mu_i+\mu_j)
$$

**특징:**
* 중위수 사용으로 극단값의 영향을 평균보다 적게 받음
  (단, 전체 군집 구조 안정성 문제를 해결할 수준의 강인성(robustness)은 아님)
* 중위수 기반이므로 계산 과정은 상대적으로 단순하고 구현 난이도도 낮음

* 역병합(Reversal) 발생 가능성 높음
  (Centroid와 마찬가지로 병합 후 새 중위수 위치가 기존 거리 구조를 비단조적으로 변화시켜
  **덴드로그램 높이 역전(Non-monotonicity)**이 발생할 수 있음)

* 여러 연구에서 Centroid와 유사하게 군집 구조가 불안정하다는 보고가 존재
  (특히 군집 분리 기준이 덴드로그램에서 명확하지 않은 경우가 잦음)
* 데이터가 비정상적 분포(heavy-tailed)거나 극단값이 많은 경우 평균 기반보다 중위수 기반이 유리할 수 있음
  (그러나 덴드로그램 해석의 비단조성 문제와 불안정성 때문에 
  Single, Complete, Average, Ward 방식보다 권장 빈도가 현저히 낮음)
* 따라서 실무·통계 패키지에서 기본 옵션으로 잘 사용되지 않으며, 실증 연구에서도 활용 빈도가 다른 연결법 대비 매우 낮음

### 2.6 Ward의 방법 
`Ward’s Minimum Variance Method`<br>
군집 병합 시 전체 군집 내 오류제곱합(SSE, Within-Cluster Sum of Squares)의 증가를 최소화하는 방법.

$$
\text{군집} C \text{의} SSE: \quad SSE(C)=\sum_{x\in C}|x-\mu_C|^2
$$

Ward 방식은 다음을 최소화한다:<br>
$$
\Delta = SSE(C_i \cup C_j) - (SSE(C_i)+SSE(C_j))
$$

**특징:**
* **가장 널리 사용되는 연결법**
* 군집이 구형(spherical) 형태로 형성됨
  (분산 최소화 원리로 인해 밀집되고 균형 잡힌 구형 구조를 만들기 쉬움)
* 단일·최장 연결법보다 이상치 영향이 적고 안정적인 군집 구조를 형성
  (병합 높이(height)가 비교적 균일하게 증가 → 매우 안정적이고 해석이 쉬운 덴드로그램 구조를 제공
  군집 간 병합 폭이 일정해 분기(branch)가 균형적으로 나타남)
* 군집 내 제곱합 기반이라 군집 간 차이가 직관적으로 해석 가능
* k-means와 유사한 알고리즘적 성향 (분산 최소화)
  (둘 다 군집 내 변동을 최소화하는 방향으로 동작 → 결과 군집 형태가 유사해지는 경향.)

**실무적 고려:** 
* 데이터가 연속형이며, 군집이 구형에 가까운 구조일 때 가장 적합
* 고차원 데이터에서도 안정적이지만, 분산 계산 특성상 변수 스케일링(표준화)이 필수
* 비구형 구조(long, chain-like cluster)를 가진 데이터에서는 과도하게 조밀한 군집이 생성될 수 있음

## 3. 실무적 고려 사항
계층적 군집 분석은 다음과 같은 실무적 특성이 존재한다.

1. **연산 복잡도**
* 거리 행렬 계산: $O(n^2)$
* 전체 병합 과정: $O(n^3)$ (일반적 구현 기준)<br>

→ 데이터가 많아지면 실무에서 **수천 개 이상은 사실상 불가능**<br> 
→ 샘플링, 차원 축소 병행 필요

2. **데이터 전처리 필요성**
* 거리 기반이므로 **표준화(Standardization)** 필수
  $$
  x'=\frac{x-\mu}{\sigma}
  $$

* 이상치(outlier)에 매우 민감 → 사전 처리 필요
* 고차원 데이터에서는 차원의 저주로 성능 저하 → PCA 필요

3. **군집 수 결정**
* 덴드로그램의 컷 높이(cut height)
* 비일관성 계수(inconsistency coefficient)
* 코페네틱 상관계수(cophenetic correlation coefficient) 등 고려

4. **주관성 존재**
* 덴드로그램 컷(cut height) 결정, 연결법 선택,
* 최종 군집 수 결정은 분석자의 경험과 목적에 크게 의존

5. **실무 활용**
* 단위가 다른 변수는 반드시 표준화 필요
* 통계적 솔루션만 적용하면 데이터의 감성적 패턴 반영 부족
* 마케팅/금융에서 고객 세그멘테이션, 1년 단위 갱신 등 경험적 기준 적용

6. **알고리즘 관점**
* 최단, 최장, 평균, 중심 연결법 결과는<br>
  병합 순서나 군집 모양에서 차이가 발생하지만, 최종 그룹화 자체는 모두 유사
* 관점 차이일 뿐, 전체적인 군집 구조 탐색에는 유용함

---
---

# 03 protein.csv

---
---

# 04 K-평균 군집 분석
`K-means clustering`<br>
비지도 학습(Unsupervised Learning)의 대표적 알고리즘으로,<br>
관측값들을 K개의 군집으로 분류하는 프로토타입 기반(Prototype-based) 군집화 방법이다.<br>

데이터의 유클리드 거리(Euclidean distance)를 활용하여 군집 중심(centroid)에<br>
가장 가까운 개체를 반복적으로 할당함으로써 군집 내 동질성을 최대화하고 군집 간 이질성을 극대화하는 것이 목적이다.<br>

K-평균은 계산 효율이 높고 구현이 간단하며 다양한 산업 분야에서 널리 적용되어 실무적 가치가 높다.<br>
그러나 초기 중심 선택, 이상치(outlier) 민감성, 군집형태 제약(구형 구조), 변수 단위 문제 등 여러 제약을 반드시 고려해야 한다.<br>

---

## 1. K-평균 군집의 목적 함수

K-평균 알고리즘은 다음의 목적 함수(Objective Function)를 최소화하는 문제로 정의된다.

$$
\min_{C_1,\dots,C_K} \sum_{k=1}^{K} \sum_{x_i \in C_k} | x_i - \mu_k |^2
$$

* $C_k$: k번째 군집
* $\mu_k$: k번째 군집의 중심(centroid)
* $x_i$: 군집에 속한 데이터 포인트
* $|x_i - \mu_k|^2$: 유클리드 거리의 제곱
* 목적: 군집 내 제곱합(WSS, Within-Cluster Sum of Squares)의 최소화

이 함수가 최소화될 때, 각 군집은 내부적으로 가장 조밀하며, 군집 간 분리는 상대적으로 크다.

## 2. K-평균 알고리즘 절차
K-평균 알고리즘의 표준 절차는 다음과 같다.

1. **군집 수(K)의 결정**
K는 사전에 사용자가 결정해야 하는 **하이퍼파라미터**이다.<br>
일반적으로 엘보우 기법(Elbow method), 실루엣 계수(Silhouette coefficient), Gap Statistic 등으로 후보값을 선정한다.<br>

특히 엘보우 기법은 실무에서 가장 흔하게 사용된다.

2. **초기 중심 선택**
`Initial Centroid`<br>
초기 중심은 임의로 선택하거나, K-means++ 등 보다 안정적인 방법으로 선정할 수 있다.<br>
초기값에 따라 최종 군집 결과가 달라질 수 있어 초기 중심 선택은 중요한 단계이다.

3. **개체의 군집 할당**
`Assignment Step`<br>
각 데이터 포인트 $x_i$ 는 가장 가까운 중심 $\mu_k$ 에 할당된다.<br>
거리 계산은 일반적으로 유클리드 거리로 수행한다:
$$
d(x_i, \mu_k) = \sqrt{\sum_{j=1}^{p} (x_{ij} - \mu_{kj})^2}
$$
- $p$: 변수의 개수

4. **군집 중심 재계산**
`Update Step`<br>
각 군집에 속한 데이터의 평균 벡터를 이용해 새로운 중심을 계산한다:

$$
\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
$$

각 개체가 추가될 때마다(혹은 반복적 재계산 과정에서) 중심값이 변동한다.

5. **수렴 조건 충족 시까지 반복**
다음 중 하나의 조건이 충족하면 알고리즘은 종료된다.

* 중심 벡터 변화량이 특정 임계값 이하일 때
  $$
  |\mu_k^{(t+1)} - \mu_k^{(t)}| < \epsilon
  $$

* 일정 횟수(T) 이상 반복 수행
* WSS 변화가 미미할 때

이러한 반복 과정으로 K-means는 **군집 내 동질성이 가장 높은 상태**로 수렴한다.

## 3. 엘보우 기법
`Elbow`<br>
엘보우 기법은 군집 수 K를 선택하기 위한 정형화된 방법이다.

1. **WSS 정의**

$$
WSS(K) = \sum_{k=1}^{K}\sum_{x_i\in C_k}|x_i - \mu_k|^2
$$

군집 수 K가 증가할수록 WSS는 감소한다.<br> 
이는 군집이 나뉠수록 각 군집이 더 조밀해지기 때문이다.

2. **엘보우의 해석**
* WSS는 K가 증가할수록 급격히 감소하다가,
* 어느 지점에서 감소 폭이 완만해지는 시점이 나타난다.
* 이 지점이 "엘보우(elbow)"이며 적절한 K의 후보로 간주된다.

단, 엘보우는 절대적인 정답이 아니며, 실루엣 계수 등 다른 지표와 병행해야 한다.

## 4. 데이터 전처리와 표준화

K-평균은 거리 기반 모델이므로 변수의 단위 차이가 큰 경우 오차가 발생한다.<br>
예: 나이(20 ~ 80)와 수입(1,000 ~ 1억)이 함께 있을 경우 수입 변수가 거리 계산을 압도한다.<br>

따라서 다음과 같은 표준화가 필요하다.

$$
x' = \frac{x - \mu}{\sigma}
$$

표준화는 거리 측정의 공정성을 확보하지만,
"정보 손실"이 있는 것은 아니며 **변수의 스케일 해석이 달라지는 것**이 정확한 표현이다.

## 5. 차원 축소

대부분의 실무 데이터는 3차원 이상의 다변량 구조를 가진다.<br>
K-평균으로 군집을 생성한 뒤, 결과를 2차원 또는 3차원 그래프로 시각화하기 위해 차원 축소 기법(PCA, t-SNE 등)을 사용한다.

* PCA를 통해 고차원 공간의 분산을 보존하면서 차원을 축약
* 축약된 공간에서 각 군집을 색상(라벨)으로 표시
* 이는 군집 구조를 탐색하고 설명하는 데 유용

군집 라벨은 비지도 학습의 결과이며 "예측값"이라기보다
*"모델이 발견한 데이터 구조에 대한 할당 결과"*에 가깝다.

## 6. 실무적 장점과 제약

1. **장점**
* 계산 효율이 매우 높음
* 대규모 데이터에 적합
* 구현이 단순하고 결과가 직관적
* 마케팅·고객 세그멘테이션·금융 데이터 분석에서 표준처럼 사용됨

2. **제약**
* 구형(spherical) 군집 가정
* 비정형, 길쭉한 군집 구조에서는 부정확
* 사전에 K를 결정해야 하는 제약이 존재한다.
* K-means++ 등의 기법을 활용해야 초기값이 안정적
* 이상치는 중심값을 크게 왜곡시켜 전체 군집 구조를 변형시킨다.

---

# 05 군집수 결정 지표

군집 분석에서는 "정답(ground truth)"이 존재하지 않기 때문에,<br>
군집 수(k)의 결정은 통계적 지표, 산술적 근거, 데이터의 특성, 실무 목적 등이 복합적으로 작용한다.<br>

본 장에서는 군집 수 결정에 사용되는 대표적 성능 지표로 **실루엣 계수(Silhouette Coefficient)**, **엘보우 기법(Elbow Method)**, **갭 통계량(Gap Statistic)**을 중심으로 그 이론적 배경, 계산 방식, 해석 기준, 실무적 의사결정 요소의 중요성을 체계적으로 기술한다.

## 1. 실루엣 계수
`Silhouette Coefficient`<br>

### 1.1 수학적 정의

실루엣 계수는 개별 데이터 포인트가 **자신이 속한 군집과 얼마나 응집(Cohesion)** 되어 있으며,<br>
동시에 **다른 군집과는 얼마나 분리(Separation)** 되어 있는지를 정량적으로 측정하는 지표이다.

특정 데이터 포인트 $i$ 에 대해 다음과 같이 정의한다.

1. 동일 군집 내 거리 평균
   $$
   a(i) = \frac{1}{|C_i|-1}\sum_{j\in C_i, j\neq i} d(i,j)
   $$

2. 가장 가까운 외군집과의 평균거리
   $$
   b(i) = \min_{C_k \ne C_i} \left( \frac{1}{|C_k|} \sum_{j\in C_k} d(i,j) \right)
   $$

3. 실루엣 계수 공식
   $$
   s(i)=\frac{b(i)-a(i)}{\max(a(i),b(i))}, \quad \text{범위}: -1 \le s(i) \le 1
   $$

### 1.2 해석 기준

* $s(i) \approx 1$: 군집이 매우 잘 형성됨 (높은 응집 + 높은 분리)
* $s(i) \approx 0$: 군집 간 경계에 위치
* $s(i) < 0$: 잘못된 군집 배정 가능성이 높음

군집 전체의 실루엣 계수는 다음과 같이 평균으로 계산한다.

$$
S(k) = \frac{1}{n}\sum_{i=1}^{n}s(i)
$$

이 값이 최대가 되는 k를 선택하는 것이 대표적 접근이다.

## 1.3 실무 적용성과 한계

**장점**
  * 개별 포인트 단위의 군집 품질 평가 가능
  * 모델 비교 가능 (예: K=2~10)

**한계**
  * **비선형 구조(예: 두 개의 링 모양)에서는 K-means와 함께 비적합**
  * 고차원 데이터에서 거리 기반 접근의 신뢰도 저하
  * 실루엣 계수가 높아도 실무 요구와 맞지 않을 수 있음(예: 비즈니스 세분화 목적이 다른 경우)

## 2. 엘보우 기법
`Elbow Method`, 군집 내 응집도를 나타내는 **군집 내 제곱합(WSS, Within-Cluster Sum of Squares)**에 기반한다.<br>

WSS는 다음과 같이 정의한다.

$$
WSS(k)=\sum_{i=1}^{k}\sum_{x \in C_i} | x - \mu_i |^2
$$
* $C_i$: i번째 군집
* $\mu_i$: 해당 군집의 중심(centroid)

**해석 기준**
  * WSS(k)는 k가 증가할수록 항상 감소한다.
  * 감소 곡선에서 **기울기 변화가 급격 → 완만**으로 바뀌는 지점을 "팔꿈치(Elbow)"라고 한다.
  * 이 지점이 **적절한 군집 수 후보**가 된다.

**한계**
  * 엘보우 지점이 명확히 보이지 않는 경우가 매우 많다.
  * 시각적 판단 의존도가 높아 **객관성이 떨어진다**.
  * 실무에서는 "엘보우처럼 보이는 두세 지점"이 나오는 경우가 흔하며, 이를 전문가가 해석해야 한다.

## 3. 갭 통계량
`Gap Statistic`<br>
갭 통계량은 Tibshirani(2001)에 의해 제안되었으며,<br>
"관측 데이터의 군집 응집도"와 "참조분포(보통 균일분포) 기반 기대치"를 비교하여 군집의 구조가 통계적으로 의미 있는지를 측정한다.

$$
Gap(k)=E^*[\log(W_k)] - \log(W_k)
$$
* $W_k$: k개의 군집으로 분할했을 때의 군집 내 분산
* $E^*[\cdot]$: 참조 분포에서의 몬테카를로(Monte Carlo) 기준 기대값

값이 클수록 군집이 "랜덤한 분포보다 더 잘 분리되어 있다"는 의미이다.

**최적 k 선택 규칙**
Tibshirani는 다음 조건을 만족하는 최초의 k를 선택하도록 제안하였다.

$$
Gap(k) \ge Gap(k+1) - s_{k+1}
$$

* $s_{k+1}$: 표준오차(SE)에 기반한 보정 항

**장점**
* 엘보우 기법보다 객관적
* 내재적 군집 구조를 통계적으로 검증 가능

**단점**
* **부트스트랩 반복수가 많을수록 시간 비용이 큼**
* 대규모 데이터에서는 사용되지 않는 경우가 많음

## 4. 실무 관점의 종합적 의사결정

### 4.1 지표 간 결과 차이의 존재
엘보우 기법과 실루엣 계수는 서로 다른 관점에서 k를 선택하기 때문에 서로 다른 결과가 나오기 쉽다.
갭 통계량까지 고려하면 결과는 더욱 다양해진다.

따라서 **한 가지 지표만으로 군집 수를 결정하는 것은 통계적으로 부적절**하다.

### 4.2 산업 현장에서의 실제 의사결정 방식

기업의 고객군 세분화(마스터 세그멘테이션), 금융 리스크 분류, 구좌별 소비자 유형 분류 등에서는 다음의 요소가 함께 고려된다.

1. 지표 값의 통계적 안정성
2. 군집의 해석 가능성(Interpretability)
3. 비즈니스 목적에 맞는 분류 구조
4. 실무 담당자 및 도메인 전문가의 판단
5. 추후 유지·갱신 가능성
6. 스케일링(표준화) 여부의 영향

특히,<BR>
* 변수 단위가 모두 다를 경우 표준화(Z-score normalization)는 필수적이다.<br>
* 표준화는 "정보 손실"이 아니라 "스케일 기반 의미가 소거된다는 문제"로 해석하는 것이 정확하다.

## 5. 학문성과 해석의 주관성

군집 분석은 지도학습이 아니며,
통계적으로 "가장 좋은 군집"이라는 절대적 기준이 존재하지 않는다.
따라서 해석자, 전문가, 비즈니스 목적에 의해 결과가 달라진다.

이것이 군집 분석을 최근 연구에서 **Exploratory Data Analysis(EDA)** 기반 기법으로 분류하는 이유이기도 하다.

즉, 이론적 수학 모델은 제공된다.<br>
그러나 최종 모델 선택은 **이론+실무 목적+해석의 융통성**이 결합된 의사결정이다.

---

# 06

분석사례 - NbClust (군집수를 결정하는 추가적인 통계량 제공)
왜 이 많은 걸 볼까? 그건 각 통계량마다 헛점들이 많기 때문이다.
만약에 2, 3가지로 봐서 명확하게 나오면 괜춘. 근데 애매하면 꼭 여러가지 기법을 총동원해서 
최적의 군집을 찾고 그 성능과 신뢰도까지 보여줄 수 있어야 한다.

---

# 07 베이지안 Gaussian 혼합 모델
`Bayesian Gaussian Mixture Model, Bayesian GMM`<br>

기존 거리 기반 방법(K-means 등)은 데이터 포인트 간의 거리 계산에 의존하지만,<br>
Gaussian 혼합 모델(GMM)은 각 군집을 확률 분포로 가정하여 **데이터가 각 군집에 속할 확률**을 계산한다.<br>

Bayesian GMM은 이러한 GMM을 **베이지안 관점에서 확장**한 모델로,<br>
군집 수에 대한 불확실성을 고려하며 데이터 기반 사전(prior)을 적용할 수 있다.<br>

## 1. GMM
다변량 정규분포(Multivariate Gaussian Distribution)의 혼합으로 데이터 분포를 나타낸다.<br>
각 데이터 포인트 $x_i \in \mathbb{R}^d$ 는 다음과 같이 확률적으로 군집에 속한다.<br>

$$
p(x_i) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)
$$

* $K$ : 군집 수
* $\pi_k$ : k번째 군집의 혼합 비율 ((\sum_{k=1}^{K} \pi_k = 1))
* $\mu_k$ : k번째 군집의 평균 벡터
* $\Sigma_k$ : k번째 군집의 공분산 행렬
* $\mathcal{N}(x_i \mid \mu_k, \Sigma_k)$ : 다변량 정규분포 확률밀도함수

## 2 Bayesian GMM
**군집 수 K에 대한 불확실성을 모델링**하며,<br>
사전 분포(prior)와 데이터로부터 얻은 우도(likelihood)를 결합하여 **후행 분포(posterior)**를 계산한다.<br>

* 혼합 비율 $\pi_k$ : Dirichlet 사전 분포
* 평균 $\mu_k$ : Gaussian 사전 분포
* 공분산 $\Sigma_k$ : Inverse-Wishart 사전 분포

$$
p(\pi, \mu, \Sigma \mid X) \propto p(X \mid \pi, \mu, \Sigma) , p(\pi) , p(\mu) , p(\Sigma)
$$

Bayesian 추정에서는 EM(Expectation-Maximization)과 유사한 반복적 최적화 또는 변분 추정(Variational Inference)을 사용한다.

**특징**
1. 데이터 포인트마다 **군집 소속 확률** 제공
2. 군집 수가 명확하지 않을 때 사전 분포를 통한 **자동 결정 가능성**
3. **특이값(outlier)에 민감**

   * 이유: 공분산 행렬 추정 시 이상치가 분산을 왜곡하기 때문
   * 단순 거리 기반이 아니며, 확률적 분포 추정 과정에서 민감

## 3. 군집 안정성 검증
군집 결과가 신뢰할 수 있는지 평가하기 위해 여러 방법이 사용된다.

1. **동일 자료에 다양한 군집 기법 적용**
* 같은 데이터셋에 K-means, GMM, 계층적 군집 분석 등 **다양한 가정 기반 군집 방법**을 적용
* 결과 군집이 **유사하게 나타나는지 비교**하여 안정성을 검증

2. **데이터 분할** `Cross-validation`<br>
* 데이터셋을 임의로 두 부분으로 분할
* 각 부분을 독립적으로 군집 분석 수행
* **군집 구조가 일관되게 나타나는지** 확인

3. **변수 제거/추가** `Sensitivity Analysis`<br>
* 일부 변수를 제거하거나 추가하여 군집 분석 수행
* **군집 구조가 어떻게 변화하는지** 관찰
* 특정 변수에 과도하게 의존하는 군집인지 평가 가능

4. **실무 적용**
* Bayesian GMM은 **다중 패턴이 혼합된 데이터**<br>
  예: 고객 세분화, 금융 리스크 평가, 헬스케어 환자 그룹 분류에 적합
* 안정성 검증 방법은 **실무 데이터 분석에서 필수적**

  * 데이터 분할, 변수 제거/추가, 다른 알고리즘 비교를 통해 신뢰성 확보
  
* 모델 복잡도가 높으므로, **분석 목적, 데이터 특성, 해석 가능성**을 함께 고려해야 함

---

파셜 주성분 분석.(교수님 박사 논문)