{
  "hash": "7f5a65cde4a3cfebce33ae5841ce2424",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"제 9장-다중 회귀\"\nformat: html\n---\n\n>Reporting Date: September. 07, 2025\n다중 회귀분석에 대해 다루고자 한다.\n\n## 01 다중 회귀분석\n현실의 사회적 또는 자연적 현상을 설명할 때,\n반응 변수 \\(y\\) 의 변화를 단일 설명 변수만으로 충분히 설명할 수 있는 경우는 거의 없다.\n\n따라서 여러 개의 설명 변수를 적절히 선택하고,\n이들의 함수로 반응 변수를 설명하면 보다 정확한 예측과 분석이 가능하다.\n\n이러한 상황을 다루기 위해 사용하는 방법이 다중 선형 회귀 모형이며,\n일반적인 수식으로는 다음과 같이 표현된다:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i, \\quad i = 1, 2, \\dots, n\n$$\n\n여기서 추정해야 할 모수(parameters)는 p + 1개의 회귀계수\n\\beta_0, \\beta_1, \\dots, \\beta_p​이며, \\varepsilon_i​는 오차항을 나타낸다.\n\n### 1 .  오차항 가정\n회귀분석의 타당성을 위해 오차항 ε 은 다음과 같은 가정을 만족해야 한다:\n\n$$\n{E}[\\varepsilon] = 0, \\quad \\mathrm{Var}(\\varepsilon) = \\sigma^2\n$$\n\n기댓값은 0이며, 분산은 일정(등분산성)\n서로 다른 관측치의 오차항은 서로 독립적\n반응 변수 y의 전체 변동은 회귀모형과 오차항으로 분해 가능\n이러한 변동 분해를 제곱합의 분할이라고 하며, 식으로 표현하면 다음과 같다:\n\n$$\n\\text{SST} = \\text{SSR} + \\text{SSE}\n$$\n\n### 1 .  전체제곱합\nTSS, SST, Total Sum of Squares\n\n반응변수 y 의 총 변동을 나타낸다.\n\n이는 각 관측값이 전체 평균으로부터 얼마나 떨어져 있는지를 제곱하여 모두 합한 값이다.\n따라서 y 의 전체 변동량을 측정하는 지표라고 할 수 있다.\n전체제곱합의 자유도는 n − 1 이다.\n\n### 2 .  회귀제곱합\nSSR, Sum of Squares due to Regression\nSSM, Sum of Squares due to Model\n\n회귀모형에 포함된 설명변수들이 반응변수의 변동을 얼마나 설명하는지를 나타낸다.\n즉, 평균으로부터의 변동 중에서 회귀모형이 설명할 수 있는 부분이다.\n회귀제곱합의 자유도는 설명변수의 개수 p 이다.\n\n\n\n### 3 .  잔차제곱합\nSSE, Sum of Squares due to Error\n\n회귀모형이 설명하지 못하는 변동으로,\n실제값과 회귀모형이 예측한 값 간의 차이를 제곱하여 합한 값이다.\n\n즉, 오차항에 의한 변동을 의미한다.\n잔차제곱합의 자유도는 n − p − 1 이다.\n\n\n\n총제곱합(SST, Total Sum of Squares): 반응 변수 yyy의 전체 변동량을 나타내며, 각 관측값이 평균으로부터 얼마나 떨어져 있는지를 제곱하여 합한 값입니다. 자유도는 n−1n-1n−1입니다.\n회귀제곱합(SSR, Sum of Squares due to Regression / SSM, Sum of Squares due to Model): 회귀모형이 반응 변수 변동 중 얼마를 설명할 수 있는지를 나타냅니다. 자유도는 설명 변수의 개수 p이다.\n잔차제곱합(SSE, Sum of Squares due to Error): 회귀모형으로 설명되지 않는 변동으로, 실제값과 예측값 간의 차이를 제곱하여 합한 값입니다. 자유도는 n−p−1n - p - 1n−p−1입니다.\n즉, 반응 변수 y의 총 편차는 회귀모형에 의해 설명된\n편차와 오차항에 의한 편차로 나뉘게 되며, 이는 다음과 같이 해석할 수 있다:\n\n$$\n\\underbrace{\\text{총 편차}}_{\\text{SST}} = \\underbrace{\\text{회귀로 설명된 편차}}_{\\text{SSR}} + \\underbrace{\\text{잔차}}_{\\text{SSE}}\n$$\n\n또한 각 제곱합을 자유도로 나눈 값을 평균제곱합(Mean Squares, MS)이라 하고,\n이 값들을 요약한 표를 분산분석표(ANOVA table)라고 한다.\n\nANOVA 테이블\n변동 요인(Source)\t제곱합(SS)\t자유도(df)\t평균제곱합(MS)\tF-값\n회귀(Regression, 모형)\tSSR\tp = k-1\tMSR = \\dfrac{SSR}{p}​\tF = \\dfrac{MSR}{MSE}​\n오차(Error, 잔차)\tSSE\tn − p − 1\tMSE = \\dfrac{SSE}{n-p-1}​\t\n전체(Total)\tTSS\tn − 1\t\n\nF 통계량은 회귀모형이나 분산분석에서 모형이 설명하는 변동(설명된 변동)과 \n잔차 변동(설명되지 않은 변동)을 비교하여, 모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표이다.\n값이 클수록 모형의 설명력이 상대적으로 높다고 해석할 수 있다.\n\n이는 전체 변동 중 모형이 설명하는 비율인 결정계수와도 연결된다.\n일반적으로 1에 가까울수록 모형이 데이터를 잘 설명한다고 본다.\n\n다만, 단순히 F 통계량이나 R2 값의 크기만으로는 모형의 유의성을 단정할 수 없다.\n자유도와 표본 크기, 변동의 구조에 따라 \"얼마나 크면 충분히 큰가\"라는\n절대적 기준이 달라지기 때문이다.\n\n따라서 F 통계량과 함께 p-값을 확인하는 것이 필수적이다.\np-값이 유의수준(예: 0.05)보다 작으면, \"모든 집단의 평균이 같다\" 혹은\n\"회귀계수가 모두 0이다\" 라는 귀무가설을 기각할 수 있으며, 모형이 통계적으로 유의함을 의미한다.\n\n반대로 p-값이 크면 귀무가설을 기각할 증거가 부족하다는 뜻이다.\n이 경우 반드시 모형을 \"처음부터 다시\" 만들어야 한다는 것은 아니며,\n변수 선택, 데이터 수집, 가정 검토 등의 추가 작업이 필요할 수 있다.\n\n다중회귀분석에서는 p 개의 설명변수가\n반응변수 y 를 설명하는 데 유의하게 기여하는지를 확인할 필요가 있다.\n\n이를 위해 귀무가설 H₀ :  β₁ = β₂ = ⋯ = βp = 0 를 세우고,\n다음과 같은 검정통계량을 사용한다:\n\n\n이 검정통계량은 귀무가설이 참일 때\n자유도 (p, n − p − 1)를 갖는 F − 분포를 따른다.\n\n일원 분산분석\nOne-way ANOVA table\n\n다중 회귀분석의 ANOVA 표와\n일원 분산분석(One-way ANOVA) 표는\n기본적으로 동일한 통계적 원리 위에서 구성된다.\n\n두 분석 모두 전체 변동(총제곱합, SST)을\n설명 가능한 부분과 설명 불가능한 부분으로 분해하고,\n이를 토대로 F-검정을 수행하여 모형 또는 집단 간 차이가 유의한지 판단한다는 점에서 연결된다.\n\n다중 회귀에서는 총제곱합을 회귀제곱합(SSR)과 잔차제곱합(SSE)으로 분해하며,\n이는 설명변수가 종속변수를 얼마나 잘 설명하는지 평가하는 과정이다.\n\n일원 분산분석에서는 총제곱합을 집단 간 제곱합(SSB)과\n집단 내 제곱합(SSE)으로 분해하는데,\n\n이는 집단 평균 간 차이가 통계적으로 유의한지 검정하는 과정이다.\n이러한 연결점은 일원 분산분석이 다중 회귀분석의 특수한 형태라는 점에서 명확해진다.\n\n즉, 범주형 독립변수(집단)를 더미 변수로 변환하여 회귀 모형에 포함하면,\n일원 분산분석은 다중 회귀분석으로 표현될 수 있다.\n\n따라서 두 분석을 나란히 이해하는 것은\n회귀분석과 분산분석의 수학적·논리적 일관성을 보여주며,\n분석자가 다양한 데이터 구조를 동일한 틀에서 해석할 수 있도록 돕는다.\n\n그럼에도 불구하고 두 분석은 활용 목적과 해석에서 차이를 가진다.\n\n다중 회귀분석은 여러 개의 독립변수가\n종속변수에 미치는 개별적·동시적 효과를 추정하고,\n각 회귀계수에 대한 유의성 검정을 통해 변수가 기여하는 정도를 해석한다.\n\n반면, 일원 분산분석은 주로 집단 평균 차이라는 단일한 질문에 초점을 두며,\n개별 집단 간 차이를 사후검정(Post-hoc test)을 통해 추가적으로 분석한다.\n\n결과적으로 회귀분석의 ANOVA 표는\n모형 전체의 설명력을 평가하는 과정에서 필요하고,\n일원 ANOVA 표는 집단 간 평균 차이가 존재하는지를 평가하는 과정에서 필요하다.\n\n즉, 두 방법론은 수학적 구조를 공유하지만,\n회귀는 예측과 변수 효과 추정에 강점이 있고,\nANOVA는 집단 비교에 특화되어 있다는 점에서 차별화된다.\n\n따라서 연구자가 어떤 질문을 던지는지에 따라\n적절한 분석 방법을 선택해야 하는 것이 중요하다.\n\n변동 요인(Source)\t제곱합(SS)\t자유도(df)\t평균제곱합(MS)\tF-값\nBetween Groups (SSB, 집단 간)\tSSB\tk − 1\tMSB = SSB / (k − 1)\tF = MSB / MSE\nWithin Groups (SSE, 집단 내)\tSSE\tN − k\tMSE = SSE / (N − k)\t\nTotal (SST)\tSST\tN − 1\t\n\n다중 회귀분석 사례\n700명의 고객을 대상으로 어떤 제품에 대해 조사하려 얻은 것이다.\n\n::: {#9c3b46b6 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satisfaction.csv\"\nSatisfaction = pd.read_csv(url)\nSatisfaction.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Y</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Gender1</th>\n      <th>Age1</th>\n      <th>Age2</th>\n      <th>Age3</th>\n      <th>Age4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nX1: 다자인 만족도\nX2: 사용 편리성 만족도\nX3: 성능 만족도\nX4: 고장 및 견고성 만족도\nY:    구입 의향\n\n설문 데이터 분석에서의 접근\n설문을 통해 얻은 만족도 및 구입 의향 데이터는\n보통 순위형(ordinal) 데이터로 수집된다.\n\n예를 들어 \" 매우 만족 ~ 매우 불만족 \" 과 같은 리커트 척도는 순서가 존재하지만,\n각 항목 간 간격이 동일하다고 보장되지 않는 특성이 있다.\n\n가전제품과 같은 대기업의 고객 만족 조사에서는\n위과 같은 변수를 설정할 수 있다.\n\n이때 연구의 초점은 Y(구입 의향)을 종속변수로 두고,\n네 가지 만족도 요인이 이에 얼마나 영향을 미치는지를 분석하는 것이다.\n\n분석 절차는 보통 다음과 같이 진행된다.\n\n① 상관계수 확인\n각 독립변수(X1 ~ X4)와 종속변수(Y) 간의 관계를 파악한다.\n데이터가 순위형일 경우 피어슨 상관계수보다는\n스피어만 상관계수 같은 방법이 더 적절할 수 있다.\n\n::: {#fc39d71d .cell execution_count=2}\n``` {.python .cell-code}\n# 피어슨의 상관계수\nSatisfaction.iloc[:, 1:6].corr()\n\nfrom scipy.stats import pearsonr\n\npearsonr(Satisfaction.Y, Satisfaction.X1)\npearsonr(Satisfaction.Y, Satisfaction.X2)\npearsonr(Satisfaction.Y, Satisfaction.X3)\npearsonr(Satisfaction.Y, Satisfaction.X4)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nPearsonRResult(statistic=np.float64(0.20687830951579425), pvalue=np.float64(3.321873797880896e-08))\n```\n:::\n:::\n\n\n##### ② 결정계수 확인\n단순 상관관계에서는 상관계수 r 의 제곱값 r² 이 결정계수로 해석될 수 있으며,\n이는 종속변수의 변동 중 독립변수가 설명하는 비율을 의미한다.\n\n여러 독립변수가 있을 경우에는 다중회귀분석을 통해 전체 결정계수 R² 를 산출한다.\n\n::: {#3e34e648 .cell execution_count=3}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction).fit()\nprint(SatisfactionFit.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.141\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     28.56\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           5.38e-22\nTime:                        16:29:03   Log-Likelihood:                -1174.4\nNo. Observations:                 700   AIC:                             2359.\nDf Residuals:                     695   BIC:                             2382.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.2918      0.347      3.719      0.000       0.610       1.974\nX1             0.2352      0.058      4.077      0.000       0.122       0.349\nX2             0.1666      0.066      2.543      0.011       0.038       0.295\nX3             0.2309      0.068      3.421      0.001       0.098       0.363\nX4             0.0585      0.050      1.168      0.243      -0.040       0.157\n==============================================================================\nOmnibus:                      147.199   Durbin-Watson:                   1.804\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              482.307\nSkew:                           0.987   Prob(JB):                    1.86e-105\nKurtosis:                       6.556   Cond. No.                         75.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n회귀분석 결과에서는 결정계수 R² = 0.141로,\n전체 독립변수들이 구입 의향(Y)의 변동 중 약 14.1%만 설명하고 있다.\n즉, 설명력이 높지는 않지만 전혀 의미 없는 수준도 아니다.\n\n전체 모형의 F 통계량 (F ≈ 28.56, p < 0.001)은\n\"모형이 유의미하다\"는 것을 보여준다.\n\n— 무작위로 요인들을 넣은 것보다,\n적어도 일부 독립변수가 종속변수에 영향을 준다는 증거가 있음.\n\n개별 계수 검정에서는 X1, X2, X3이 유의하였고(X1, X3 특히 강함),\nX4는 유의하지 않음 (p ≈ 0.243)\n\n— 현재 모형 하에서는 고장·견고성 만족도가\n구입 의향에 통계적으로 유의한 영향을 미친다고 보기 어렵다.\n\n결정계수가 낮은 원인으로는\n가격 요소 등 중요한 변수가 빠져 있을 가능성,\n또는 구입 의향과 관련된 여타 요인(브랜드 신뢰도, 마케팅, 사회적 영향 등)을\n포함하지 않았을 가능성 등을 고려해야 한다.\n\n##### ③ 표준화 회귀계수\n\n변수들이 서로 다른 단위를 가지고 있으므로,\n회귀분석에서 비표준화 회귀계수만 보면 어떤 변수가 \nY(구입 의향)에 더 큰 영향을 주는지 절대비교가 어렵다.\n\n따라서 표준화 회귀계수를 사용하면, 각 변수의 변화가\n\"자신의 표준편차 단위 하나 증가\"했을 때 Y가 얼마나 표준편차 단위로\n바뀌는지를 보여주므로, 변수들 간의 상대적 영향력 비교가 가능해진다.\n\n::: {#0335e943 .cell execution_count=4}\n``` {.python .cell-code}\n# 표준화 회귀계수\nfrom scipy import stats\nSatisfaction_z = Satisfaction.iloc[:,1:6].apply(stats.zscore)\nSatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',\n                         data=Satisfaction_z).fit()\nSatisfactionFit.params.round(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nIntercept    0.00000\nX1           0.16435\nX2           0.11285\nX3           0.15935\nX4           0.04635\ndtype: float64\n```\n:::\n:::\n\n\n예를 들어, 표준화 계수가 라면, X1과 X3이 구입 의향에 가장 큰 영향을 미치고,\nX4는 가장 영향이 작다는 해석이 가능하다.\n\n다만 표준화하면 절편은 일반적으로 의미가 0 근처가 되며,\n변수의 원 단위 변화에 대한 실질적인 의미(예: 점수 1점의 변화)가 사라지므로,\n연구 목적이 \"영향력의 비교\"인지 \"실질적 예측\"인지에 따라 비표준화 계수도 함께 살펴야 한다.\n\n편회귀계수(partial regression coefficient)\n\n다중회귀분석에서 각 독립변수가 종속변수에 미치는 순수한 영향력을 평가하기 위해 사용된다.\n이는 다른 독립변수들의 영향을 고정한 상태에서 특정 독립변수의 기여도를 나타낸다.\n\n회귀계수는 모든 독립변수의 영향을 고려한 상태에서 특정 독립변수의 영향을 나타내며,\n편회귀계수는 이러한 회귀계수의 일종으로 볼 수 있다.\n\n따라서 회귀계수와 유사하나 동일하지 않으며,\n다중회귀분석에서 각 독립변수의 개별적인 영향을 평가할 때 중요한 지표로 활용된다.\n\n## 02 변수 선택\n풀모델(완전모델)은 모든 독립변수를 포함하여\n종속변수에 영향을 줄 수 있는 요인을 전부 반영한 모델이다.\n\n그러나 실제로는 변수 과다로 인해 과적합이 발생할 수 있으므로,\n변수를 일부만 선택하여 축소모델을 사용하는 것이 바람직하다.\n이러한 과정이 바로 변수선택법이다.\n\n::: {#09bb403c .cell execution_count=5}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\nSatisfactionFit1 = smf.ols(formula='Y~X1+X2+X3',\n                          data=Satisfaction).fit()\nprint(SatisfactionFit1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.139\nModel:                            OLS   Adj. R-squared:                  0.136\nMethod:                 Least Squares   F-statistic:                     37.61\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           1.56e-22\nTime:                        16:29:03   Log-Likelihood:                -1175.1\nNo. Observations:                 700   AIC:                             2358.\nDf Residuals:                     696   BIC:                             2376.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.4020      0.334      4.193      0.000       0.746       2.059\nX1             0.2436      0.057      4.254      0.000       0.131       0.356\nX2             0.1763      0.065      2.712      0.007       0.049       0.304\nX3             0.2504      0.065      3.829      0.000       0.122       0.379\n==============================================================================\nOmnibus:                      140.914   Durbin-Watson:                   1.803\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              452.399\nSkew:                           0.951   Prob(JB):                     5.79e-99\nKurtosis:                       6.449   Cond. No.                         62.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nX4를 제외하고 회귀분석을 다시 수행한 결과,\n결정계수(R²)는 0.141에서 0.139로 약간 감소했다.\n\n이는 X4를 제거함으로써 모델의 전체 설명력이 조금 줄었음을 의미하며,\nX4가 종속변수에 미치는 직접적 영향은 미미하지만 다른 변수와의\n상호작용 등을 통해 간접적 기여가 있을 수 있음을 시사한다.\n\nF-통계량은 37.61로 이전보다 증가했고,\nP-value는 매우 작아(1.56e-22) 모델 전체는 여전히 통계적으로 유의하다.\n\n개별 회귀계수 검정에서는 X1, X2, X3는 유의하지만,\nX4는 유의하지 않아 실질적인 영향력은 거의 없다고 볼 수 있다.\n\n\n\n변수선택법에는 크게 세 가지가 있다.\n\n① 전진선택법(Forward Selection)\n가장 설명력이 높은 변수를 하나씩 추가하며,\n선택된 변수는 이후 단계에서 제거되지 않는다.\n\n② 후진제거법(Backward Elimination)\n모든 변수를 포함한 상태에서, 의미 없는 변수를 하나씩 제거하며,\n제거 기준은 보통 p-value를 사용한다.\n\n③ 단계적 방법(Stepwise Selection)\n전진선택법과 후진제거법을 혼합하여,\n변수를 추가하면서 동시에 불필요한 변수가 없는지 검토하는 방식이다.\n\n모형 선택 시 결정계수(R²)만으로 평가해서는 안 되며,\n조정된 결정계수(Adjusted R²), 평균제곱오차(MSE), AIC, BIC, 멜로우 C 등\n여러 지표를 종합적으로 고려해야 한다.\n\n이러한 과정을 거친 후, 잔차 분석 등을 통해 모델 가정을 확인하는 것이 옳다.\n\n회귀분석에서 변수 선택 기능의 지원 정도 비교\n\n\n\nSAS\nPROC REG와 같은 절차를 통해\n전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.\n예를 들어, SELECTION=STEPWISE 옵션을 사용하여 단계적 선택법을 수행할 수 있다.\n\nR\nstep() 함수를 사용하여 전진 선택법,\n후진 제거법, 단계적 선택법을 수행할 수 있다.\n이 함수는 AIC를 기준으로 변수 선택을 수행한다.\n\npython\nmlxtend:\nSequentialFeatureSelector를 제공하여\n전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.\n\nabess:\n고속 최적 부분집합 선택을 위한 라이브러리로,\n선형 회귀 및 분류 문제에 적용할 수 있다.\n\n\n# 03 다중공선성\n회귀분석에서는 모든 독립변수가 서로 독립적이어야 하지만,\n실제로는 변수들 간에 선형관계가 존재할 수 있다.\n\n이러한 경우, 어떤 변수들 사이에서 높은 상관성이 있는지 확인하고,\n필요 시 일부 변수를 제거해야 한다.\n\n예를 들어, 겉보기에는 의미 있는\n변수인데 통계적으로 유의하지 않게 나타나거나,\n반대로 예상과 달리 유의하게 나타나는 경우가 있다.\n\n결정계수(R²), F-통계량, 회귀계수(베타) 값이 0 이 아니더라도,\n개별 변수의 P-value가 유의하지 않다면 다중공선성을 의심하고 진단할 필요가 있다.\n\n다중공선성은 독립변수 간 높은 상관으로 인해 회귀계수 추정이 불안정해지고,\n변수의 순수한 영향력을 평가하기 어렵게 만듭니다.\n\n이를 확인하기 위해 상관행렬이나 분산팽창지수(VIF)를 활용하며,\n해결 방법으로는 상관이 높은 변수 제거, 주성분분석(PCA),\n릿지 회귀(Ridge Regression) 등의 기법을 사용할 수 있다.\n\n\n\n다중 공선성 사례\n\n::: {#a61050e0 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Multico.csv\"\nMultico = pd.read_csv(url)\nMultico.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Y</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4.5</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#d529f839 .cell execution_count=7}\n``` {.python .cell-code}\nMulticoFit = smf.ols(formula = 'Y~X1+X2+X3+X4', data = Multico).fit()\nprint(MulticoFit.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.540\nModel:                            OLS   Adj. R-squared:                  0.417\nMethod:                 Least Squares   F-statistic:                     4.404\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):             0.0149\nTime:                        16:29:03   Log-Likelihood:                -5.8055\nNo. Observations:                  20   AIC:                             21.61\nDf Residuals:                      15   BIC:                             26.59\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.1511      0.858      1.341      0.200      -0.678       2.980\nX1             0.4306      0.504      0.854      0.407      -0.644       1.505\nX2            -0.1246      0.158     -0.786      0.444      -0.462       0.213\nX3             0.0118      0.491      0.024      0.981      -1.035       1.058\nX4             0.4448      0.966      0.460      0.652      -1.614       2.504\n==============================================================================\nOmnibus:                        8.376   Durbin-Watson:                   2.843\nProb(Omnibus):                  0.015   Jarque-Bera (JB):                6.223\nSkew:                          -0.892   Prob(JB):                       0.0445\nKurtosis:                       5.070   Cond. No.                         113.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nR² = 0.540,   Adjusted R² = 0.417\n→ 모델이 종속변수 Y 변동의 약 54%를 설명하지만,\n변수 수를 고려하면 조정된 R²는 41.7%로 다소 낮다.\n\nF-statistic = 4.404,    P = 0.0149\n→ 전체 모델은 통계적으로 유의하며,\n독립변수들이 종속변수에 영향을 미칠 가능성이 있음.\n\n모든 변수의 P-value > 0.05\n→ 개별 변수는 통계적으로 유의하지 않아,\n단독으로 Y에 영향을 준다고 보기 어렵다.\n\n회귀계수는 0이 아니지만 유의하지 않으므로 \n다중공선성 가능성을 확인할 필요가 있다.\n진단 방법에는 두 가지가 있다.\n\n① 분산확대인자 (VIF)\n\nVIF 계산식\n여기서 R_j^2는 j번째 독립변수를 다른 모든 독립변수들로 회귀한 결정계수이다.\n\n일반적으로 VIF가 10을 초과하면 심각한 다중공선성이 존재한다고 판단한다.\n일부 연구에서는 VIF가 5를 초과하면 다중공선성의 가능성을 고려해야 한다고 권장한다.\n\n\n10.7 - Detecting Multicollinearity Using Variance Inflation Factors | STAT 462\n\nOkay, now that we know the effects that multicollinearity can have on our regression analyses and subsequent conclusions, how do we tell when it exists? That is, how can we tell if multicollinearity is present in our data? Some of the common methods used f\n\n::: {#4cc7b5b4 .cell execution_count=8}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\nMulticoModel = smf.ols('Y~X1+X2+X3+X4', Multico)\nfrom statsmodels.stats.outliers_influence import\\\nvariance_inflation_factor\nprint(variance_inflation_factor(MulticoModel.exog,1))\n\nimport pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_data = pd.DataFrame({\n    'X': MulticoModel.exog_names,\n    'VIE': [variance_inflation_factor(MulticoModel.exog, i) \n            for i in range(MulticoModel.exog.shape[1])]\n})\n\nvif_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6.927097111033764\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>VIE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Intercept</td>\n      <td>105.596937</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>X1</td>\n      <td>6.927097</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>X2</td>\n      <td>1.440306</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>X3</td>\n      <td>17.688462</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>X4</td>\n      <td>31.442395</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n② 공차한계 (Tolerance)\n\n공차한계가 낮다는 것은 R_j^2​가\n높다는 것을 의미하며, 이는 VIF가 높다는 것과 동일하다.\n공차한계가 0.1 이하이면 VIF가 10 이상이므로, 다중공선성이 심각하다고 볼 수 있다.\n\n③ 상태지수 (Condition Number)\n출력값이 100 이상이면 모델에 심각한 다중공선성이 존재할 가능성이 높다.\n\n::: {#3f65c58a .cell execution_count=9}\n``` {.python .cell-code}\nMulticoFit = smf.ols(formula='Y~X1+X2+X3+X4', data=Multico).fit()\nMulticoFit.condition_number\n\nimport statsmodels.formula.api as smf\nMulticoFit1 = smf.ols(formula='Y~X1+X3', data=Multico).fit()\nprint(MulticoFit1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.521\nModel:                            OLS   Adj. R-squared:                  0.464\nMethod:                 Least Squares   F-statistic:                     9.231\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):            0.00193\nTime:                        16:29:03   Log-Likelihood:                -6.2205\nNo. Observations:                  20   AIC:                             18.44\nDf Residuals:                      17   BIC:                             21.43\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.9231      0.776      1.190      0.251      -0.714       2.560\nX1             0.5972      0.193      3.087      0.007       0.189       1.005\nX3             0.2208      0.118      1.873      0.078      -0.028       0.469\n==============================================================================\nOmnibus:                        7.800   Durbin-Watson:                   2.794\nProb(Omnibus):                  0.020   Jarque-Bera (JB):                5.803\nSkew:                          -0.792   Prob(JB):                       0.0550\nKurtosis:                       5.110   Cond. No.                         54.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n필요한 변수(X2, X4)를 제거하고 X1과 X3만 사용했더니 모델 설명력은 유지되거나 약간 개선되었다.\nX2, X4를 제거한 방식은 후진 제거법을 이용한 변수 선택이라고 할 수 있다.\n\nR² = 0.521,    Adjusted R² = 0.464\n→ 모델이 종속변수 Y 변동의 약 52.1%를 설명하며,\n변수 수를 고려하면 조정된 R²는 46.4%로 적당한 설명력을 보여준다.\n\nF-statistic = 9.231,    P = 0.00193\n→ 전체 모델은 통계적으로 유의하며,\n독립변수들이 종속변수 Y에 영향을 미칠 가능성이 높다.\nX1은 핵심 변수, X3는 보조 변수 정도로 해석할 수 있다.\n\n샘플 수가 적기 때문에 t-값과 P-value는 다소 변동 가능하며,\n추가 자료 확보 시 모델 신뢰성이 높아질 수 있다.\n\n::: {#00971cd5 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nMulticoFit1 = smf.ols(formula='Y ~ X1 + X3', data=Multico).fit()\n\nvif_data = pd.DataFrame({\n    'X': MulticoFit1.model.exog_names,\n    'VIE': [variance_inflation_factor(MulticoFit1.model.exog, i)\n            for i in range(MulticoFit1.model.exog.shape[1])]\n})\n\nvif_data\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>VIE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Intercept</td>\n      <td>93.846154</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>X1</td>\n      <td>1.108333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>X3</td>\n      <td>1.108333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nX1, X3 모두 VIF가 5 이하이므로 다중공선성이 없다.\n절편의 VIF가 높게 나오는 것은 상수항 특성 때문이며, 모델 해석에 영향이 없다.\n\n따라서 이전에 제거했던 X2, X4가 다중공선성을\n유발했을 가능성을 보여주는 간접적인 결과로 볼 수 있다.\n\n## 04 가변수\nDummy Variable\n\n범주형 변수는 회귀 분석에 직접 사용할 수 없으므로,\n이를 수치형 변수로 변환해야 한다.\n\n일반적으로 k 개의 범주를 가진 변수는\nk - 1 개의 더미 변수로 변환하여 사용한다.\n\n이때, k - 1 개의 더미 변수 중 하나는 기준(reference) 범주로 설정되며,\n나머지 범주들은 이 기준 범주와의 차이를 나타내는 변수로 해석된다.\n\n\n나이별 사는 지역 변수 처리\n\n::: {#5990614e .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Dummy.csv\"\nDummy = pd.read_csv(url)\nDummy.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Y</th>\n      <th>Age</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>46</td>\n      <td>21</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>39</td>\n      <td>21</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>62</td>\n      <td>21</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>38</td>\n      <td>21</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>39</td>\n      <td>21</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n주어진 데이터에서 'Region' 변수는 3개의 범주를 가집니다:\n\nRegion 1\nRegion 2\nRegion 3\n이때, Region 3이 기준 범주(reference category)가 되며,\n회귀식은 다음과 같이 표현된다:\n\n\n여기서:\n\nβ₁: Region 1에 해당하는 경우, Region 3과 비교하여 Y값의 차이\nβ₂: Region 2에 해당하는 경우, Region 3과 비교하여 Y값의 차이\nβ₀: Region 3에 해당하는 경우의 Y값 (기준 범주)\n따라서, 회귀 계수 β₁과 β₂는 각각 Region 1와 Region 2이\nRegion 3에 비해 Y에 미치는 영향을 나타낸다.\n\n\n\nimport pandas as pd\nDummy['D1'] = 0\nDummy.loc[Dummy['Region']==1, 'D1']=1\nDummy['D2'] = 0\nDummy.loc[Dummy['Region']==2, 'D2']=1\nDummy.head()\n\nimport statsmodels.formula.api as smf\nDummyFit = smf.ols(formula='Y~Age+D1+D2', data=Dummy).fit()\nprint(DummyFit.summary())\n\n\n\nimport statsmodels.formula.api as smf\nDummyFit1 = smf.ols(formula='Y~Age+C(Region)', data=Dummy).fit()\nprint(DummyFit1.summary())\n\n\n\nimport statsmodels.api as sm\nsm.stats.anova_lm(DummyFit1, typ=3)\n\n\n\n05 변수변환과 비선형 회귀분석\n선형되지 않을 때 선형으로 바꿔주는 것.\n예를 들면 로지스틱 회귀.\n\n---\n\n교제: 파이썬을 활용한 데이터 분석과 응용\n\n",
    "supporting": [
      "DAD09.1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}