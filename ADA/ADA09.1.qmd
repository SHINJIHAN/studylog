---
title: "다중 회귀"
format: html
---

>Reporting Date: September. 07, 2025

다중 회귀분석에 대해 다루고자 한다.

# 01 다중 회귀분석
현실의 사회적 또는 자연적 현상을 설명할 때,
반응 변수 \(y\) 의 변화를 단일 설명 변수만으로 충분히 설명할 수 있는 경우는 거의 없다.
따라서 여러 개의 설명 변수를 적절히 선택하고, 이들의 함수로 반응 변수를 설명하면 보다 정확한 예측과 분석이 가능하다.

이러한 상황을 다루기 위해 사용하는 방법이 다중 선형 회귀 모형이며,
일반적인 수식으로는 다음과 같이 표현된다:

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i, \quad i = 1, 2, \dots, n
$$

여기서 추정해야 할 모수(parameters)는 p + 1개의 회귀계수
\beta_0, \beta_1, \dots, \beta_p​이며, \varepsilon_i​는 오차항을 나타낸다.

## 1. 오차항 가정
회귀분석의 타당성을 위해 오차항 ε 은 다음과 같은 가정을 만족해야 한다:

$$
{E}[\varepsilon] = 0, \quad \mathrm{Var}(\varepsilon) = \sigma^2
$$

기댓값은 0이며, 분산은 일정(등분산성)
서로 다른 관측치의 오차항은 서로 독립적
반응 변수 y의 전체 변동은 회귀모형과 오차항으로 분해 가능
이러한 변동 분해를 제곱합의 분할이라고 하며, 식으로 표현하면 다음과 같다:

$$
\text{SST} = \text{SSR} + \text{SSE}
$$

## 1. 전체제곱합
`TSS, SST, Total Sum of Squares`
반응변수 y 의 총 변동을 나타낸다.

이는 각 관측값이 전체 평균으로부터 얼마나 떨어져 있는지를 제곱하여 모두 합한 값이다.
따라서 y 의 전체 변동량을 측정하는 지표라고 할 수 있다.
전체제곱합의 자유도는 n − 1 이다.

## 2. 회귀제곱합
`SSR, Sum of Squares due to Regression`
`SSM, Sum of Squares due to Model`

회귀모형에 포함된 설명변수들이 반응변수의 변동을 얼마나 설명하는지를 나타낸다.
즉, 평균으로부터의 변동 중에서 회귀모형이 설명할 수 있는 부분이다.
회귀제곱합의 자유도는 설명변수의 개수 p 이다.

## 3. 잔차제곱합
`SSE, Sum of Squares due to Error`
회귀모형이 설명하지 못하는 변동으로, 실제값과 회귀모형이 예측한 값 간의 차이를 제곱하여 합한 값이다.
즉, 오차항에 의한 변동을 의미한다. 잔차제곱합의 자유도는 n − p − 1 이다.

총제곱합(SST, Total Sum of Squares): 
반응 변수 yyy의 전체 변동량을 나타내며, 각 관측값이 평균으로부터 얼마나 떨어져 있는지를 제곱하여 합한 값입니다. 자유도는 n−1n-1n−1입니다.

회귀제곱합(SSR, Sum of Squares due to Regression / SSM, Sum of Squares due to Model): 
회귀모형이 반응 변수 변동 중 얼마를 설명할 수 있는지를 나타냅니다. 자유도는 설명 변수의 개수 p이다.

잔차제곱합(SSE, Sum of Squares due to Error): 
회귀모형으로 설명되지 않는 변동으로, 실제값과 예측값 간의 차이를 제곱하여 합한 값입니다. 자유도는 n−p−1n - p - 1n−p−1입니다.

즉, 반응 변수 y의 총 편차는 회귀모형에 의해 설명된
편차와 오차항에 의한 편차로 나뉘게 되며, 이는 다음과 같이 해석할 수 있다:

$$
\underbrace{\text{총 편차}}_{\text{SST}} = \underbrace{\text{회귀로 설명된 편차}}_{\text{SSR}} + \underbrace{\text{잔차}}_{\text{SSE}}
$$

또한 각 제곱합을 자유도로 나눈 값을 평균제곱합(Mean Squares, MS)이라 하고,
이 값들을 요약한 표를 분산분석표(ANOVA table)라고 한다.

ANOVA 테이블
변동 요인(Source)	제곱합(SS)	자유도(df)	평균제곱합(MS)	F-값
회귀(Regression, 모형)	SSR	p = k-1	MSR = \dfrac{SSR}{p}​	F = \dfrac{MSR}{MSE}​
오차(Error, 잔차)	SSE	n − p − 1	MSE = \dfrac{SSE}{n-p-1}​	
전체(Total)	TSS	n − 1	

F 통계량은 회귀모형이나 분산분석에서 모형이 설명하는 변동(설명된 변동)과 
잔차 변동(설명되지 않은 변동)을 비교하여, 모형이 데이터를 얼마나 잘 설명하는지를 나타내는 지표이다.
값이 클수록 모형의 설명력이 상대적으로 높다고 해석할 수 있다.

이는 전체 변동 중 모형이 설명하는 비율인 결정계수와도 연결된다.
일반적으로 1에 가까울수록 모형이 데이터를 잘 설명한다고 본다.

다만, 단순히 F 통계량이나 R2 값의 크기만으로는 모형의 유의성을 단정할 수 없다.
자유도와 표본 크기, 변동의 구조에 따라 "얼마나 크면 충분히 큰가"라는 절대적 기준이 달라지기 때문이다.

따라서 F 통계량과 함께 p-값을 확인하는 것이 필수적이다.
p-값이 유의수준(예: 0.05)보다 작으면, "모든 집단의 평균이 같다" 혹은
"회귀계수가 모두 0이다" 라는 귀무가설을 기각할 수 있으며, 모형이 통계적으로 유의함을 의미한다.

반대로 p-값이 크면 귀무가설을 기각할 증거가 부족하다는 뜻이다.
이 경우 반드시 모형을 "처음부터 다시" 만들어야 한다는 것은 아니며,
변수 선택, 데이터 수집, 가정 검토 등의 추가 작업이 필요할 수 있다.

다중회귀분석에서는 p 개의 설명변수가 반응변수 y 를 설명하는 데 유의하게 기여하는지를 확인할 필요가 있다.
이를 위해 귀무가설 H₀ :  β₁ = β₂ = ⋯ = βp = 0 를 세우고, 다음과 같은 검정통계량을 사용한다:

이 검정통계량은 귀무가설이 참일 때 자유도 (p, n − p − 1)를 갖는 F − 분포를 따른다.

## 일원 분산분석
`One-way ANOVA table`
다중 회귀분석의 ANOVA 표와 일원 분산분석(One-way ANOVA) 표는 기본적으로 동일한 통계적 원리 위에서 구성된다.
두 분석 모두 전체 변동(총제곱합, SST)을 설명 가능한 부분과 설명 불가능한 부분으로 분해하고,
이를 토대로 F-검정을 수행하여 모형 또는 집단 간 차이가 유의한지 판단한다는 점에서 연결된다.

다중 회귀에서는 총제곱합을 회귀제곱합(SSR)과 잔차제곱합(SSE)으로 분해하며,
이는 설명변수가 종속변수를 얼마나 잘 설명하는지 평가하는 과정이다.

일원 분산분석에서는 총제곱합을 집단 간 제곱합(SSB)과 집단 내 제곱합(SSE)으로 분해하는데,
이는 집단 평균 간 차이가 통계적으로 유의한지 검정하는 과정이다.

이러한 연결점은 일원 분산분석이 다중 회귀분석의 특수한 형태라는 점에서 명확해진다.
즉, 범주형 독립변수(집단)를 더미 변수로 변환하여 회귀 모형에 포함하면, 
일원 분산분석은 다중 회귀분석으로 표현될 수 있다.

따라서 두 분석을 나란히 이해하는 것은 회귀분석과 분산분석의 수학적·논리적 일관성을 보여주며,
분석자가 다양한 데이터 구조를 동일한 틀에서 해석할 수 있도록 돕는다.
그럼에도 불구하고 두 분석은 활용 목적과 해석에서 차이를 가진다.

다중 회귀분석은 여러 개의 독립변수가 종속변수에 미치는 개별적·동시적 효과를 추정하고,
각 회귀계수에 대한 유의성 검정을 통해 변수가 기여하는 정도를 해석한다.

반면, 일원 분산분석은 주로 집단 평균 차이라는 단일한 질문에 초점을 두며,
개별 집단 간 차이를 사후검정(Post-hoc test)을 통해 추가적으로 분석한다.

결과적으로 회귀분석의 ANOVA 표는 모형 전체의 설명력을 평가하는 과정에서 필요하고,
일원 ANOVA 표는 집단 간 평균 차이가 존재하는지를 평가하는 과정에서 필요하다.

즉, 두 방법론은 수학적 구조를 공유하지만, 회귀는 예측과 변수 효과 추정에 강점이 있고,
ANOVA는 집단 비교에 특화되어 있다는 점에서 차별화된다.

따라서 연구자가 어떤 질문을 던지는지에 따라
적절한 분석 방법을 선택해야 하는 것이 중요하다.

변동 요인(Source)	제곱합(SS)	자유도(df)	평균제곱합(MS)	F-값
Between Groups (SSB, 집단 간)	SSB	k − 1	MSB = SSB / (k − 1)	F = MSB / MSE
Within Groups (SSE, 집단 내)	SSE	N − k	MSE = SSE / (N − k)	
Total (SST)	SST	N − 1	

다중 회귀분석 사례
700명의 고객을 대상으로 어떤 제품에 대해 조사하려 얻은 것이다.

```{python}
import pandas as pd

url = "https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Satisfaction.csv"
Satisfaction = pd.read_csv(url)
Satisfaction.head()
```

X1: 다자인 만족도
X2: 사용 편리성 만족도
X3: 성능 만족도
X4: 고장 및 견고성 만족도
Y:    구입 의향

설문 데이터 분석에서의 접근 설문을 통해 얻은 만족도 및 구입 의향 데이터는 보통 순위형(ordinal) 데이터로 수집된다.
예를 들어 " 매우 만족 ~ 매우 불만족 " 과 같은 리커트 척도는 순서가 존재하지만, 각 항목 간 간격이 동일하다고 보장되지 않는 특성이 있다.

가전제품과 같은 대기업의 고객 만족 조사에서는 위과 같은 변수를 설정할 수 있다.
이때 연구의 초점은 Y(구입 의향)을 종속변수로 두고, 네 가지 만족도 요인이 이에 얼마나 영향을 미치는지를 분석하는 것이다.

분석 절차는 보통 다음과 같이 진행된다.

① 상관계수 확인
각 독립변수(X1 ~ X4)와 종속변수(Y) 간의 관계를 파악한다.
데이터가 순위형일 경우 피어슨 상관계수보다는 스피어만 상관계수 같은 방법이 더 적절할 수 있다.

```{python}
# 피어슨의 상관계수
Satisfaction.iloc[:, 1:6].corr()

from scipy.stats import pearsonr

pearsonr(Satisfaction.Y, Satisfaction.X1)
pearsonr(Satisfaction.Y, Satisfaction.X2)
pearsonr(Satisfaction.Y, Satisfaction.X3)
pearsonr(Satisfaction.Y, Satisfaction.X4)
```

##### ② 결정계수 확인
단순 상관관계에서는 상관계수 r 의 제곱값 r² 이 결정계수로 해석될 수 있으며,
이는 종속변수의 변동 중 독립변수가 설명하는 비율을 의미한다.

여러 독립변수가 있을 경우에는 다중회귀분석을 통해 전체 결정계수 R² 를 산출한다.

```{python}
import statsmodels.formula.api as smf
SatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',
                         data=Satisfaction).fit()
print(SatisfactionFit.summary())
```

회귀분석 결과에서는 결정계수 R² = 0.141로,
전체 독립변수들이 구입 의향(Y)의 변동 중 약 14.1%만 설명하고 있다.
즉, 설명력이 높지는 않지만 전혀 의미 없는 수준도 아니다.

전체 모형의 F 통계량 (F ≈ 28.56, p < 0.001)은 "모형이 유의미하다"는 것을 보여준다.
— 무작위로 요인들을 넣은 것보다, 적어도 일부 독립변수가 종속변수에 영향을 준다는 증거가 있음.
개별 계수 검정에서는 X1, X2, X3이 유의하였고(X1, X3 특히 강함), X4는 유의하지 않음 (p ≈ 0.243)
— 현재 모형 하에서는 고장·견고성 만족도가 구입 의향에 통계적으로 유의한 영향을 미친다고 보기 어렵다.

결정계수가 낮은 원인으로는 가격 요소 등 중요한 변수가 빠져 있을 가능성,
또는 구입 의향과 관련된 여타 요인(브랜드 신뢰도, 마케팅, 사회적 영향 등)을 포함하지 않았을 가능성 등을 고려해야 한다.

##### ③ 표준화 회귀계수
변수들이 서로 다른 단위를 가지고 있으므로, 회귀분석에서 비표준화 회귀계수만 보면 어떤 변수가 
Y(구입 의향)에 더 큰 영향을 주는지 절대비교가 어렵다.

따라서 표준화 회귀계수를 사용하면, 각 변수의 변화가 "자신의 표준편차 단위 하나 증가"했을 때 
Y가 얼마나 표준편차 단위로 바뀌는지를 보여주므로, 변수들 간의 상대적 영향력 비교가 가능해진다.

```{python}
# 표준화 회귀계수
from scipy import stats
Satisfaction_z = Satisfaction.iloc[:,1:6].apply(stats.zscore)
SatisfactionFit = smf.ols(formula='Y~X1+X2+X3+X4',
                         data=Satisfaction_z).fit()
SatisfactionFit.params.round(5)
```

예: 표준화 계수가 라면, X1과 X3이 구입 의향에 가장 큰 영향을 미치고,
X4는 가장 영향이 작다는 해석이 가능하다.

다만 표준화하면 절편은 일반적으로 의미가 0 근처가 되며,
변수의 원 단위 변화에 대한 실질적인 의미(예: 점수 1점의 변화)가 사라지므로,
연구 목적이 "영향력의 비교"인지 "실질적 예측"인지에 따라 비표준화 계수도 함께 살펴야 한다.

편회귀계수
`partial regression coefficient`
다중회귀분석에서 각 독립변수가 종속변수에 미치는 순수한 영향력을 평가하기 위해 사용된다.
이는 다른 독립변수들의 영향을 고정한 상태에서 특정 독립변수의 기여도를 나타낸다.

회귀계수는 모든 독립변수의 영향을 고려한 상태에서 특정 독립변수의 영향을 나타내며,
편회귀계수는 이러한 회귀계수의 일종으로 볼 수 있다.

따라서 회귀계수와 유사하나 동일하지 않으며,
다중회귀분석에서 각 독립변수의 개별적인 영향을 평가할 때 중요한 지표로 활용된다.

# 02 변수 선택
풀모델(완전모델)은 모든 독립변수를 포함하여
종속변수에 영향을 줄 수 있는 요인을 전부 반영한 모델이다.

그러나 실제로는 변수 과다로 인해 과적합이 발생할 수 있으므로,
변수를 일부만 선택하여 축소모델을 사용하는 것이 바람직하다. 

이러한 과정이 바로 변수선택법이다.

```{python}
import statsmodels.formula.api as smf
SatisfactionFit1 = smf.ols(formula='Y~X1+X2+X3',
                          data=Satisfaction).fit()
print(SatisfactionFit1.summary())
```

X4를 제외하고 회귀분석을 다시 수행한 결과, 결정계수(R²)는 0.141에서 0.139로 약간 감소했다.

이는 X4를 제거함으로써 모델의 전체 설명력이 조금 줄었음을 의미하며,
X4가 종속변수에 미치는 직접적 영향은 미미하지만 다른 변수와의
상호작용 등을 통해 간접적 기여가 있을 수 있음을 시사한다.

F-통계량은 37.61로 이전보다 증가했고, P-value는 매우 작아(1.56e-22) 모델 전체는 여전히 통계적으로 유의하다.
개별 회귀계수 검정에서는 X1, X2, X3는 유의하지만, X4는 유의하지 않아 실질적인 영향력은 거의 없다고 볼 수 있다.

변수선택법에는 크게 세 가지가 있다.

① 전진선택법(Forward Selection)
가장 설명력이 높은 변수를 하나씩 추가하며, 선택된 변수는 이후 단계에서 제거되지 않는다.

② 후진제거법(Backward Elimination)
모든 변수를 포함한 상태에서, 의미 없는 변수를 하나씩 제거하며, 제거 기준은 보통 p-value를 사용한다.

③ 단계적 방법(Stepwise Selection)
전진선택법과 후진제거법을 혼합하여, 변수를 추가하면서 동시에 불필요한 변수가 없는지 검토하는 방식이다.
모형 선택 시 결정계수(R²)만으로 평가해서는 안 되며, 조정된 결정계수(Adjusted R²), 평균제곱오차(MSE), AIC, BIC, 멜로우 C 등 여러 지표를 종합적으로 고려해야 한다.

이러한 과정을 거친 후, 잔차 분석 등을 통해 모델 가정을 확인하는 것이 옳다.
회귀분석에서 변수 선택 기능의 지원 정도 비교

SAS
PROC REG와 같은 절차를 통해 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.
예를 들어, SELECTION=STEPWISE 옵션을 사용하여 단계적 선택법을 수행할 수 있다.

R
step() 함수를 사용하여 전진 선택법, 후진 제거법, 단계적 선택법을 수행할 수 있다.
이 함수는 AIC를 기준으로 변수 선택을 수행한다.

python
mlxtend: SequentialFeatureSelector를 제공하여 전진 선택법, 후진 제거법, 단계적 선택법을 지원한다.
abess: 고속 최적 부분집합 선택을 위한 라이브러리로, 선형 회귀 및 분류 문제에 적용할 수 있다.

---

# 03 다중공선성
회귀분석에서는 모든 독립변수가 서로 독립적이어야 하지만, 실제로는 변수들 간에 선형관계가 존재할 수 있다.
이러한 경우, 어떤 변수들 사이에서 높은 상관성이 있는지 확인하고, 필요 시 일부 변수를 제거해야 한다.

예: 겉보기에는 의미 있는 변수인데 통계적으로 유의하지 않게 나타나거나,
반대로 예상과 달리 유의하게 나타나는 경우가 있다.

결정계수(R²), F-통계량, 회귀계수(베타) 값이 0 이 아니더라도,
개별 변수의 P-value가 유의하지 않다면 다중공선성을 의심하고 진단할 필요가 있다.
다중공선성은 독립변수 간 높은 상관으로 인해 회귀계수 추정이 불안정해지고, 변수의 순수한 영향력을 평가하기 어렵게 만듭니다.

이를 확인하기 위해 상관행렬이나 분산팽창지수(VIF)를 활용하며,
해결 방법으로는 상관이 높은 변수 제거, 주성분분석(PCA), 릿지 회귀(Ridge Regression) 등의 기법을 사용할 수 있다.

다중 공선성 사례

```{python}
import pandas as pd

url = "https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Multico.csv"
Multico = pd.read_csv(url)
Multico.head()
```

```{python}
MulticoFit = smf.ols(formula = 'Y~X1+X2+X3+X4', data = Multico).fit()
print(MulticoFit.summary())
```

R² = 0.540,   Adjusted R² = 0.417
→ 모델이 종속변수 Y 변동의 약 54%를 설명하지만, 변수 수를 고려하면 조정된 R²는 41.7%로 다소 낮다.

F-statistic = 4.404,    P = 0.0149
→ 전체 모델은 통계적으로 유의하며, 독립변수들이 종속변수에 영향을 미칠 가능성이 있음.

모든 변수의 P-value > 0.05
→ 개별 변수는 통계적으로 유의하지 않아, 단독으로 Y에 영향을 준다고 보기 어렵다.

회귀계수는 0이 아니지만 유의하지 않으므로 다중공선성 가능성을 확인할 필요가 있다.
진단 방법에는 두 가지가 있다.

① 분산확대인자 (VIF)

VIF 계산식
여기서 R_j^2는 j번째 독립변수를 다른 모든 독립변수들로 회귀한 결정계수이다.

일반적으로 VIF가 10을 초과하면 심각한 다중공선성이 존재한다고 판단한다.
일부 연구에서는 VIF가 5를 초과하면 다중공선성의 가능성을 고려해야 한다고 권장한다.

10.7 - Detecting Multicollinearity Using Variance Inflation Factors | STAT 462

Okay, now that we know the effects that multicollinearity can have on our regression analyses and subsequent conclusions, how do we tell when it exists? That is, how can we tell if multicollinearity is present in our data? Some of the common methods used f

```{python}
import statsmodels.formula.api as smf
MulticoModel = smf.ols('Y~X1+X2+X3+X4', Multico)
from statsmodels.stats.outliers_influence import\
variance_inflation_factor
print(variance_inflation_factor(MulticoModel.exog,1))

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame({
    'X': MulticoModel.exog_names,
    'VIE': [variance_inflation_factor(MulticoModel.exog, i) 
            for i in range(MulticoModel.exog.shape[1])]
})

vif_data
```

② 공차한계 (Tolerance)

공차한계가 낮다는 것은 R_j^2​가 높다는 것을 의미하며, 이는 VIF가 높다는 것과 동일하다.
공차한계가 0.1 이하이면 VIF가 10 이상이므로, 다중공선성이 심각하다고 볼 수 있다.

③ 상태지수 (Condition Number)
출력값이 100 이상이면 모델에 심각한 다중공선성이 존재할 가능성이 높다.

```{python}
MulticoFit = smf.ols(formula='Y~X1+X2+X3+X4', data=Multico).fit()
MulticoFit.condition_number

import statsmodels.formula.api as smf
MulticoFit1 = smf.ols(formula='Y~X1+X3', data=Multico).fit()
print(MulticoFit1.summary())
```

필요한 변수(X2, X4)를 제거하고 X1과 X3만 사용했더니 모델 설명력은 유지되거나 약간 개선되었다.
X2, X4를 제거한 방식은 후진 제거법을 이용한 변수 선택이라고 할 수 있다.

R² = 0.521,    Adjusted R² = 0.464
→ 모델이 종속변수 Y 변동의 약 52.1%를 설명하며,
변수 수를 고려하면 조정된 R²는 46.4%로 적당한 설명력을 보여준다.

F-statistic = 9.231,    P = 0.00193
→ 전체 모델은 통계적으로 유의하며, 독립변수들이 종속변수 Y에 영향을 미칠 가능성이 높다.
X1은 핵심 변수, X3는 보조 변수 정도로 해석할 수 있다.

샘플 수가 적기 때문에 t-값과 P-value는 다소 변동 가능하며, 추가 자료 확보 시 모델 신뢰성이 높아질 수 있다.

```{python}
import pandas as pd
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor

MulticoFit1 = smf.ols(formula='Y ~ X1 + X3', data=Multico).fit()

vif_data = pd.DataFrame({
    'X': MulticoFit1.model.exog_names,
    'VIE': [variance_inflation_factor(MulticoFit1.model.exog, i)
            for i in range(MulticoFit1.model.exog.shape[1])]
})

vif_data
```

X1, X3 모두 VIF가 5 이하이므로 다중공선성이 없다.
절편의 VIF가 높게 나오는 것은 상수항 특성 때문이며, 모델 해석에 영향이 없다.
따라서 이전에 제거했던 X2, X4가 다중공선성을 유발했을 가능성을 보여주는 간접적인 결과로 볼 수 있다.

---

# 04 가변수
`Dummy Variable`
범주형 변수는 회귀 분석에 직접 사용할 수 없으므로, 이를 수치형 변수로 변환해야 한다.
일반적으로 k 개의 범주를 가진 변수는 k - 1 개의 더미 변수로 변환하여 사용한다.
이때, k - 1 개의 더미 변수 중 하나는 기준(reference) 범주로 설정되며,
나머지 범주들은 이 기준 범주와의 차이를 나타내는 변수로 해석된다.

나이별 사는 지역 변수 처리

```{python}
import pandas as pd

url = "https://raw.githubusercontent.com/SHINJIHAN/advanced-bigdata/main/data/Dummy.csv"
Dummy = pd.read_csv(url)
Dummy.head()
```

주어진 데이터에서 'Region' 변수는 3개의 범주를 가집니다:

Region 1
Region 2
Region 3

이때, Region 3이 기준 범주(reference category)가 되며,
회귀식은 다음과 같이 표현된다:

여기서:

β₁: Region 1에 해당하는 경우, Region 3과 비교하여 Y값의 차이
β₂: Region 2에 해당하는 경우, Region 3과 비교하여 Y값의 차이
β₀: Region 3에 해당하는 경우의 Y값 (기준 범주)
따라서, 회귀 계수 β₁과 β₂는 각각 Region 1와 Region 2이
Region 3에 비해 Y에 미치는 영향을 나타낸다.

import pandas as pd
Dummy['D1'] = 0
Dummy.loc[Dummy['Region']==1, 'D1']=1
Dummy['D2'] = 0
Dummy.loc[Dummy['Region']==2, 'D2']=1
Dummy.head()

import statsmodels.formula.api as smf
DummyFit = smf.ols(formula='Y~Age+D1+D2', data=Dummy).fit()
print(DummyFit.summary())

import statsmodels.formula.api as smf
DummyFit1 = smf.ols(formula='Y~Age+C(Region)', data=Dummy).fit()
print(DummyFit1.summary())

import statsmodels.api as sm
sm.stats.anova_lm(DummyFit1, typ=3)

---

# 05 변수변환과 비선형 회귀분석
선형되지 않을 때 선형으로 바꿔주는 것.
예를 들면 로지스틱 회귀.

---

교제: 파이썬을 활용한 데이터 분석과 응용